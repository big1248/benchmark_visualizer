import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import os
import glob
from pathlib import Path
import numpy as np
from scipy import stats

import streamlit as st
import requests
import zipfile
import os

@st.cache_data(ttl=86400)
def download_data_from_github():
    """GitHub Releasesì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
    
    data_dir = Path('./data')
    
    # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
    if data_dir.exists() and len(list(data_dir.glob('*.csv'))) > 0:
        return
    
    try:
        # ê¸°ì¡´ data í´ë” ì‚­ì œ
        if data_dir.exists():
            import shutil
            shutil.rmtree(data_dir)
        
        repo = "kjs9964/benchmark_visualizer"
        tag = "v2.2.0"
        url = f"https://github.com/{repo}/releases/download/{tag}/data.zip"
        
        # ë‹¤ìš´ë¡œë“œ
        st.info("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
        response = requests.get(url, timeout=60)
        response.raise_for_status()
        
        # ì €ì¥ ë° ì••ì¶• í•´ì œ
        with open('data.zip', 'wb') as f:
            f.write(response.content)
        
        with zipfile.ZipFile('data.zip', 'r') as zip_ref:
            zip_ref.extractall('.')
        
        os.remove('data.zip')
        
        # ê²€ì¦
        if not data_dir.exists():
            raise Exception("data í´ë”ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        csv_count = len(list(data_dir.glob('*.csv')))
        if csv_count == 0:
            raise Exception("CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        st.success(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({csv_count}ê°œ íŒŒì¼)")
        
    except Exception as e:
        st.error(f"âŒ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        st.error("GitHub Release í™•ì¸: https://github.com/kjs9964/benchmark_visualizer/releases")
        st.stop()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="LLM ë²¤ì¹˜ë§ˆí¬ ì‹œê°í™”",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë‹¤êµ­ì–´ ì§€ì› ì„¤ì •
LANGUAGES = {
    'ko': {
        'title': 'LLM ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì‹œê°í™” ë„êµ¬',
        'data_dir': 'ë°ì´í„° ë””ë ‰í† ë¦¬',
        'filters': 'í•„í„° ì˜µì…˜',
        'testname': 'í…ŒìŠ¤íŠ¸ëª…',
        'all': 'ì „ì²´',
        'model': 'ëª¨ë¸',
        'detail_type': 'ìƒì„¸ë„',
        'prompting': 'í”„ë¡¬í”„íŒ… ë°©ì‹',
        'session': 'ì„¸ì…˜',
        'problem_type': 'ë¬¸ì œ ìœ í˜•',
        'image_problem': 'ì´ë¯¸ì§€ ë¬¸ì œ',
        'text_only': 'í…ìŠ¤íŠ¸ë§Œ',
        'year': 'ì—°ë„',
        'law_type': 'ë²•ë ¹ êµ¬ë¶„',
        'law': 'ë²•ë ¹',
        'non_law': 'ë¹„ë²•ë ¹',
        'overview': 'ì „ì²´ ìš”ì•½',
        'model_comparison': 'ëª¨ë¸ë³„ ë¹„êµ',
        'response_time_analysis': 'ì‘ë‹µì‹œê°„ ë¶„ì„',
        'law_analysis': 'ë²•ë ¹/ë¹„ë²•ë ¹ ë¶„ì„',
        'subject_analysis': 'ê³¼ëª©ë³„ ë¶„ì„',
        'year_analysis': 'ì—°ë„ë³„ ë¶„ì„',
        'incorrect_analysis': 'ì˜¤ë‹µ ë¶„ì„',
        'difficulty_analysis': 'ë‚œì´ë„ ë¶„ì„',
        'testset_stats': 'í…ŒìŠ¤íŠ¸ì…‹ í†µê³„',
        'total_problems': 'ì´ ë¬¸ì œ ìˆ˜',
        'accuracy': 'ì •í™•ë„',
        'correct': 'ì •ë‹µ',
        'wrong': 'ì˜¤ë‹µ',
        'law_problems': 'ë²•ë ¹ ë¬¸ì œ',
        'non_law_problems': 'ë¹„ë²•ë ¹ ë¬¸ì œ',
        'correct_rate': 'ì •ë‹µë¥ ',
        'wrong_rate': 'ì˜¤ë‹µë¥ ',
        'performance_by_model': 'ëª¨ë¸ë³„ ì„±ëŠ¥ ì§€í‘œ',
        'comparison_chart': 'ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸',
        'overall_comparison': 'ì „ì²´ í…ŒìŠ¤íŠ¸ ë¹„êµ',
        'heatmap': 'ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ì…‹ ì •ë‹µë„ íˆíŠ¸ë§µ',
        'law_ratio': 'ë²•ë ¹/ë¹„ë²•ë ¹ ì „ì²´ í†µê³„',
        'model_law_performance': 'ëª¨ë¸ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì„±ëŠ¥ ë¹„êµ',
        'law_distribution': 'ëª¨ë¸ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë„',
        'subject_performance': 'ê³¼ëª©ë³„ ì„±ëŠ¥',
        'year_performance': 'ì—°ë„ë³„ ì„±ëŠ¥',
        'top_incorrect': 'ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ Top 20',
        'all_models_incorrect': 'ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ',
        'most_models_incorrect': 'ëŒ€ë¶€ë¶„ ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ (â‰¥50%)',
        'test_info': 'í…ŒìŠ¤íŠ¸',
        'problem_id': 'ë¬¸ì œë²ˆí˜¸',
        'incorrect_count': 'ì˜¤ë‹µ ëª¨ë¸ìˆ˜',
        'correct_count': 'ì •ë‹µ ëª¨ë¸ìˆ˜',
        'total_models': 'ì´ ëª¨ë¸ìˆ˜',
        'attempted_models': 'ì‹œë„í•œ ëª¨ë¸',
        'question': 'ë¬¸ì œ',
        'difficulty_score': 'ë‚œì´ë„ ì ìˆ˜',
        'by_session': 'ì„¸ì…˜ë³„',
        'by_subject': 'ê³¼ëª©ë³„',
        'by_year': 'ì—°ë„ë³„',
        'problem_count': 'ë¬¸ì œ ìˆ˜',
        'session_distribution': 'ì„¸ì…˜ë³„ ë¬¸ì œ ë¶„í¬',
        'subject_distribution': 'ê³¼ëª©ë³„ ë¬¸ì œ ë¶„í¬',
        'year_distribution': 'ì—°ë„ë³„ ë¬¸ì œ ë¶„í¬',
        'law_distribution_stat': 'ë²•ë ¹/ë¹„ë²•ë ¹ ë¬¸ì œ ë¶„í¬',
        'basic_stats': 'ê¸°ë³¸ í†µê³„',
        'help': 'ë„ì›€ë§',
        'new_features': 'ìƒˆë¡œìš´ ê¸°ëŠ¥',
        'existing_features': 'ê¸°ì¡´ ê¸°ëŠ¥',
        'current_data': 'í˜„ì¬ í‘œì‹œ ì¤‘ì¸ ë°ì´í„°',
        'problems': 'ê°œ ë¬¸ì œ',
        'session_filter': 'íŠ¹ì • ì„¸ì…˜ì˜ ê²°ê³¼ë§Œ ë¶„ì„',
        'incorrect_pattern': 'ì–´ë ¤ìš´ ë¬¸ì œì™€ ì˜¤ë‹µ íŒ¨í„´ ë¶„ì„',
        'difficulty_comparison': 'ë¬¸ì œ ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ',
        'problem_type_filter': 'ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¬¸ì œ êµ¬ë¶„',
        'basic_filters': 'í…ŒìŠ¤íŠ¸ëª…, ëª¨ë¸, ìƒì„¸ë„, í”„ë¡¬í”„íŒ… ë°©ì‹ìœ¼ë¡œ í•„í„°ë§',
        'law_analysis_desc': 'ë²•ë ¹/ë¹„ë²•ë ¹ êµ¬ë¶„ ë¶„ì„',
        'detail_analysis': 'ê³¼ëª©ë³„, ì—°ë„ë³„ ìƒì„¸ ë¶„ì„',
        'font_size': 'í™”ë©´ í°íŠ¸ í¬ê¸°',
        'chart_text_size': 'ì°¨íŠ¸ í…ìŠ¤íŠ¸ í¬ê¸°',
        'year_problem_distribution': 'ì—°ë„ë³„ ë¬¸ì œ ìˆ˜ ë¶„í¬',
        'problem_count_table': 'ì—°ë„ë³„ ë¬¸ì œ ìˆ˜ í…Œì´ë¸”',
        'year_problem_chart': 'ì—°ë„ë³„ ë¬¸ì œ ìˆ˜',
        'total_problem_count': 'ì´ ë¬¸ì œ ìˆ˜',
        'correct_models': 'ì •ë‹µ ëª¨ë¸',
        'incorrect_models': 'ì˜¤ë‹µ ëª¨ë¸',
        'avg_accuracy_by_model': 'ëª¨ë¸ë³„ í‰ê·  ì •í™•ë„',
        'difficulty_range': 'ë‚œì´ë„ êµ¬ê°„',
        'avg_difficulty': 'í‰ê·  ë‚œì´ë„',
        'difficulty_stats_by_range': 'ë‚œì´ë„ êµ¬ê°„ë³„ ìƒì„¸ í†µê³„',
        'very_hard': 'ë§¤ìš° ì–´ë ¤ì›€',
        'hard': 'ì–´ë ¤ì›€',
        'medium': 'ë³´í†µ',
        'easy': 'ì‰¬ì›€',
        'very_easy': 'ë§¤ìš° ì‰¬ìš´',
        'problem_distribution': 'ë¬¸ì œ ë¶„í¬',
        'response_time': 'ì‘ë‹µ ì‹œê°„',
        'avg_response_time': 'í‰ê·  ì‘ë‹µ ì‹œê°„',
        'response_time_distribution': 'ì‘ë‹µ ì‹œê°„ ë¶„í¬',
        'response_time_by_model': 'ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„',
        'response_time_stats': 'ì‘ë‹µ ì‹œê°„ í†µê³„',
        'fastest_model': 'ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸',
        'slowest_model': 'ê°€ì¥ ëŠë¦° ëª¨ë¸',
        'response_time_vs_accuracy': 'ì‘ë‹µ ì‹œê°„ vs ì •í™•ë„',
        'time_per_problem': 'ë¬¸ì œë‹¹ ì‹œê°„',
        'total_time': 'ì´ ì†Œìš” ì‹œê°„',
        'seconds': 'ì´ˆ',
        'minutes': 'ë¶„',
        # í† í° ë° ë¹„ìš© ê´€ë ¨
        'token_cost_analysis': 'í† í° ë° ë¹„ìš© ë¶„ì„',
        'token_usage': 'í† í° ì‚¬ìš©ëŸ‰',
        'input_tokens': 'ì…ë ¥ í† í°',
        'output_tokens': 'ì¶œë ¥ í† í°',
        'total_tokens': 'ì´ í† í°',
        'avg_tokens_per_problem': 'ë¬¸ì œë‹¹ í‰ê·  í† í°',
        'token_distribution': 'í† í° ë¶„í¬',
        'token_efficiency': 'í† í° íš¨ìœ¨ì„±',
        'token_stats': 'í† í° í†µê³„',
        'io_ratio': 'ì…ì¶œë ¥ í† í° ë¹„ìœ¨',
        'token_per_correct': 'ì •ë‹µë‹¹ í† í°',
        'tokens': 'í† í°',
        'cost_level': 'ë¹„ìš© ìˆ˜ì¤€',
        'cost_analysis': 'ë¹„ìš© ë¶„ì„',
        'cost_per_problem': 'ë¬¸ì œë‹¹ ë¹„ìš©',
        'total_cost_estimate': 'ì´ ì˜ˆìƒ ë¹„ìš©',
        'cost_vs_accuracy': 'ë¹„ìš© vs ì •í™•ë„',
        'cost_efficiency': 'ë¹„ìš© íš¨ìœ¨ì„±',
        'most_efficient': 'ê°€ì¥ íš¨ìœ¨ì ì¸ ëª¨ë¸',
        'least_efficient': 'ê°€ì¥ ë¹„íš¨ìœ¨ì ì¸ ëª¨ë¸',
        'cost_stats': 'ë¹„ìš© í†µê³„',
        'high': 'ë†’ìŒ',
        'medium_cost': 'ì¤‘ê°„',
        'low': 'ë‚®ìŒ',
        'very_low': 'ë§¤ìš°ë‚®ìŒ',
        'free': 'ë¬´ë£Œ',
        'cost': 'ë¹„ìš©',
        'actual_cost': 'ì‹¤ì œ ë¹„ìš©',
        'estimated_cost': 'ì˜ˆìƒ ë¹„ìš©',
        'cost_per_1k_tokens': '1K í† í°ë‹¹ ë¹„ìš©',
        'total_estimated_cost': 'ì´ ì˜ˆìƒ ë¹„ìš©',
        'usd': 'ë‹¬ëŸ¬',
    },
    'en': {
        'title': 'LLM Benchmark Results Visualization Tool',
        'data_dir': 'Data Directory',
        'filters': 'Filter Options',
        'testname': 'Test Name',
        'all': 'All',
        'model': 'Model',
        'detail_type': 'Detail Type',
        'prompting': 'Prompting Method',
        'session': 'Session',
        'problem_type': 'Problem Type',
        'image_problem': 'Image Problem',
        'text_only': 'Text Only',
        'year': 'Year',
        'law_type': 'Law Type',
        'law': 'Law',
        'non_law': 'Non-Law',
        'overview': 'Overview',
        'model_comparison': 'Model Comparison',
        'response_time_analysis': 'Response Time Analysis',
        'law_analysis': 'Law/Non-Law Analysis',
        'subject_analysis': 'Subject Analysis',
        'year_analysis': 'Year Analysis',
        'incorrect_analysis': 'Incorrect Answer Analysis',
        'difficulty_analysis': 'Difficulty Analysis',
        'testset_stats': 'Test Set Statistics',
        'total_problems': 'Total Problems',
        'accuracy': 'Accuracy',
        'correct': 'Correct',
        'wrong': 'Wrong',
        'law_problems': 'Law Problems',
        'non_law_problems': 'Non-Law Problems',
        'correct_rate': 'Correct Rate',
        'wrong_rate': 'Wrong Rate',
        'performance_by_model': 'Performance Metrics by Model',
        'comparison_chart': 'Model Performance Comparison Chart',
        'overall_comparison': 'Overall Test Comparison',
        'heatmap': 'Model Ã— Test Set Accuracy Heatmap',
        'law_ratio': 'Law/Non-Law Overall Statistics',
        'model_law_performance': 'Model Law/Non-Law Performance Comparison',
        'law_distribution': 'Law/Non-Law Accuracy by Model',
        'subject_performance': 'Performance by Subject',
        'year_performance': 'Performance by Year',
        'top_incorrect': 'Top 20 Problems with Highest Incorrect Rate',
        'all_models_incorrect': 'Problems All Models Got Wrong',
        'most_models_incorrect': 'Problems Most Models Got Wrong (â‰¥50%)',
        'test_info': 'Test',
        'problem_id': 'Problem ID',
        'incorrect_count': 'Incorrect Models',
        'correct_count': 'Correct Models',
        'total_models': 'Total Models',
        'attempted_models': 'Attempted Models',
        'question': 'Question',
        'difficulty_score': 'Difficulty Score',
        'by_session': 'By Session',
        'by_subject': 'By Subject',
        'by_year': 'By Year',
        'problem_count': 'Problem Count',
        'session_distribution': 'Problem Distribution by Session',
        'subject_distribution': 'Problem Distribution by Subject',
        'year_distribution': 'Problem Distribution by Year',
        'law_distribution_stat': 'Law/Non-Law Problem Distribution',
        'basic_stats': 'Basic Statistics',
        'help': 'Help',
        'new_features': 'New Features',
        'existing_features': 'Existing Features',
        'current_data': 'Currently Displayed Data',
        'problems': ' problems',
        'session_filter': 'Analyze specific session results only',
        'incorrect_pattern': 'Analyze difficult problems and incorrect patterns',
        'difficulty_comparison': 'Compare model performance by problem difficulty',
        'problem_type_filter': 'Distinguish image/text problems',
        'basic_filters': 'Filter by test name, model, detail type, prompting method',
        'law_analysis_desc': 'Analyze law/non-law distinction',
        'detail_analysis': 'Detailed analysis by subject and year',
        'font_size': 'Screen Font Size',
        'chart_text_size': 'Chart Text Size',
        'year_problem_distribution': 'Problem Distribution by Year',
        'problem_count_table': 'Problem Count by Year',
        'year_problem_chart': 'Problems by Year',
        'total_problem_count': 'Total Problems',
        'correct_models': 'Correct Models',
        'incorrect_models': 'Incorrect Models',
        'avg_accuracy_by_model': 'Average Accuracy by Model',
        'difficulty_range': 'Difficulty Range',
        'avg_difficulty': 'Average Difficulty',
        'difficulty_stats_by_range': 'Detailed Statistics by Difficulty Range',
        'very_hard': 'Very Hard',
        'hard': 'Hard',
        'medium': 'Medium',
        'easy': 'Easy',
        'very_easy': 'Very Easy',
        'problem_distribution': 'Problem Distribution',
        'response_time': 'Response Time',
        'avg_response_time': 'Average Response Time',
        'response_time_distribution': 'Response Time Distribution',
        'response_time_by_model': 'Response Time by Model',
        'response_time_stats': 'Response Time Statistics',
        'fastest_model': 'Fastest Model',
        'slowest_model': 'Slowest Model',
        'response_time_vs_accuracy': 'Response Time vs Accuracy',
        'time_per_problem': 'Time per Problem',
        'total_time': 'Total Time',
        'seconds': 'seconds',
        'minutes': 'minutes',
        # Token & Cost related
        'token_cost_analysis': 'Token & Cost Analysis',
        'token_usage': 'Token Usage',
        'input_tokens': 'Input Tokens',
        'output_tokens': 'Output Tokens',
        'total_tokens': 'Total Tokens',
        'avg_tokens_per_problem': 'Avg Tokens per Problem',
        'token_distribution': 'Token Distribution',
        'token_efficiency': 'Token Efficiency',
        'token_stats': 'Token Statistics',
        'io_ratio': 'Input/Output Token Ratio',
        'token_per_correct': 'Tokens per Correct Answer',
        'tokens': 'tokens',
        'cost_level': 'Cost Level',
        'cost_analysis': 'Cost Analysis',
        'cost_per_problem': 'Cost per Problem',
        'total_cost_estimate': 'Total Cost Estimate',
        'cost_vs_accuracy': 'Cost vs Accuracy',
        'cost_efficiency': 'Cost Efficiency',
        'most_efficient': 'Most Efficient Model',
        'least_efficient': 'Least Efficient Model',
        'cost_stats': 'Cost Statistics',
        'high': 'High',
        'medium_cost': 'Medium',
        'low': 'Low',
        'very_low': 'Very Low',
        'free': 'Free',
        'cost': 'cost',
        'actual_cost': 'Actual Cost',
        'estimated_cost': 'Estimated Cost',
        'cost_per_1k_tokens': 'Cost per 1K Tokens',
        'total_estimated_cost': 'Total Estimated Cost',
        'usd': 'USD',
    }
}

# ì»¤ìŠ¤í…€ CSS - í°íŠ¸ í¬ê¸° ë° ë ˆì´ì•„ì›ƒ ì¡°ì •
def apply_custom_css(font_size_multiplier=1.0):
    base_font = int(16 * font_size_multiplier)
    metric_value = int(32 * font_size_multiplier)
    metric_label = int(18 * font_size_multiplier)
    h1_size = f"{3 * font_size_multiplier}rem"
    h2_size = f"{2.2 * font_size_multiplier}rem"
    h3_size = f"{1.8 * font_size_multiplier}rem"
    
    st.markdown(f"""
    <style>
        /* ì „ì²´ í°íŠ¸ í¬ê¸° ì¦ê°€ */
        html, body, [class*="css"] {{
            font-size: {base_font}px;
        }}
        
        /* ë©”íŠ¸ë¦­ ì¹´ë“œ í°íŠ¸ í¬ê¸° */
        [data-testid="stMetricValue"] {{
            font-size: {metric_value}px !important;
        }}
        
        [data-testid="stMetricLabel"] {{
            font-size: {metric_label}px !important;
        }}
        
        /* í—¤ë” í°íŠ¸ í¬ê¸° */
        h1 {{
            font-size: {h1_size} !important;
            font-weight: 700 !important;
        }}
        
        h2 {{
            font-size: {h2_size} !important;
            font-weight: 600 !important;
            margin-top: 1.5rem !important;
        }}
        
        h3 {{
            font-size: {h3_size} !important;
            font-weight: 600 !important;
        }}
        
        /* í…Œì´ë¸” í°íŠ¸ í¬ê¸° */
        .dataframe {{
            font-size: {int(16 * font_size_multiplier)}px !important;
        }}
        
        .dataframe th {{
            font-size: {int(16 * font_size_multiplier)}px !important;
            font-weight: 600 !important;
        }}
        
        .dataframe td {{
            font-size: {int(16 * font_size_multiplier)}px !important;
        }}
        
        /* ì‚¬ì´ë“œë°” í°íŠ¸ í¬ê¸° */
        .css-1d391kg, [data-testid="stSidebar"] {{
            font-size: {int(15 * font_size_multiplier)}px !important;
        }}
        
        /* íƒ­ í°íŠ¸ í¬ê¸° */
        .stTabs [data-baseweb="tab-list"] button {{
            font-size: {int(18 * font_size_multiplier)}px !important;
            padding: 12px 20px !important;
        }}
        
        /* ë²„íŠ¼ í°íŠ¸ í¬ê¸° */
        .stButton>button {{
            font-size: {base_font}px !important;
            padding: 0.5rem 1rem !important;
        }}
        
        /* ì…€ë ‰íŠ¸ë°•ìŠ¤ í°íŠ¸ í¬ê¸° */
        .stSelectbox label, .stMultiSelect label {{
            font-size: {base_font}px !important;
            font-weight: 600 !important;
        }}
        
        /* ì°¨íŠ¸ ì—¬ë°± ì¡°ì • */
        .js-plotly-plot {{
            margin: 1rem 0 !important;
        }}
        
        /* ì»¨í…Œì´ë„ˆ íŒ¨ë”© */
        .block-container {{
            padding-top: 2rem !important;
            padding-bottom: 2rem !important;
        }}
    </style>
    """, unsafe_allow_html=True)

# Plotly ì°¨íŠ¸ ê¸€ë¡œë²Œ í°íŠ¸ í¬ê¸° ì„¤ì •
def set_plotly_font_size(chart_text_multiplier=1.0):
    """ëª¨ë“  Plotly ì°¨íŠ¸ì— ì ìš©ë  ê¸°ë³¸ í°íŠ¸ í¬ê¸° ì„¤ì •"""
    import plotly.io as pio
    
    # ê¸°ë³¸ í°íŠ¸ í¬ê¸° ê³„ì‚°
    title_size = int(20 * chart_text_multiplier)
    axis_size = int(14 * chart_text_multiplier)
    tick_size = int(12 * chart_text_multiplier)
    legend_size = int(12 * chart_text_multiplier)
    
    # plotly ê¸°ë³¸ í…œí”Œë¦¿ ë³µì‚¬
    pio.templates["custom"] = pio.templates["plotly"]
    
    # ì „ì—­ í°íŠ¸ í¬ê¸° ì„¤ì •
    pio.templates["custom"].layout.font.size = axis_size
    pio.templates["custom"].layout.title.font.size = title_size
    
    # ì¶• í°íŠ¸ ì„¤ì •
    pio.templates["custom"].layout.xaxis.tickfont.size = tick_size
    pio.templates["custom"].layout.xaxis.title.font.size = axis_size
    pio.templates["custom"].layout.yaxis.tickfont.size = tick_size
    pio.templates["custom"].layout.yaxis.title.font.size = axis_size
    
    # ë²”ë¡€ í°íŠ¸ ì„¤ì •
    pio.templates["custom"].layout.legend.font.size = legend_size
    
    # ê¸°ë³¸ í…œí”Œë¦¿ìœ¼ë¡œ ì„¤ì •
    pio.templates.default = "custom"
    
    return int(12 * chart_text_multiplier)  # íˆíŠ¸ë§µìš© í¬ê¸° ë°˜í™˜

# ì•ˆì „í•œ ì •ë ¬ í•¨ìˆ˜ (íƒ€ì… í˜¼í•© ëŒ€ì‘)
def safe_sort(values):
    """ë¬¸ìì—´ê³¼ ìˆ«ìê°€ ì„ì—¬ìˆì–´ë„ ì•ˆì „í•˜ê²Œ ì •ë ¬"""
    try:
        # íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì •ë ¬: ìˆ«ì ë¨¼ì €, ê·¸ ë‹¤ìŒ ë¬¸ìì—´
        return sorted(values, key=lambda x: (isinstance(x, str), x))
    except:
        # ì‹¤íŒ¨í•˜ë©´ ëª¨ë‘ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬
        return sorted(values, key=str)

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
@st.cache_data
def load_data(data_dir):
    """ëª¨ë“  CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  í†µí•©"""
    
    # testset íŒŒì¼ë“¤ ë¡œë“œ
    testset_files = glob.glob(os.path.join(data_dir, "testset_*.csv"))
    testsets = {}
    for file in testset_files:
        test_name = os.path.basename(file).replace("testset_", "").replace(".csv", "")
        try:
            df = pd.read_csv(file, encoding='utf-8')
            testsets[test_name] = df
        except:
            try:
                df = pd.read_csv(file, encoding='cp949')
                testsets[test_name] = df
            except:
                continue
    
    # testsetì—ì„œ í…ŒìŠ¤íŠ¸ëª… ëª©ë¡ ì¶”ì¶œ (ìë™ ê°ì§€)
    available_test_names = list(testsets.keys())
    
    # ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ
    result_files = glob.glob(os.path.join(data_dir, "*_detailed_*.csv")) + \
                   glob.glob(os.path.join(data_dir, "*_summary_*.csv"))
    
    results = []
    for file in result_files:
        filename = os.path.basename(file)
        
        try:
            # íŒŒì¼ëª… í˜•ì‹: {ëª¨ë¸ëª…}_{ìƒì„¸ë„}_{í”„ë¡¬í”„íŒ…}_{í…ŒìŠ¤íŠ¸ëª…}.csv
            # ì˜ˆ: llama-3-3-70b_detailed_noprompting_ì‚°ì—…ì•ˆì „ê¸°ì‚¬.csv
            
            # í…ŒìŠ¤íŠ¸ëª… ì°¾ê¸° ë° ì œê±° (testsetì—ì„œ ì¶”ì¶œí•œ ëª©ë¡ ì‚¬ìš©)
            test_name = None
            filename_without_csv = filename.replace('.csv', '')
            
            # ê°€ì¥ ê¸´ í…ŒìŠ¤íŠ¸ëª…ë¶€í„° ë§¤ì¹­ (ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€)
            sorted_test_names = sorted(available_test_names, key=len, reverse=True)
            
            for tn in sorted_test_names:
                if filename_without_csv.endswith('_' + tn):
                    test_name = tn
                    # í…ŒìŠ¤íŠ¸ëª… ì œê±°
                    filename_without_test = filename_without_csv[:-len('_' + tn)]
                    break
            
            if test_name is None:
                continue
            
            # ë‚¨ì€ ë¶€ë¶„ì„ '_'ë¡œ ë¶„ë¦¬
            parts = filename_without_test.split('_')
            
            if len(parts) < 3:
                continue
            
            # ìƒì„¸ë„ ì°¾ê¸° (detailed ë˜ëŠ” summary)
            detail_type = None
            detail_idx = -1
            for i, part in enumerate(parts):
                if part in ['detailed', 'summary']:
                    detail_type = part
                    detail_idx = i
                    break
            
            if detail_type is None or detail_idx == -1:
                continue
            
            # ëª¨ë¸ëª… ì¶”ì¶œ (ìƒì„¸ë„ ì´ì „ê¹Œì§€ì˜ ëª¨ë“  ë¶€ë¶„ì„ ê²°í•©)
            model_parts = parts[:detail_idx]
            model_raw = '_'.join(model_parts)
            
            # í”„ë¡¬í”„íŒ… ë°©ì‹ ì¶”ì¶œ (ìƒì„¸ë„ ë‹¤ìŒë¶€í„° ëê¹Œì§€)
            prompt_parts = parts[detail_idx + 1:]
            prompt_raw = '_'.join(prompt_parts)
            
            # í”„ë¡¬í”„íŒ… ë°©ì‹ ì •ê·œí™”
            if "noprompting" in prompt_raw.lower() or "no-prompting" in prompt_raw.lower() or "no_prompting" in prompt_raw.lower():
                prompt_type = "no-prompting"
            elif "few-shot" in prompt_raw.lower() or "few_shot" in prompt_raw.lower() or "fewshot" in prompt_raw.lower():
                prompt_type = "few-shot"
            elif "cot" in prompt_raw.lower() or "chain-of-thought" in prompt_raw.lower():
                prompt_type = "cot"
            else:
                prompt_type = prompt_raw if prompt_raw else "unknown"
            
            # ğŸ”¥ ëª¨ë¸ëª… ìë™ íŒŒì‹± ë° ì •ê·œí™” (í•˜ë“œì½”ë”© ì œê±°)
            # ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ í•˜ì´í”ˆìœ¼ë¡œ ë³€í™˜í•˜ê³  ì†Œë¬¸ìë¡œ ì •ê·œí™”
            model_normalized = model_raw.lower().replace('_', '-')
            
            # ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ëª… í‘œì‹œ ë³€í™˜ í•¨ìˆ˜
            def format_model_name(model_str):
                """
                ëª¨ë¸ëª…ì„ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                ì˜ˆ: claude-sonnet-4-5-20250929 â†’ Claude-Sonnet-4.5
                    gpt-4o-mini â†’ GPT-4o-Mini
                    llama-3-3-70b â†’ Llama-3.3-70b
                """
                # ë‚ ì§œ íŒ¨í„´ ì œê±° (8ìë¦¬ ìˆ«ì)
                import re
                model_str = re.sub(r'-\d{8}$', '', model_str)
                
                # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: GPT ëª¨ë¸
                if model_str.startswith('gpt-'):
                    # gpt-4o-mini â†’ GPT-4o-Mini
                    parts = model_str.split('-')
                    formatted_parts = ['GPT']
                    
                    for i in range(1, len(parts)):
                        part = parts[i]
                        # 4oëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (ì†Œë¬¸ì o)
                        if part == '4o' or part == '3.5':
                            formatted_parts.append(part)
                        # ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ
                        elif part.isdigit():
                            formatted_parts.append(part)
                        # mini, turbo ë“±ì€ ì²« ê¸€ìë§Œ ëŒ€ë¬¸ì
                        else:
                            formatted_parts.append(part.capitalize())
                    
                    return '-'.join(formatted_parts)
                
                # Claude ëª¨ë¸ ì²˜ë¦¬
                if model_str.startswith('claude-'):
                    parts = model_str.split('-')
                    formatted_parts = ['Claude']
                    
                    i = 1
                    while i < len(parts):
                        part = parts[i]
                        
                        # ë²„ì „ ë²ˆí˜¸ ì²˜ë¦¬ (4-5 â†’ 4.5, 3-5 â†’ 3.5)
                        if i + 1 < len(parts) and part.isdigit() and parts[i+1].isdigit():
                            formatted_parts.append(f"{part}.{parts[i+1]}")
                            i += 2
                        # ëª¨ë¸ íƒ€ì…ì€ ì²« ê¸€ì ëŒ€ë¬¸ì
                        elif part in ['sonnet', 'haiku', 'opus']:
                            formatted_parts.append(part.capitalize())
                            i += 1
                        # ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ
                        elif part.isdigit():
                            formatted_parts.append(part)
                            i += 1
                        else:
                            formatted_parts.append(part.capitalize())
                            i += 1
                    
                    return '-'.join(formatted_parts)
                
                # ê¸°íƒ€ ëª¨ë¸: ìŠ¤ë§ˆíŠ¸ ë²„ì „ ë²ˆí˜¸ ì²˜ë¦¬
                # ì˜ˆ: llama-3-3-70b â†’ Llama-3.3-70b
                #     qwen-2-5-72b â†’ Qwen-2.5-72b
                #     qwen2-5-32b â†’ Qwen2.5-32b
                parts = model_str.split('-')
                formatted_parts = []
                
                i = 0
                while i < len(parts):
                    part = parts[i]
                    
                    # ì²« ë²ˆì§¸ íŒŒíŠ¸ (ëª¨ë¸ëª…)
                    if i == 0:
                        # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: qwen2, llama3 ë“± ìˆ«ìê°€ ë¶™ì€ ëª¨ë¸ëª…
                        if part[:-1].isalpha() and part[-1].isdigit():
                            # ë‹¤ìŒ íŒŒíŠ¸ê°€ í•œ ìë¦¬ ìˆ«ìë©´ ë²„ì „ìœ¼ë¡œ ë³€í™˜
                            if i + 1 < len(parts) and parts[i+1].isdigit() and len(parts[i+1]) == 1:
                                formatted_parts.append(f"{part.capitalize()}.{parts[i+1]}")
                                i += 2
                                continue
                        formatted_parts.append(part.capitalize())
                        i += 1
                    # ì—°ì†ëœ ë‘ ê°œì˜ í•œ ìë¦¬ ìˆ«ì â†’ ë²„ì „ ë²ˆí˜¸ë¡œ ë³€í™˜
                    elif (i + 1 < len(parts) and 
                          part.isdigit() and len(part) == 1 and 
                          parts[i+1].isdigit() and len(parts[i+1]) == 1):
                        formatted_parts.append(f"{part}.{parts[i+1]}")
                        i += 2
                    # ì¼ë°˜ ë‹¨ì–´ëŠ” ì²« ê¸€ì ëŒ€ë¬¸ì
                    elif not part.isdigit() and not any(c.isdigit() for c in part):
                        formatted_parts.append(part.capitalize())
                        i += 1
                    # ìˆ«ìë‚˜ ìˆ«ì+ë¬¸ì ì¡°í•©ì€ ê·¸ëŒ€ë¡œ
                    else:
                        formatted_parts.append(part)
                        i += 1
                
                return '-'.join(formatted_parts)
            
            model = format_model_name(model_normalized)
            
            # CSV íŒŒì¼ ì½ê¸°
            try:
                df = pd.read_csv(file, encoding='utf-8')
            except:
                try:
                    df = pd.read_csv(file, encoding='cp949')
                except:
                    continue
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            df['ëª¨ë¸'] = model
            df['ìƒì„¸ë„'] = detail_type
            df['í”„ë¡¬í”„íŒ…'] = prompt_type
            df['í…ŒìŠ¤íŠ¸ëª…'] = test_name
            
            results.append(df)
            
        except Exception as e:
            st.sidebar.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {os.path.basename(file)}")
            continue
    
    if results:
        results_df = pd.concat(results, ignore_index=True)
    else:
        results_df = pd.DataFrame()
    
    return testsets, results_df

def safe_convert_to_int(value):
    """ì•ˆì „í•˜ê²Œ ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜ - ì‰¼í‘œ êµ¬ë¶„ì ì²˜ë¦¬ ê°œì„ """
    try:
        # Noneì´ë‚˜ NaN ì²˜ë¦¬
        if pd.isna(value):
            return None
            
        # ë¬¸ìì—´ì¸ ê²½ìš° ì‰¼í‘œ ì œê±° (ì²œ ë‹¨ìœ„ êµ¬ë¶„ì)
        if isinstance(value, str):
            # ì‰¼í‘œëŠ” ì²œ ë‹¨ìœ„ êµ¬ë¶„ìì´ë¯€ë¡œ ê·¸ëƒ¥ ì œê±°
            value = value.replace(',', '')
        
        # floatë¡œ ë³€í™˜ í›„ intë¡œ ë³€í™˜
        return int(float(value))
    except (ValueError, TypeError):
        return None

def get_available_sessions(df, test_names):
    """íŠ¹ì • í…ŒìŠ¤íŠ¸ë“¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ì…˜ ëª©ë¡ ë°˜í™˜ (ë¬¸ìì—´ê³¼ ìˆ«ì ëª¨ë‘ ì§€ì›)"""
    if df is None or len(df) == 0:
        return []
    
    # ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì„ íƒ ì‹œ í•„í„°ë§
    if test_names:
        test_df = df[df['í…ŒìŠ¤íŠ¸ëª…'].isin(test_names)] if 'í…ŒìŠ¤íŠ¸ëª…' in df.columns else df
    else:
        test_df = df
    
    if 'Session' in test_df.columns:
        sessions_raw = test_df['Session'].dropna().unique().tolist()
        sessions_clean = []
        
        for s in sessions_raw:
            # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ ë¨¼ì € ì‹œë„
            s_int = safe_convert_to_int(s)
            if s_int is not None:
                # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•˜ë©´ ì •ìˆ˜ë¡œ ì €ì¥
                if s_int not in sessions_clean:
                    sessions_clean.append(s_int)
            else:
                # ìˆ«ìë¡œ ë³€í™˜ ë¶ˆê°€ëŠ¥í•˜ë©´ ë¬¸ìì—´ë¡œ ì €ì¥
                if isinstance(s, str):
                    s_clean = s.strip()
                    if s_clean and s_clean not in sessions_clean:
                        sessions_clean.append(s_clean)
        
        # ì •ë ¬: ìˆ«ì ë¨¼ì €, ê·¸ ë‹¤ìŒ ë¬¸ìì—´
        return sorted(sessions_clean, key=lambda x: (isinstance(x, str), x))
    return []

def create_problem_identifier(row, lang='ko'):
    """ë¬¸ì œ ì‹ë³„ì ìƒì„± (í…ŒìŠ¤íŠ¸ëª…/ì—°ë„/ì„¸ì…˜/ê³¼ëª©/ë¬¸ì œë²ˆí˜¸)"""
    parts = []
    
    if 'Test Name' in row and pd.notna(row['Test Name']):
        parts.append(str(row['Test Name']))
    elif 'í…ŒìŠ¤íŠ¸ëª…' in row and pd.notna(row['í…ŒìŠ¤íŠ¸ëª…']):
        parts.append(str(row['í…ŒìŠ¤íŠ¸ëª…']))
    
    if 'Year' in row and pd.notna(row['Year']):
        year_int = safe_convert_to_int(row['Year'])
        if year_int:
            parts.append(str(year_int))
    
    if 'Session' in row and pd.notna(row['Session']):
        session_int = safe_convert_to_int(row['Session'])
        if session_int:
            parts.append(f"S{session_int}")
    
    if 'Subject' in row and pd.notna(row['Subject']):
        parts.append(str(row['Subject']))
    
    if 'Number' in row and pd.notna(row['Number']):
        number_int = safe_convert_to_int(row['Number'])
        if number_int:
            parts.append(f"Q{number_int}")
    
    return " / ".join(parts) if parts else "Unknown"

def get_testset_statistics(testsets, test_name, lang='ko'):
    """í…ŒìŠ¤íŠ¸ì…‹ì˜ ê¸°ì´ˆ í†µê³„ ë°˜í™˜"""
    t = LANGUAGES[lang]
    
    if test_name not in testsets:
        return None
    
    df = testsets[test_name]
    stats = {}
    
    # ì´ ë¬¸ì œ ìˆ˜
    stats['total_problems'] = len(df)
    
    # ë²•ë ¹/ë¹„ë²•ë ¹ ë¬¸ì œ ìˆ˜
    if 'law' in df.columns:
        stats['law_problems'] = len(df[df['law'] == 'O'])
        stats['non_law_problems'] = len(df[df['law'] != 'O'])
    
    # ê³¼ëª©ë³„ ë¬¸ì œ ìˆ˜
    if 'Subject' in df.columns:
        stats['by_subject'] = df['Subject'].value_counts().to_dict()
    
    # ì—°ë„ë³„ ë¬¸ì œ ìˆ˜
    if 'Year' in df.columns:
        stats['by_year'] = df['Year'].value_counts().sort_index().to_dict()
    
    # ì„¸ì…˜ë³„ ë¬¸ì œ ìˆ˜
    if 'Session' in df.columns:
        stats['by_session'] = df['Session'].value_counts().sort_index().to_dict()
    
    return stats

# ë©”ì¸ ì‹¤í–‰
def main():
    # ğŸ”¥ GitHubì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)
    download_data_from_github()
    
    # ì–¸ì–´ ì„ íƒ (ì‚¬ì´ë“œë°” ìƒë‹¨ì— ë°°ì¹˜)
    st.sidebar.selectbox(
        "Language / ì–¸ì–´",
        options=['ko', 'en'],
        format_func=lambda x: "í•œêµ­ì–´" if x == 'ko' else "English",
        key='language'
    )
    
    lang = st.session_state.language
    t = LANGUAGES[lang]
    
    # í™”ë©´ ì„¤ì •
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ¨ " + ("í™”ë©´ ì„¤ì •" if lang == 'ko' else "Display Settings"))
    
    # í°íŠ¸ í¬ê¸° ì¡°ì •
    font_size = st.sidebar.slider(
        t['font_size'],
        min_value=0.8,
        max_value=1.5,
        value=1.0,
        step=0.1,
        help="í™”ë©´ ì „ì²´ì˜ í°íŠ¸ í¬ê¸°ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤"
    )
    
    # ì°¨íŠ¸ í…ìŠ¤íŠ¸ í¬ê¸° ì¡°ì •
    chart_text_size = st.sidebar.slider(
        t['chart_text_size'],
        min_value=0.7,
        max_value=1.8,
        value=1.0,
        step=0.1,
        help="ì°¨íŠ¸ ë‚´ë¶€ í…ìŠ¤íŠ¸, ìˆ«ì, ë ˆì´ë¸” í¬ê¸°ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤"
    )
    
    apply_custom_css(font_size)
    annotation_size = set_plotly_font_size(chart_text_size)
    
    # ì œëª©
    st.title(f"ğŸ¯ {t['title']}")
    st.markdown("---")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ëŠ” í•­ìƒ ./data (GitHubì—ì„œ ë‹¤ìš´ë¡œë“œí•œ í´ë”)
    data_dir = "./data"
    
    if not os.path.exists(data_dir):
        st.error(f"Directory not found: {data_dir}")
        return
    
    # ë°ì´í„° ë¡œë“œ
    testsets, results_df = load_data(data_dir)
    
    if results_df.empty:
        st.warning("No data files found in the specified directory.")
        return
    
    # ì •ë‹µì—¬ë¶€ ì»¬ëŸ¼ ìƒì„±
    if 'Answer' in results_df.columns and 'ì˜ˆì¸¡ë‹µ' in results_df.columns:
        results_df['ì •ë‹µì—¬ë¶€'] = results_df.apply(
            lambda row: row['Answer'] == row['ì˜ˆì¸¡ë‹µ'] if pd.notna(row['Answer']) and pd.notna(row['ì˜ˆì¸¡ë‹µ']) else False,
            axis=1
        )
    
    # ì‚¬ì´ë“œë°” í•„í„°
    st.sidebar.markdown("---")
    st.sidebar.markdown(f"## {t['filters']}")
    
    # í…ŒìŠ¤íŠ¸ëª… í•„í„° (multiselectë¡œ ë³€ê²½)
    test_names = sorted(results_df['í…ŒìŠ¤íŠ¸ëª…'].unique().tolist())
    selected_tests = st.sidebar.multiselect(
        t['testname'],
        options=test_names,
        default=test_names,
        help="ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    
    # í…ŒìŠ¤íŠ¸ ì„ íƒì— ë”°ë¥¸ ë°ì´í„° í•„í„°ë§
    if selected_tests:
        filtered_df = results_df[results_df['í…ŒìŠ¤íŠ¸ëª…'].isin(selected_tests)].copy()
    else:
        filtered_df = results_df.copy()
    
    # ëª¨ë¸ í•„í„°
    models = sorted(filtered_df['ëª¨ë¸'].unique().tolist())
    selected_models = st.sidebar.multiselect(
        t['model'],
        options=models,
        default=models
    )
    
    if selected_models:
        filtered_df = filtered_df[filtered_df['ëª¨ë¸'].isin(selected_models)]
    
    # ìƒì„¸ë„ í•„í„° (multiselectë¡œ ë³€ê²½)
    details = sorted(filtered_df['ìƒì„¸ë„'].unique().tolist())
    selected_details = st.sidebar.multiselect(
        t['detail_type'],
        options=details,
        default=details,
        help="ì—¬ëŸ¬ ìƒì„¸ë„ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    
    if selected_details:
        filtered_df = filtered_df[filtered_df['ìƒì„¸ë„'].isin(selected_details)]
    
    # í”„ë¡¬í”„íŒ… ë°©ì‹ í•„í„° (multiselectë¡œ ë³€ê²½)
    prompts = sorted(filtered_df['í”„ë¡¬í”„íŒ…'].unique().tolist())
    selected_prompts = st.sidebar.multiselect(
        t['prompting'],
        options=prompts,
        default=prompts,
        help="ì—¬ëŸ¬ í”„ë¡¬í”„íŒ… ë°©ì‹ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    
    if selected_prompts:
        filtered_df = filtered_df[filtered_df['í”„ë¡¬í”„íŒ…'].isin(selected_prompts)]
    
    # ì„¸ì…˜ í•„í„° (ì›ë³¸ ë°ì´í„°ì—ì„œ ì¶”ì¶œ, multiselectë¡œ ë³€ê²½)
    if selected_tests:
        # ì„ íƒëœ í…ŒìŠ¤íŠ¸ë“¤ì˜ ì›ë³¸ ë°ì´í„°ì—ì„œ ì„¸ì…˜ ì¶”ì¶œ
        available_sessions = get_available_sessions(results_df, selected_tests)
        if available_sessions:
            selected_sessions = st.sidebar.multiselect(
                t['session'],
                options=available_sessions,
                default=available_sessions,
                help="ì—¬ëŸ¬ ì„¸ì…˜ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
            )
            
            if selected_sessions:
                # ì„ íƒëœ ì„¸ì…˜ê³¼ ë§¤ì¹­ (ë¬¸ìì—´ê³¼ ìˆ«ì ëª¨ë‘ ì§€ì›)
                def match_session(x):
                    if pd.isna(x):
                        return False
                    
                    # xë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ ì‹œë„
                    x_int = safe_convert_to_int(x)
                    
                    # ì„ íƒëœ ì„¸ì…˜ì— ì •ìˆ˜ë¡œ ë³€í™˜ëœ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                    if x_int is not None and x_int in selected_sessions:
                        return True
                    
                    # ë¬¸ìì—´ë¡œ ì§ì ‘ ë¹„êµ
                    if isinstance(x, str):
                        x_clean = x.strip()
                        return x_clean in selected_sessions
                    
                    return False
                
                filtered_df = filtered_df[filtered_df['Session'].apply(match_session)]
    
    # ë¬¸ì œ ìœ í˜• í•„í„°
    if 'image' in filtered_df.columns:
        problem_types = [t['all'], t['image_problem'], t['text_only']]
        selected_problem_type = st.sidebar.selectbox(
            t['problem_type'],
            options=problem_types
        )
        
        if selected_problem_type == t['image_problem']:
            filtered_df = filtered_df[filtered_df['image'] != 'text_only']
        elif selected_problem_type == t['text_only']:
            filtered_df = filtered_df[filtered_df['image'] == 'text_only']
    
    # ì—°ë„ í•„í„° (ì›ë³¸ ë°ì´í„°ì—ì„œ ì¶”ì¶œí•˜ì—¬ ëª¨ë“  ì—°ë„ í‘œì‹œ)
    if 'Year' in results_df.columns:
        # ì„ íƒëœ í…ŒìŠ¤íŠ¸ë“¤ì˜ ì—°ë„ë§Œ í‘œì‹œ
        if selected_tests:
            year_source_df = results_df[results_df['í…ŒìŠ¤íŠ¸ëª…'].isin(selected_tests)]
        else:
            year_source_df = results_df
        
        # ì—°ë„ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
        years_raw = year_source_df['Year'].dropna().unique().tolist()
        years_int = []
        for y in years_raw:
            y_int = safe_convert_to_int(y)
            if y_int and y_int not in years_int:
                years_int.append(y_int)
        years = safe_sort(years_int)
        
        if years:
            selected_years = st.sidebar.multiselect(
                t['year'],
                options=years,
                default=years
            )
            
            if selected_years:
                # ì„ íƒëœ ì—°ë„ì™€ ë§¤ì¹­ë˜ëŠ” ì›ë³¸ ë°ì´í„° í•„í„°ë§
                filtered_df = filtered_df[filtered_df['Year'].apply(
                    lambda x: safe_convert_to_int(x) in selected_years if pd.notna(x) else False
                )]
    
    # ë²•ë ¹ êµ¬ë¶„ í•„í„°
    if 'law' in filtered_df.columns:
        law_options = [t['all'], t['law'], t['non_law']]
        selected_law = st.sidebar.selectbox(
            t['law_type'],
            options=law_options
        )
        
        if selected_law == t['law']:
            filtered_df = filtered_df[filtered_df['law'] == 'O']
        elif selected_law == t['non_law']:
            filtered_df = filtered_df[filtered_df['law'] != 'O']
    
    # í•„í„°ë§ëœ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
    if filtered_df.empty:
        st.warning("No data matches the selected filters.")
        return
    
    # íƒ­ ìƒì„±
    tabs = st.tabs([
        f"ğŸ“Š {t['overview']}",
        f"ğŸ” {t['model_comparison']}",
        f"â±ï¸ {t['response_time_analysis']}",
        f"âš–ï¸ {t['law_analysis']}",
        f"ğŸ“š {t['subject_analysis']}",
        f"ğŸ“… {t['year_analysis']}",
        f"âŒ {t['incorrect_analysis']}",
        f"ğŸ“ˆ {t['difficulty_analysis']}",
        f"ğŸ’° {t['token_cost_analysis']}",
        f"ğŸ“‹ {t['testset_stats']}"
    ])
    
    # íƒ­ 1: ì „ì²´ ìš”ì•½
    with tabs[0]:
        st.header(f"ğŸ“Š {t['overview']}")
        
        # í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ë¬¸ì œ ìˆ˜ ê³„ì‚°
        total_problems = 0
        if selected_tests:
            for test_name in selected_tests:
                if test_name in testsets:
                    total_problems += len(testsets[test_name])
        
        # ê³ ìœ  ë¬¸ì œ ìˆ˜ëŠ” filtered_dfì—ì„œ ì¤‘ë³µ ì œê±° (ë°±ì—…ìš©)
        unique_questions = filtered_df['Question'].nunique()
        num_models = filtered_df['ëª¨ë¸'].nunique()
        
        # í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë³¸ ì •ë³´
        st.subheader("ğŸ“‹ " + ("í…ŒìŠ¤íŠ¸ì…‹ ì •ë³´" if lang == 'ko' else "Test Set Information"))
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # í…ŒìŠ¤íŠ¸ì…‹ íŒŒì¼ì˜ ì‹¤ì œ ë¬¸ì œ ìˆ˜ ì‚¬ìš©
            display_problems = total_problems if total_problems > 0 else unique_questions
            st.metric(
                "ì´ ë¬¸ì œ ìˆ˜" if lang == 'ko' else "Total Problems",
                f"{display_problems:,}"
            )
        with col2:
            st.metric(
                "í‰ê°€ ëª¨ë¸ ìˆ˜" if lang == 'ko' else "Number of Models",
                f"{num_models}"
            )
        with col3:
            # ìˆ˜ì •: ì´ í‰ê°€ íšŸìˆ˜ = ì´ ë¬¸ì œ ìˆ˜ Ã— ëª¨ë¸ ìˆ˜
            actual_eval_count = display_problems * num_models
            st.metric(
                "ì´ í‰ê°€ íšŸìˆ˜" if lang == 'ko' else "Total Evaluations",
                f"{actual_eval_count:,}"
            )
        
        st.markdown("---")
        
        # ëª¨ë¸ í‰ê·  ì„±ëŠ¥
        st.subheader("ğŸ¯ " + ("ëª¨ë¸ í‰ê·  ì„±ëŠ¥" if lang == 'ko' else "Average Model Performance"))
        col1, col2, col3, col4 = st.columns(4)
        
        # ëª¨ë¸ë³„ ì •í™•ë„ ê³„ì‚° í›„ í‰ê· 
        model_accuracies = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean()
        avg_accuracy = model_accuracies.mean() * 100
        
        # í‰ê·  ì •ë‹µ/ì˜¤ë‹µ ìˆ˜ (ëª¨ë¸ë‹¹)
        avg_problems_per_model = display_problems  # ëª¨ë¸ë‹¹ í‰ê°€í•œ ë¬¸ì œ ìˆ˜ (í…ŒìŠ¤íŠ¸ì…‹ ê¸°ì¤€)
        avg_correct = (avg_problems_per_model * avg_accuracy / 100) if avg_problems_per_model > 0 else 0
        avg_wrong = avg_problems_per_model - avg_correct
        
        with col1:
            st.metric(
                "í‰ê·  ì •í™•ë„" if lang == 'ko' else "Average Accuracy",
                f"{avg_accuracy:.2f}%"
            )
        with col2:
            st.metric(
                "ëª¨ë¸ë‹¹ í‰ê·  ë¬¸ì œ ìˆ˜" if lang == 'ko' else "Avg Problems per Model",
                f"{avg_problems_per_model:.0f}"
            )
        with col3:
            st.metric(
                "í‰ê·  ì •ë‹µ ìˆ˜" if lang == 'ko' else "Avg Correct Answers",
                f"{avg_correct:.0f}"
            )
        with col4:
            st.metric(
                "í‰ê·  ì˜¤ë‹µ ìˆ˜" if lang == 'ko' else "Avg Wrong Answers",
                f"{avg_wrong:.0f}"
            )
        
        # ë²•ë ¹/ë¹„ë²•ë ¹ í†µê³„
        if 'law' in filtered_df.columns:
            st.markdown("---")
            st.subheader("âš–ï¸ " + ("ë²•ë ¹/ë¹„ë²•ë ¹ ë¶„ì„" if lang == 'ko' else "Law/Non-Law Analysis"))
            
            # í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë°˜ìœ¼ë¡œ ë²•ë ¹/ë¹„ë²•ë ¹ ë¬¸ì œ ìˆ˜ ê³„ì‚°
            law_count_testset = 0
            non_law_count_testset = 0
            
            if selected_tests:
                for test_name in selected_tests:
                    if test_name in testsets and 'law' in testsets[test_name].columns:
                        test_df = testsets[test_name]
                        law_count_testset += len(test_df[test_df['law'] == 'O'])
                        non_law_count_testset += len(test_df[test_df['law'] != 'O'])
            
            # ë°±ì—…: filtered_dfì—ì„œ ê³„ì‚° (í…ŒìŠ¤íŠ¸ì…‹ì´ ì—†ëŠ” ê²½ìš°)
            unique_problems = filtered_df[['Question', 'law']].drop_duplicates()
            law_count_backup = len(unique_problems[unique_problems['law'] == 'O'])
            non_law_count_backup = len(unique_problems[unique_problems['law'] != 'O'])
            
            # í…ŒìŠ¤íŠ¸ì…‹ ê°’ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë°±ì—… ì‚¬ìš©
            law_count = law_count_testset if law_count_testset > 0 else law_count_backup
            non_law_count = non_law_count_testset if non_law_count_testset > 0 else non_law_count_backup
            
            # ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  (ëª¨ë“  ëª¨ë¸ í‰ê· )
            law_df = filtered_df[filtered_df['law'] == 'O']
            non_law_df = filtered_df[filtered_df['law'] != 'O']
            
            law_accuracy = (law_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if len(law_df) > 0 else 0
            non_law_accuracy = (non_law_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if len(non_law_df) > 0 else 0
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric(t['law_problems'], f"{law_count:,}")
            with col2:
                st.metric(f"{t['law']} {t['correct_rate']}", f"{law_accuracy:.2f}%")
            with col3:
                st.metric(t['non_law_problems'], f"{non_law_count:,}")
            with col4:
                st.metric(f"{t['non_law']} {t['correct_rate']}", f"{non_law_accuracy:.2f}%")
        
        # ì‹œê°í™” ì°¨íŠ¸ ì¶”ê°€
        st.markdown("---")
        st.subheader("ğŸ“Š " + ("ì£¼ìš” ì§€í‘œ ì‹œê°í™”" if lang == 'ko' else "Key Metrics Visualization"))
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ëª¨ë¸ë³„ í‰ê·  ì •í™•ë„ ë°” ì°¨íŠ¸
            model_acc_df = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
            model_acc_df.columns = [t['model'], t['accuracy']]
            model_acc_df[t['accuracy']] = model_acc_df[t['accuracy']] * 100
            model_acc_df = model_acc_df.sort_values(t['accuracy'], ascending=False)
            
            fig = px.bar(
                model_acc_df,
                x=t['model'],
                y=t['accuracy'],
                title=t['avg_accuracy_by_model'],
                text=t['accuracy'],
                color=t['accuracy'],
                color_continuous_scale='RdYlGn'
            )
            fig.update_traces(
                texttemplate='%{text:.1f}%',
                textposition='outside',
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_layout(
                height=400,
                showlegend=False,
                yaxis_title=t['accuracy'] + ' (%)',
                xaxis_title=t['model'],
                yaxis=dict(range=[0, 100])
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  ë¹„êµ ì°¨íŠ¸
            if 'law' in filtered_df.columns:
                law_comparison = pd.DataFrame({
                    'êµ¬ë¶„': [t['law'], t['non_law']],
                    'ì •ë‹µë¥ ': [law_accuracy, non_law_accuracy],
                    'ë¬¸ì œìˆ˜': [law_count, non_law_count]
                })
                
                fig = go.Figure()
                fig.add_trace(go.Bar(
                    name=t['correct_rate'] if lang == 'ko' else 'Accuracy',
                    x=law_comparison['êµ¬ë¶„'],
                    y=law_comparison['ì •ë‹µë¥ '],
                    text=law_comparison['ì •ë‹µë¥ '].round(1),
                    texttemplate='%{text}%',
                    textposition='outside',
                    marker_color=['#FF6B6B', '#4ECDC4'],
                    marker_line_color='black',
                    marker_line_width=1.5,
                    yaxis='y'
                ))
                
                fig.add_trace(go.Scatter(
                    name=t['problem_count'] if lang == 'ko' else 'Problem Count',
                    x=law_comparison['êµ¬ë¶„'],
                    y=law_comparison['ë¬¸ì œìˆ˜'],
                    text=law_comparison['ë¬¸ì œìˆ˜'],
                    texttemplate='%{text}ê°œ',
                    textposition='top center',
                    mode='lines+markers+text',
                    marker=dict(size=10, color='orange'),
                    line=dict(width=2, color='orange'),
                    yaxis='y2'
                ))
                
                fig.update_layout(
                    title='ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  ë° ë¬¸ì œ ìˆ˜ ë¹„êµ' if lang == 'ko' else 'Law/Non-Law Accuracy and Problem Count Comparison',
                    height=400,
                    yaxis=dict(
                        title=('ì •ë‹µë¥  (%)' if lang == 'ko' else 'Accuracy (%)'),
                        range=[0, 100]
                    ),
                    yaxis2=dict(
                        title=(t['problem_count'] if lang == 'ko' else 'Problem Count'),
                        overlaying='y',
                        side='right',
                        range=[0, max(law_count, non_law_count) * 1.2]
                    ),
                    hovermode='x unified',
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    )
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                # ë²•ë ¹ ì •ë³´ê°€ ì—†ì„ ë•Œ - ëª¨ë¸ë³„ ì •ë‹µ/ì˜¤ë‹µ ìˆ˜ ì°¨íŠ¸
                model_correct_wrong = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].agg(['sum', 'count']).reset_index()
                model_correct_wrong.columns = ['ëª¨ë¸', 'ì •ë‹µ', 'ì´ë¬¸ì œ']
                model_correct_wrong['ì˜¤ë‹µ'] = model_correct_wrong['ì´ë¬¸ì œ'] - model_correct_wrong['ì •ë‹µ']
                
                fig = go.Figure()
                fig.add_trace(go.Bar(
                    name='ì •ë‹µ',
                    x=model_correct_wrong['ëª¨ë¸'],
                    y=model_correct_wrong['ì •ë‹µ'],
                    marker_color='lightgreen'
                ))
                fig.add_trace(go.Bar(
                    name='ì˜¤ë‹µ',
                    x=model_correct_wrong['ëª¨ë¸'],
                    y=model_correct_wrong['ì˜¤ë‹µ'],
                    marker_color='lightcoral'
                ))
                
                fig.update_layout(
                    barmode='stack',
                    title='ëª¨ë¸ë³„ ì •ë‹µ/ì˜¤ë‹µ ìˆ˜',
                    height=400,
                    yaxis_title='ë¬¸ì œ ìˆ˜',
                    xaxis_title='ëª¨ë¸'
                )
                st.plotly_chart(fig, use_container_width=True)
        
        # í…ŒìŠ¤íŠ¸ì…‹ë³„ ë¶„í¬ (ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ê°€ ìˆì„ ê²½ìš°)
        if 'í…ŒìŠ¤íŠ¸ëª…' in filtered_df.columns and filtered_df['í…ŒìŠ¤íŠ¸ëª…'].nunique() > 1:
            st.markdown("---")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # í…ŒìŠ¤íŠ¸ì…‹ë³„ ë¬¸ì œ ìˆ˜
                test_problem_count = filtered_df.groupby('í…ŒìŠ¤íŠ¸ëª…')['Question'].nunique().reset_index()
                test_problem_count.columns = ['í…ŒìŠ¤íŠ¸ëª…', 'ë¬¸ì œìˆ˜']
                test_problem_count = test_problem_count.sort_values('ë¬¸ì œìˆ˜', ascending=False)
                
                fig = px.bar(
                    test_problem_count,
                    x='í…ŒìŠ¤íŠ¸ëª…',
                    y='ë¬¸ì œìˆ˜',
                    title='í…ŒìŠ¤íŠ¸ì…‹ë³„ ë¬¸ì œ ìˆ˜',
                    text='ë¬¸ì œìˆ˜',
                    color='ë¬¸ì œìˆ˜',
                    color_continuous_scale='Blues'
                )
                fig.update_traces(textposition='outside')
                fig.update_layout(
                    height=400,
                    showlegend=False,
                    yaxis_title='ë¬¸ì œ ìˆ˜',
                    xaxis_title='í…ŒìŠ¤íŠ¸ëª…'
                )
                fig.update_xaxes(tickangle=45)
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # í…ŒìŠ¤íŠ¸ì…‹ë³„ í‰ê·  ì •í™•ë„
                test_accuracy = filtered_df.groupby('í…ŒìŠ¤íŠ¸ëª…')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
                test_accuracy.columns = ['í…ŒìŠ¤íŠ¸ëª…', 'ì •í™•ë„']
                test_accuracy['ì •í™•ë„'] = test_accuracy['ì •í™•ë„'] * 100
                test_accuracy = test_accuracy.sort_values('ì •í™•ë„', ascending=False)
                
                fig = px.bar(
                    test_accuracy,
                    x='í…ŒìŠ¤íŠ¸ëª…',
                    y='ì •í™•ë„',
                    title='í…ŒìŠ¤íŠ¸ì…‹ë³„ í‰ê·  ì •í™•ë„',
                    text='ì •í™•ë„',
                    color='ì •í™•ë„',
                    color_continuous_scale='RdYlGn'
                )
                fig.update_traces(
                texttemplate='%{text:.1f}%',
                textposition='outside',
                marker_line_color='black',
                marker_line_width=1.5
            )
                fig.update_layout(
                    height=400,
                    showlegend=False,
                    yaxis_title='ì •í™•ë„ (%)',
                    xaxis_title='í…ŒìŠ¤íŠ¸ëª…',
                    yaxis=dict(range=[0, 100])
                )
                fig.update_xaxes(tickangle=45)
                st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 2: ëª¨ë¸ë³„ ë¹„êµ
    with tabs[1]:
        st.header(f"ğŸ” {t['model_comparison']}")
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ê³„ì‚°
        model_stats = filtered_df.groupby('ëª¨ë¸').agg({
            'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
        }).reset_index()
        model_stats.columns = ['ëª¨ë¸', 'ì •ë‹µ', 'ì´ë¬¸ì œ', 'ì •í™•ë„']
        model_stats['ì •í™•ë„'] = model_stats['ì •í™•ë„'] * 100
        model_stats['ì˜¤ë‹µ'] = model_stats['ì´ë¬¸ì œ'] - model_stats['ì •ë‹µ']
        model_stats = model_stats.sort_values('ì •í™•ë„', ascending=False)
        
        # ì„±ëŠ¥ ì§€í‘œ í…Œì´ë¸”
        st.subheader(t['performance_by_model'])
        
        # í…Œì´ë¸” ì»¬ëŸ¼ëª… ë³€ê²½
        display_stats = model_stats.copy()
        if lang == 'en':
            display_stats.columns = ['Model', 'Correct', 'Total', 'Accuracy', 'Wrong']
        
        st.dataframe(
            display_stats.style.format({
                'ì •í™•ë„' if lang == 'ko' else 'Accuracy': '{:.2f}%'
            }).background_gradient(subset=['ì •í™•ë„' if lang == 'ko' else 'Accuracy'], cmap='RdYlGn'),
            use_container_width=True
        )
        
        # ë¹„êµ ì°¨íŠ¸
        st.markdown("---")
        st.subheader(t['comparison_chart'])
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ì •í™•ë„ ë°” ì°¨íŠ¸
            fig = px.bar(
                model_stats,
                x='ëª¨ë¸',
                y='ì •í™•ë„',
                title=t['overall_comparison'],
                text='ì •í™•ë„',
                color='ì •í™•ë„',
                color_continuous_scale='RdYlGn'
            )
            fig.update_traces(
                texttemplate='%{text:.1f}%',
                textposition='outside',
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_layout(
                height=400,
                showlegend=False,
                yaxis_title=t['accuracy'] + ' (%)',
                xaxis_title=t['model']
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # ì •ë‹µ/ì˜¤ë‹µ ìŠ¤íƒ ë°” ì°¨íŠ¸
            fig = go.Figure()
            fig.add_trace(go.Bar(
                name=t['correct'],
                x=model_stats['ëª¨ë¸'],
                y=model_stats['ì •ë‹µ'],
                marker_color='lightgreen',
                marker_line_color='black',
                marker_line_width=1.5
            ))
            fig.add_trace(go.Bar(
                name=t['wrong'],
                x=model_stats['ëª¨ë¸'],
                y=model_stats['ì˜¤ë‹µ'],
                marker_color='lightcoral',
                marker_line_color='black',
                marker_line_width=1.5
            ))
            
            fig.update_layout(
                barmode='stack',
                title=f"{t['correct']}/{t['wrong']} {t['comparison_chart']}",
                height=400,
                yaxis_title=t['problem_count'],
                xaxis_title=t['model']
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # íˆíŠ¸ë§µ
        if 'í…ŒìŠ¤íŠ¸ëª…' in filtered_df.columns:
            st.markdown("---")
            st.subheader(t['heatmap'])
            
            # ëª¨ë¸ë³„, í…ŒìŠ¤íŠ¸ë³„ ì •í™•ë„ ê³„ì‚°
            heatmap_data = filtered_df.groupby(['ëª¨ë¸', 'í…ŒìŠ¤íŠ¸ëª…'])['ì •ë‹µì—¬ë¶€'].mean() * 100
            heatmap_pivot = heatmap_data.unstack(fill_value=0)
            
            # íˆíŠ¸ë§µ ìƒì„± (ìˆ«ì í‘œì‹œ ë° ì…€ ê²½ê³„ì„  ì¶”ê°€)
            fig = go.Figure(data=go.Heatmap(
                z=heatmap_pivot.values,
                x=heatmap_pivot.columns,
                y=heatmap_pivot.index,
                colorscale='RdYlGn',
                text=np.round(heatmap_pivot.values, 1),
                texttemplate='%{text:.1f}',
                textfont={"size": int(12 * chart_text_size)},
                colorbar=dict(title=t['accuracy'] + " (%)"),
                xgap=2,  # ì…€ ê²½ê³„ì„ 
                ygap=2
            ))
            
            fig.update_layout(height=400)
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 3: ì‘ë‹µì‹œê°„ ë¶„ì„
    with tabs[2]:
        st.header(f"â±ï¸ {t['response_time_analysis']}")
        
        # ë¬¸ì œë‹¹í‰ê· ì‹œê°„(ì´ˆ) ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        time_columns = ['ë¬¸ì œë‹¹í‰ê· ì‹œê°„(ì´ˆ)', 'ì´ì†Œìš”ì‹œê°„(ì´ˆ)', 'question_duration']
        available_time_col = None
        for col in time_columns:
            if col in filtered_df.columns:
                available_time_col = col
                break
        
        if available_time_col is None:
            st.info("Response time data not available in the dataset.")
        else:
            # ì‘ë‹µì‹œê°„ ë°ì´í„° ì¤€ë¹„
            if available_time_col == 'question_duration':
                # question_durationì€ ê°œë³„ ë¬¸ì œ ì‹œê°„
                time_col = 'question_duration'
                is_per_problem = True
            elif available_time_col == 'ë¬¸ì œë‹¹í‰ê· ì‹œê°„(ì´ˆ)':
                time_col = 'ë¬¸ì œë‹¹í‰ê· ì‹œê°„(ì´ˆ)'
                is_per_problem = True
            else:
                time_col = 'ì´ì†Œìš”ì‹œê°„(ì´ˆ)'
                is_per_problem = False
            
            # NaN ê°’ ì œê±°
            time_df = filtered_df[filtered_df[time_col].notna()].copy()
            
            if len(time_df) == 0:
                st.info("No valid response time data available.")
            else:
                # 1. ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µì‹œê°„ í†µê³„
                st.subheader(t['response_time_stats'])
                
                model_time_stats = time_df.groupby('ëª¨ë¸').agg({
                    time_col: ['mean', 'median', 'std', 'min', 'max', 'count']
                }).reset_index()
                
                model_time_stats.columns = ['ëª¨ë¸', 'í‰ê· ', 'ì¤‘ì•™ê°’', 'í‘œì¤€í¸ì°¨', 'ìµœì†Œ', 'ìµœëŒ€', 'ë¬¸ì œìˆ˜']
                model_time_stats = model_time_stats.sort_values('í‰ê· ')
                
                # ì •í™•ë„ë„ í•¨ê»˜ í‘œì‹œ
                model_acc = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
                model_acc.columns = ['ëª¨ë¸', 'ì •í™•ë„']
                model_acc['ì •í™•ë„'] = model_acc['ì •í™•ë„'] * 100
                
                model_time_stats = model_time_stats.merge(model_acc, on='ëª¨ë¸')
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    fastest = model_time_stats.iloc[0]
                    st.metric(
                        t['fastest_model'],
                        fastest['ëª¨ë¸'],
                        f"{fastest['í‰ê· ']:.2f}{t['seconds']}"
                    )
                
                with col2:
                    slowest = model_time_stats.iloc[-1]
                    st.metric(
                        t['slowest_model'],
                        slowest['ëª¨ë¸'],
                        f"{slowest['í‰ê· ']:.2f}{t['seconds']}"
                    )
                
                with col3:
                    avg_time = model_time_stats['í‰ê· '].mean()
                    st.metric(
                        t['avg_response_time'],
                        f"{avg_time:.2f}{t['seconds']}"
                    )
                
                # í…Œì´ë¸”
                st.dataframe(
                    model_time_stats.style.format({
                        'í‰ê· ': '{:.2f}',
                        'ì¤‘ì•™ê°’': '{:.2f}',
                        'í‘œì¤€í¸ì°¨': '{:.2f}',
                        'ìµœì†Œ': '{:.2f}',
                        'ìµœëŒ€': '{:.2f}',
                        'ë¬¸ì œìˆ˜': '{:.0f}',
                        'ì •í™•ë„': '{:.2f}%'
                    }).background_gradient(subset=['í‰ê· '], cmap='RdYlGn_r'),
                    use_container_width=True
                )
                
                st.markdown("---")
                
                # 2. ì‹œê°í™”
                st.subheader(t['response_time_by_model'])
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # í‰ê·  ì‘ë‹µì‹œê°„ ë°” ì°¨íŠ¸
                    fig = px.bar(
                        model_time_stats,
                        x='ëª¨ë¸',
                        y='í‰ê· ',
                        title=t['avg_response_time'] + (' (' + t['time_per_problem'] + ')' if is_per_problem else ''),
                        text='í‰ê· ',
                        color='í‰ê· ',
                        color_continuous_scale='RdYlGn_r'
                    )
                    fig.update_traces(
                        texttemplate='%{text:.2f}s',
                        textposition='outside',
                        marker_line_color='black',
                        marker_line_width=1.5
                    )
                    fig.update_layout(
                        height=400,
                        showlegend=False,
                        yaxis_title=t['response_time'] + ' (' + t['seconds'] + ')',
                        xaxis_title=t['model']
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    # ë°•ìŠ¤í”Œë¡¯
                    fig = px.box(
                        time_df,
                        x='ëª¨ë¸',
                        y=time_col,
                        title=t['response_time_distribution'],
                        color='ëª¨ë¸'
                    )
                    fig.update_layout(
                        height=400,
                        showlegend=False,
                        yaxis_title=t['response_time'] + ' (' + t['seconds'] + ')',
                        xaxis_title=t['model']
                    )
                    fig.update_xaxes(tickangle=45)
                    st.plotly_chart(fig, use_container_width=True)
                
                st.markdown("---")
                
                # 3. ì‘ë‹µì‹œê°„ vs ì •í™•ë„
                st.subheader(t['response_time_vs_accuracy'])
                
                fig = px.scatter(
                    model_time_stats,
                    x='í‰ê· ',
                    y='ì •í™•ë„',
                    size='ë¬¸ì œìˆ˜',
                    text='ëª¨ë¸',
                    title=t['response_time_vs_accuracy'],
                    labels={
                        'í‰ê· ': t['avg_response_time'] + ' (' + t['seconds'] + ')',
                        'ì •í™•ë„': t['accuracy'] + ' (%)'
                    }
                )
                fig.update_traces(
                    textposition='top center',
                    marker=dict(
                        line=dict(width=2, color='black'),
                        opacity=0.7
                    )
                )
                fig.update_layout(height=500)
                st.plotly_chart(fig, use_container_width=True)
                
                # ì¸ì‚¬ì´íŠ¸
                st.info(f"""
                ğŸ’¡ **ì¸ì‚¬ì´íŠ¸**:
                - ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸: **{fastest['ëª¨ë¸']}** ({fastest['í‰ê· ']:.2f}ì´ˆ, ì •í™•ë„ {fastest['ì •í™•ë„']:.1f}%)
                - ê°€ì¥ ëŠë¦° ëª¨ë¸: **{slowest['ëª¨ë¸']}** ({slowest['í‰ê· ']:.2f}ì´ˆ, ì •í™•ë„ {slowest['ì •í™•ë„']:.1f}%)
                - ì†ë„ì™€ ì •í™•ë„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì°¨íŠ¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”.
                """)
                
                st.markdown("---")
                
                # 4. í…ŒìŠ¤íŠ¸ë³„ ì‘ë‹µì‹œê°„ (í…ŒìŠ¤íŠ¸ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°)
                if 'í…ŒìŠ¤íŠ¸ëª…' in time_df.columns and time_df['í…ŒìŠ¤íŠ¸ëª…'].nunique() > 1:
                    st.subheader(f"{t['response_time']} ({t['by_test']})" if 'by_test' in t else "í…ŒìŠ¤íŠ¸ë³„ ì‘ë‹µì‹œê°„")
                    
                    test_time = time_df.groupby(['ëª¨ë¸', 'í…ŒìŠ¤íŠ¸ëª…'])[time_col].mean().reset_index()
                    test_time.columns = ['ëª¨ë¸', 'í…ŒìŠ¤íŠ¸ëª…', 'í‰ê· ì‹œê°„']
                    
                    fig = px.bar(
                        test_time,
                        x='í…ŒìŠ¤íŠ¸ëª…',
                        y='í‰ê· ì‹œê°„',
                        color='ëª¨ë¸',
                        barmode='group',
                        title='í…ŒìŠ¤íŠ¸ë³„ ëª¨ë¸ ì‘ë‹µì‹œê°„' if lang == 'ko' else 'Response Time by Test',
                        labels={'í‰ê· ì‹œê°„': t['avg_response_time'] + ' (' + t['seconds'] + ')'}
                    )
                    fig.update_layout(
                        height=400,
                        xaxis_title=t['testname'],
                        yaxis_title=t['response_time'] + ' (' + t['seconds'] + ')'
                    )
                    fig.update_xaxes(tickangle=45)
                    st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 4: ë²•ë ¹/ë¹„ë²•ë ¹ ë¶„ì„
    with tabs[3]:
        if 'law' not in filtered_df.columns:
            st.info("Law classification data not available.")
        else:
            st.header(f"âš–ï¸ {t['law_analysis']}")
            
            # ì „ì²´ ë²•ë ¹/ë¹„ë²•ë ¹ ë¹„ìœ¨
            st.subheader(t['law_ratio'])
            
            # ì¤‘ë³µ ì œê±°í•œ ë¬¸ì œë¡œ ê³„ì‚°
            unique_problems = filtered_df.drop_duplicates(subset=['Question', 'law'])
            law_count = len(unique_problems[unique_problems['law'] == 'O'])
            non_law_count = len(unique_problems[unique_problems['law'] != 'O'])
            total_unique = law_count + non_law_count
            
            col1, col2 = st.columns(2)
            
            with col1:
                # íŒŒì´ ì°¨íŠ¸
                fig = go.Figure(data=[go.Pie(
                    labels=[t['law'], t['non_law']],
                    values=[law_count, non_law_count],
                    hole=0.3,
                    marker=dict(line=dict(color='black', width=2))
                )])
                fig.update_layout(
                    title=t['law_distribution_stat'],
                    height=400
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # ìˆ˜ì¹˜ í‘œì‹œ
                st.metric(t['law_problems'], f"{law_count} ({law_count/total_unique*100:.1f}%)")
                st.metric(t['non_law_problems'], f"{non_law_count} ({non_law_count/total_unique*100:.1f}%)")
            
            # ëª¨ë¸ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì„±ëŠ¥
            st.markdown("---")
            st.subheader(t['model_law_performance'])
            
            law_performance = []
            for model in filtered_df['ëª¨ë¸'].unique():
                model_df = filtered_df[filtered_df['ëª¨ë¸'] == model]
                
                law_model = model_df[model_df['law'] == 'O']
                non_law_model = model_df[model_df['law'] != 'O']
                
                law_acc = (law_model['ì •ë‹µì—¬ë¶€'].sum() / len(law_model) * 100) if len(law_model) > 0 else 0
                non_law_acc = (non_law_model['ì •ë‹µì—¬ë¶€'].sum() / len(non_law_model) * 100) if len(non_law_model) > 0 else 0
                
                law_performance.append({
                    'ëª¨ë¸': model,
                    'ë²•ë ¹': law_acc,
                    'ë¹„ë²•ë ¹': non_law_acc
                })
            
            law_perf_df = pd.DataFrame(law_performance)
            
            # ê·¸ë£¹ ë°” ì°¨íŠ¸
            fig = go.Figure()
            fig.add_trace(go.Bar(
                name=t['law'],
                x=law_perf_df['ëª¨ë¸'],
                y=law_perf_df['ë²•ë ¹'],
                marker_color='skyblue'
            ))
            fig.add_trace(go.Bar(
                name=t['non_law'],
                x=law_perf_df['ëª¨ë¸'],
                y=law_perf_df['ë¹„ë²•ë ¹'],
                marker_color='lightcoral'
            ))
            
            fig.update_layout(
                barmode='group',
                title=t['law_distribution'],
                height=500,
                yaxis_title=t['accuracy'] + ' (%)',
                xaxis_title=t['model']
            )
            st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 5: ê³¼ëª©ë³„ ë¶„ì„
    with tabs[4]:
        if 'Subject' not in filtered_df.columns:
            st.info("Subject data not available.")
        else:
            st.header(f"ğŸ“š {t['subject_analysis']}")
            
            # ê³¼ëª©ë³„ ì„±ëŠ¥
            subject_stats = filtered_df.groupby('Subject').agg({
                'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
            }).reset_index()
            
            # ì»¬ëŸ¼ëª… ì–¸ì–´ë³„ ì„¤ì •
            if lang == 'ko':
                subject_stats.columns = ['ê³¼ëª©', 'ì •ë‹µ', 'ì´ë¬¸ì œ', 'ì •í™•ë„']
                subj_col = 'ê³¼ëª©'
                acc_col = 'ì •í™•ë„'
                correct_col = 'ì •ë‹µ'
                total_col = 'ì´ë¬¸ì œ'
            else:
                subject_stats.columns = ['Subject', 'Correct', 'Total', 'Accuracy']
                subj_col = 'Subject'
                acc_col = 'Accuracy'
                correct_col = 'Correct'
                total_col = 'Total'
            
            subject_stats[acc_col] = subject_stats[acc_col] * 100
            subject_stats = subject_stats.sort_values(acc_col, ascending=False)
            
            col1, col2 = st.columns([1, 2])
            
            with col1:
                # í…Œì´ë¸”
                st.dataframe(
                    subject_stats.style.format({acc_col: '{:.2f}%'})
                    .background_gradient(subset=[acc_col], cmap='RdYlGn'),
                    use_container_width=True
                )
            
            with col2:
                # ë°” ì°¨íŠ¸
                fig = px.bar(
                    subject_stats,
                    x=subj_col,
                    y=acc_col,
                    title=t['subject_performance'],
                    text=acc_col,
                    color=acc_col,
                    color_continuous_scale='RdYlGn',
                    labels={subj_col: t['by_subject'].replace('ë³„', ''), acc_col: t['accuracy'] + ' (%)'}
                )
                fig.update_traces(
                    texttemplate='%{text:.1f}%',
                    textposition='outside',
                    marker_line_color='black',
                    marker_line_width=1.5
                )
                fig.update_layout(
                    height=400,
                    showlegend=False,
                    yaxis_title=t['accuracy'] + ' (%)',
                    xaxis_title=t['by_subject'].replace('ë³„', '')
                )
                fig.update_xaxes(tickangle=45)
                st.plotly_chart(fig, use_container_width=True)
            
            # ëª¨ë¸ë³„ ê³¼ëª© ì„±ëŠ¥ íˆíŠ¸ë§µ (ì…€ ê²½ê³„ì„  ì¶”ê°€)
            st.markdown("---")
            subject_model = filtered_df.groupby(['ëª¨ë¸', 'Subject'])['ì •ë‹µì—¬ë¶€'].mean() * 100
            subject_model_pivot = subject_model.unstack(fill_value=0)
            
            fig = go.Figure(data=go.Heatmap(
                z=subject_model_pivot.values,
                x=subject_model_pivot.columns,
                y=subject_model_pivot.index,
                colorscale='RdYlGn',
                text=np.round(subject_model_pivot.values, 1),
                texttemplate='%{text:.1f}',
                textfont={"size": int(12 * chart_text_size)},
                colorbar=dict(title=t['accuracy'] + " (%)"),
                xgap=2,  # ì…€ ê²½ê³„ì„ 
                ygap=2
            ))
            fig.update_layout(height=400)
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 6: ì—°ë„ë³„ ë¶„ì„
    with tabs[5]:
        if 'Year' not in filtered_df.columns:
            st.info("Year data not available.")
        else:
            st.header(f"ğŸ“… {t['year_analysis']}")
            
            # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
            with st.expander("ğŸ” ë””ë²„ê¹… ì •ë³´ (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)"):
                st.write("**í•„í„°ë§ ì „ ì›ë³¸ ë°ì´í„°:**")
                st.write(f"- ì „ì²´ ë°ì´í„° í–‰ ìˆ˜: {len(results_df):,}")
                st.write(f"- ì›ë³¸ Year ê³ ìœ ê°’: {sorted([str(y) for y in results_df['Year'].dropna().unique().tolist()])}")
                
                st.write("**í•„í„°ë§ í›„ ë°ì´í„°:**")
                st.write(f"- í•„í„°ë§ëœ ë°ì´í„° í–‰ ìˆ˜: {len(filtered_df):,}")
                st.write(f"- í•„í„°ë§ëœ Year ê³ ìœ ê°’: {sorted([str(y) for y in filtered_df['Year'].dropna().unique().tolist()])}")
                
                st.write("**í˜„ì¬ í•„í„° ì„¤ì •:**")
                st.write(f"- ì„ íƒëœ í…ŒìŠ¤íŠ¸: {selected_tests}")
                st.write(f"- ì„ íƒëœ ëª¨ë¸: {selected_models}")
                st.write(f"- ì„ íƒëœ ì—°ë„: {selected_years if 'selected_years' in locals() else 'ì „ì²´'}")
            
            # Yearë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
            filtered_df['Year_Int'] = filtered_df['Year'].apply(safe_convert_to_int)
            year_df = filtered_df[filtered_df['Year_Int'].notna()].copy()
            
            if not year_df.empty:
                # ì—°ë„ë³„ ì„±ëŠ¥
                year_stats = year_df.groupby('Year_Int').agg({
                    'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
                }).reset_index()
                year_stats.columns = ['ì—°ë„', 'ì •ë‹µ', 'ì´ë¬¸ì œ', 'ì •í™•ë„']
                year_stats['ì •í™•ë„'] = year_stats['ì •í™•ë„'] * 100
                year_stats = year_stats.sort_values('ì—°ë„')
                
                # ì—°ë„ë¥¼ ì •ìˆ˜ë¡œ í‘œì‹œ
                year_stats['ì—°ë„'] = year_stats['ì—°ë„'].astype(int)
                
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    # í…Œì´ë¸” (ì†Œìˆ˜ì  ì—†ì´ í‘œì‹œ)
                    st.dataframe(
                        year_stats.style.format({
                            'ì—°ë„': '{:.0f}',
                            'ì •ë‹µ': '{:.0f}',
                            'ì´ë¬¸ì œ': '{:.0f}',
                            'ì •í™•ë„': '{:.2f}%'
                        })
                        .background_gradient(subset=['ì •í™•ë„'], cmap='RdYlGn'),
                        use_container_width=True
                    )
                
                with col2:
                    # ë¼ì¸ ì°¨íŠ¸
                    fig = px.line(
                        year_stats,
                        x='ì—°ë„',
                        y='ì •í™•ë„',
                        title=t['year_performance'],
                        markers=True,
                        text='ì •í™•ë„'
                    )
                    fig.update_traces(
                        texttemplate='%{text:.1f}%',
                        textposition='top center',
                        marker_size=10,
                        marker_line_color='black',
                        marker_line_width=2,
                        line_width=3
                    )
                    fig.update_layout(
                        height=400,
                        yaxis_title=t['accuracy'] + ' (%)',
                        xaxis_title=t['year']
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # ì—°ë„ë³„ ë¬¸ì œ ìˆ˜ ì°¨íŠ¸ ì¶”ê°€
                st.markdown("---")
                st.subheader(f"ğŸ“Š {t['year_problem_distribution']}")
                
                # ë‹¤êµ­ì–´ ì»¬ëŸ¼ëª… ì„¤ì •
                year_col = t['year']
                count_col = t['problem_count']
                
                # í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ ì‹¤ì œ ë¬¸ì œ ìˆ˜ ê³„ì‚° (ì¤‘ë³µ ì œê±°)
                if selected_tests:
                    year_problem_count = []
                    for test_name in selected_tests:
                        if test_name in testsets and 'Year' in testsets[test_name].columns:
                            test_year_counts = testsets[test_name].groupby('Year').size()
                            for year, count in test_year_counts.items():
                                year_int = safe_convert_to_int(year)
                                if year_int:
                                    year_problem_count.append({year_col: year_int, count_col: count})
                    
                    if year_problem_count:
                        year_problem_df = pd.DataFrame(year_problem_count)
                        year_problem_df = year_problem_df.groupby(year_col)[count_col].sum().reset_index()
                        year_problem_df = year_problem_df.sort_values(year_col)
                    else:
                        # ë°±ì—…: filtered_dfì—ì„œ ê³ ìœ  ë¬¸ì œ ìˆ˜ ê³„ì‚°
                        year_problem_df = year_df.groupby('Year_Int')['Question'].nunique().reset_index()
                        year_problem_df.columns = [year_col, count_col]
                        year_problem_df[year_col] = year_problem_df[year_col].astype(int)
                        year_problem_df = year_problem_df.sort_values(year_col)
                else:
                    # í…ŒìŠ¤íŠ¸ ì„ íƒ ì•ˆ ë¨: filtered_dfì—ì„œ ê³„ì‚°
                    year_problem_df = year_df.groupby('Year_Int')['Question'].nunique().reset_index()
                    year_problem_df.columns = [year_col, count_col]
                    year_problem_df[year_col] = year_problem_df[year_col].astype(int)
                    year_problem_df = year_problem_df.sort_values(year_col)
                
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    # ì—°ë„ë³„ ë¬¸ì œ ìˆ˜ í…Œì´ë¸”
                    st.dataframe(
                        year_problem_df.style.format({
                            year_col: '{:.0f}',
                            count_col: '{:.0f}'
                        })
                        .background_gradient(subset=[count_col], cmap='Blues'),
                        use_container_width=True
                    )
                    
                    # ì´ ë¬¸ì œ ìˆ˜ í‘œì‹œ
                    st.metric(t['total_problem_count'], f"{year_problem_df[count_col].sum():,.0f}" + (t['problems'] if lang == 'ko' else ''))
                
                with col2:
                    # ë°” ì°¨íŠ¸
                    fig = px.bar(
                        year_problem_df,
                        x=year_col,
                        y=count_col,
                        title=t['year_problem_chart'],
                        text=count_col,
                        color=count_col,
                        color_continuous_scale='Blues'
                    )
                    fig.update_traces(
                texttemplate='%{text}',
                textposition='outside',
                marker_line_color='black',
                marker_line_width=1.5
            )
                    fig.update_layout(
                        height=400,
                        showlegend=False,
                        yaxis_title=t['problem_count'],
                        xaxis_title=t['year'],
                        xaxis=dict(tickmode='linear')
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # ëª¨ë¸ë³„ ì—°ë„ ì„±ëŠ¥ íˆíŠ¸ë§µ
                st.markdown("---")
                year_model = year_df.groupby(['ëª¨ë¸', 'Year_Int'])['ì •ë‹µì—¬ë¶€'].mean() * 100
                year_model_pivot = year_model.unstack(fill_value=0)
                
                # ì»¬ëŸ¼ëª…ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                year_model_pivot.columns = year_model_pivot.columns.astype(int)
                
                fig = go.Figure(data=go.Heatmap(
                    z=year_model_pivot.values,
                    x=year_model_pivot.columns,
                    y=year_model_pivot.index,
                    colorscale='RdYlGn',
                    text=np.round(year_model_pivot.values, 1),
                    texttemplate='%{text:.1f}',
                    textfont={"size": int(12 * chart_text_size)},
                    colorbar=dict(title=t['accuracy'] + " (%)"),
                    xgap=2,  # ì…€ ê²½ê³„ì„ 
                    ygap=2
                ))
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("ì—°ë„ ì •ë³´ê°€ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # íƒ­ 7: ì˜¤ë‹µ ë¶„ì„
    with tabs[6]:
        st.header(f"âŒ {t['incorrect_analysis']}")
        
        # ë¬¸ì œë³„ ì˜¤ë‹µë¥  ê³„ì‚°
        problem_analysis = filtered_df.groupby('Question').agg({
            'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
        }).reset_index()
        problem_analysis.columns = ['Question', 'correct_count', 'total_count', 'correct_rate']
        problem_analysis['incorrect_rate'] = 1 - problem_analysis['correct_rate']
        problem_analysis['incorrect_count'] = problem_analysis['total_count'] - problem_analysis['correct_count']
        
        # ë¬¸ì œ ì‹ë³„ì ì¶”ê°€
        problem_ids = []
        for question in problem_analysis['Question']:
            matching_rows = filtered_df[filtered_df['Question'] == question]
            if len(matching_rows) > 0:
                problem_id = create_problem_identifier(matching_rows.iloc[0], lang)
                problem_ids.append(problem_id)
            else:
                problem_ids.append("Unknown")
        
        problem_analysis['problem_id'] = problem_ids
        
        # ì˜¤ë‹µë¥  ìˆœìœ¼ë¡œ ì •ë ¬ (ë™ì¼í•œ ì˜¤ë‹µë¥ ì´ë©´ ë¬¸ì œ IDë¡œ ì •ë ¬)
        problem_analysis = problem_analysis.sort_values(
            by=['incorrect_rate', 'problem_id'],
            ascending=[False, True]
        )
        
        # ì‹œë„í•œ ëª¨ë¸ ëª©ë¡ ì¶”ê°€
        attempted_models = []
        for question in problem_analysis['Question']:
            models = filtered_df[filtered_df['Question'] == question]['ëª¨ë¸'].unique().tolist()
            attempted_models.append(', '.join(sorted(models)))
        
        problem_analysis['attempted_models'] = attempted_models
        
        # ëª¨ë¸ë³„ ì •ì˜¤ë‹µ ì •ë³´ ì¶”ê°€
        correct_models_list = []
        incorrect_models_list = []
        
        for question in problem_analysis['Question']:
            q_df = filtered_df[filtered_df['Question'] == question]
            correct_models = q_df[q_df['ì •ë‹µì—¬ë¶€'] == True]['ëª¨ë¸'].unique().tolist()
            incorrect_models = q_df[q_df['ì •ë‹µì—¬ë¶€'] == False]['ëª¨ë¸'].unique().tolist()
            
            correct_models_list.append('âœ“ ' + ', '.join(sorted(correct_models)) if correct_models else '-')
            incorrect_models_list.append('âœ— ' + ', '.join(sorted(incorrect_models)) if incorrect_models else '-')
        
        problem_analysis['correct_models'] = correct_models_list
        problem_analysis['incorrect_models'] = incorrect_models_list
        
        # Top 20 ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ
        st.subheader(t['top_incorrect'])
        
        top_20 = problem_analysis.head(20)
        
        # ë””ìŠ¤í”Œë ˆì´ìš© ë°ì´í„°í”„ë ˆì„
        display_top_20 = pd.DataFrame({
            t['problem_id']: top_20['problem_id'],
            t['incorrect_count']: top_20['incorrect_count'].astype(int),
            t['correct_count']: top_20['correct_count'].astype(int),
            t['total_models']: top_20['total_count'].astype(int),
            t['wrong_rate']: (top_20['incorrect_rate'] * 100).round(2),
            'ì •ë‹µ ëª¨ë¸' if lang == 'ko' else 'Correct Models': top_20['correct_models'],
            'ì˜¤ë‹µ ëª¨ë¸' if lang == 'ko' else 'Incorrect Models': top_20['incorrect_models']
        })
        
        st.dataframe(
            display_top_20.style.background_gradient(
                subset=[t['wrong_rate']],
                cmap='Reds',
                vmin=0,
                vmax=100
            ),
            use_container_width=True,
            height=600
        )
        
        # ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ
        st.markdown("---")
        st.subheader(t['all_models_incorrect'])
        
        all_wrong = problem_analysis[problem_analysis['correct_count'] == 0]
        
        if len(all_wrong) > 0:
            display_all_wrong = pd.DataFrame({
                t['problem_id']: all_wrong['problem_id'],
                t['incorrect_count']: all_wrong['incorrect_count'].astype(int),
                t['correct_count']: all_wrong['correct_count'].astype(int),
                t['total_models']: all_wrong['total_count'].astype(int),
                'ì˜¤ë‹µ ëª¨ë¸' if lang == 'ko' else 'Incorrect Models': all_wrong['incorrect_models']
            })
            
            st.dataframe(display_all_wrong, use_container_width=True)
            
            # ë¬¸ì œ ìƒì„¸ ë³´ê¸° ì˜µì…˜ - ëª¨ë“  ë¬¸ì œ í‘œì‹œ
            if st.checkbox('ë¬¸ì œ ë‚´ìš© ë³´ê¸°' if lang == 'ko' else 'Show Question Details'):
                st.info(f"ì´ {len(all_wrong)}ê°œ ë¬¸ì œì˜ ìƒì„¸ ë‚´ìš©ì„ í‘œì‹œí•©ë‹ˆë‹¤." if lang == 'ko' else f"Showing details for all {len(all_wrong)} problems.")
                for idx, row in all_wrong.iterrows():
                    with st.expander(f"{row['problem_id']}"):
                        q_detail = filtered_df[filtered_df['Question'] == row['Question']].iloc[0]
                        st.write(f"**{t['question']}:** {q_detail['Question']}")
                        if 'Subject' in q_detail and pd.notna(q_detail['Subject']):
                            st.write(f"**ê³¼ëª©/Subject:** {q_detail['Subject']}")
                        
                        # ì„ íƒì§€ í‘œì‹œ
                        if all(['Option 1' in q_detail, 'Option 2' in q_detail, 'Option 3' in q_detail, 'Option 4' in q_detail]):
                            st.write("**ì„ íƒì§€/Options:**")
                            for i in range(1, 5):
                                option_key = f'Option {i}'
                                if option_key in q_detail and pd.notna(q_detail[option_key]):
                                    st.write(f"  {i}. {q_detail[option_key]}")
                        
                        # ì •ë‹µ í‘œì‹œ
                        if 'Answer' in q_detail and pd.notna(q_detail['Answer']):
                            st.write(f"**ì •ë‹µ/Answer:** {q_detail['Answer']}")
                        
                        st.write(f"**ì˜¤ë‹µ ëª¨ë¸/Incorrect Models:** {row['incorrect_models']}")
        else:
            st.info("No problems that all models got wrong.")
        
        # ëŒ€ë¶€ë¶„ ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ (50% ì´ìƒ)
        st.markdown("---")
        st.subheader(t['most_models_incorrect'])
        
        most_wrong = problem_analysis[problem_analysis['incorrect_rate'] >= 0.5]
        
        if len(most_wrong) > 0:
            display_most_wrong = pd.DataFrame({
                t['problem_id']: most_wrong['problem_id'],
                t['incorrect_count']: most_wrong['incorrect_count'].astype(int),
                t['correct_count']: most_wrong['correct_count'].astype(int),
                t['total_models']: most_wrong['total_count'].astype(int),
                t['wrong_rate']: (most_wrong['incorrect_rate'] * 100).round(2),
                'ì •ë‹µ ëª¨ë¸' if lang == 'ko' else 'Correct Models': most_wrong['correct_models'],
                'ì˜¤ë‹µ ëª¨ë¸' if lang == 'ko' else 'Incorrect Models': most_wrong['incorrect_models']
            })
            
            st.dataframe(
                display_most_wrong.style.background_gradient(
                    subset=[t['wrong_rate']],
                    cmap='Reds',
                    vmin=0,
                    vmax=100
                ),
                use_container_width=True
            )
            
            # ë¬¸ì œ ìƒì„¸ ë³´ê¸° ì˜µì…˜ - ëª¨ë“  ë¬¸ì œ í‘œì‹œ
            if st.checkbox('ë¬¸ì œ ë‚´ìš© ë³´ê¸° (ëŒ€ë¶€ë¶„ í‹€ë¦° ë¬¸ì œ)' if lang == 'ko' else 'Show Question Details (Most Incorrect)', key='most_wrong_details'):
                st.info(f"ì´ {len(most_wrong)}ê°œ ë¬¸ì œì˜ ìƒì„¸ ë‚´ìš©ì„ í‘œì‹œí•©ë‹ˆë‹¤." if lang == 'ko' else f"Showing details for all {len(most_wrong)} problems.")
                for idx, row in most_wrong.iterrows():  # ëª¨ë“  ë¬¸ì œ í‘œì‹œ
                    with st.expander(f"{row['problem_id']} - ì˜¤ë‹µë¥  {row['incorrect_rate']*100:.1f}%"):
                        q_detail = filtered_df[filtered_df['Question'] == row['Question']].iloc[0]
                        st.write(f"**{t['question']}:** {q_detail['Question']}")
                        if 'Subject' in q_detail and pd.notna(q_detail['Subject']):
                            st.write(f"**ê³¼ëª©/Subject:** {q_detail['Subject']}")
                        
                        # ì„ íƒì§€ í‘œì‹œ
                        if all(['Option 1' in q_detail, 'Option 2' in q_detail, 'Option 3' in q_detail, 'Option 4' in q_detail]):
                            st.write("**ì„ íƒì§€/Options:**")
                            for i in range(1, 5):
                                option_key = f'Option {i}'
                                if option_key in q_detail and pd.notna(q_detail[option_key]):
                                    st.write(f"  {i}. {q_detail[option_key]}")
                        
                        # ì •ë‹µ í‘œì‹œ
                        if 'Answer' in q_detail and pd.notna(q_detail['Answer']):
                            st.write(f"**ì •ë‹µ/Answer:** {q_detail['Answer']}")
                        
                        st.write(f"**âœ“ ì •ë‹µ ëª¨ë¸/Correct Models:** {row['correct_models']}")
                        st.write(f"**âœ— ì˜¤ë‹µ ëª¨ë¸/Incorrect Models:** {row['incorrect_models']}")
        else:
            st.info("No problems that most models got wrong.")
        
        # Top 10 ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ ì°¨íŠ¸
        st.markdown("---")
        top_10_chart = top_20.head(10)
        
        fig = px.bar(
            top_10_chart,
            x='problem_id',
            y='incorrect_rate',
            title='ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ Top 10' if lang == 'ko' else 'Top 10 Problems by Incorrect Rate',
            text=[f"{x:.0%}" for x in top_10_chart['incorrect_rate']],
            color='incorrect_rate',
            color_continuous_scale='Reds',
            range_color=[0, 1]  # ì»¬ëŸ¬ë°” ë²”ìœ„ë¥¼ 0~1ë¡œ ê³ ì •
        )
        fig.update_traces(textposition='outside')
        fig.update_layout(
            height=500,
            showlegend=False,
            yaxis_title=t['wrong_rate'],
            xaxis_title=t['problem_id'],
            yaxis=dict(range=[0, 1])  # yì¶• ë²”ìœ„ë¥¼ 0~1ë¡œ ê³ ì •
        )
        fig.update_xaxes(tickangle=45)
        st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 8: ë‚œì´ë„ ë¶„ì„
    with tabs[7]:
        st.header(f"ğŸ“ˆ {t['difficulty_analysis']}")
        
        # ë¬¸ì œë³„ ë‚œì´ë„ ê³„ì‚° (ì •ë‹µë¥  ê¸°ë°˜)
        difficulty = filtered_df.groupby('Question').agg({
            'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
        }).reset_index()
        difficulty.columns = ['Question', 'correct_count', 'total_count', 'difficulty_score']
        difficulty['difficulty_score'] = difficulty['difficulty_score'] * 100
        
        # ë‚œì´ë„ êµ¬ê°„ ë¶„ë¥˜
        def classify_difficulty(score, lang='ko'):
            if lang == 'ko':
                if score < 20:
                    return 'ë§¤ìš° ì–´ë ¤ì›€ (0-20%)'
                elif score < 40:
                    return 'ì–´ë ¤ì›€ (20-40%)'
                elif score < 60:
                    return 'ë³´í†µ (40-60%)'
                elif score < 80:
                    return 'ì‰¬ì›€ (60-80%)'
                else:
                    return 'ë§¤ìš° ì‰¬ì›€ (80-100%)'
            else:  # English
                if score < 20:
                    return 'Very Hard (0-20%)'
                elif score < 40:
                    return 'Hard (20-40%)'
                elif score < 60:
                    return 'Medium (40-60%)'
                elif score < 80:
                    return 'Easy (60-80%)'
                else:
                    return 'Very Easy (80-100%)'
        
        difficulty['ë‚œì´ë„_êµ¬ê°„'] = difficulty['difficulty_score'].apply(lambda x: classify_difficulty(x, lang))
        
        # ë‚œì´ë„ êµ¬ê°„ ìˆœì„œ ì •ì˜ (ì–´ë ¤ìš´ ê²ƒë¶€í„° ì‰¬ìš´ ê²ƒ ìˆœ)
        if lang == 'ko':
            difficulty_order = [
                'ë§¤ìš° ì–´ë ¤ì›€ (0-20%)',
                'ì–´ë ¤ì›€ (20-40%)',
                'ë³´í†µ (40-60%)',
                'ì‰¬ì›€ (60-80%)',
                'ë§¤ìš° ì‰¬ì›€ (80-100%)'
            ]
        else:
            difficulty_order = [
                'Very Hard (0-20%)',
                'Hard (20-40%)',
                'Medium (40-60%)',
                'Easy (60-80%)',
                'Very Easy (80-100%)'
            ]
        difficulty['ë‚œì´ë„_êµ¬ê°„'] = pd.Categorical(difficulty['ë‚œì´ë„_êµ¬ê°„'], categories=difficulty_order, ordered=True)
        
        # ì›ë³¸ ë°ì´í„°ì— ë‚œì´ë„ ì •ë³´ ë³‘í•©
        analysis_df = filtered_df.merge(difficulty[['Question', 'difficulty_score', 'ë‚œì´ë„_êµ¬ê°„']], on='Question')
        
        # analysis_dfì—ë„ ë™ì¼í•œ ìˆœì„œ ì ìš©
        analysis_df['ë‚œì´ë„_êµ¬ê°„'] = pd.Categorical(analysis_df['ë‚œì´ë„_êµ¬ê°„'], categories=difficulty_order, ordered=True)
        
        # 1. ë‚œì´ë„ ë¶„í¬
        st.subheader("ğŸ“ˆ " + (t['problem_distribution'] if 'problem_distribution' in t else ('ë¬¸ì œ ë‚œì´ë„ ë¶„í¬' if lang == 'ko' else 'Problem Difficulty Distribution')))
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ë‚œì´ë„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
            fig = px.histogram(
                difficulty,
                x='difficulty_score',
                nbins=20,
                title=t['difficulty_score'] + ' Distribution',
                labels={'difficulty_score': t['difficulty_score'], 'count': t['problem_count']}
            )
            fig.update_traces(
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # ë‚œì´ë„ êµ¬ê°„ë³„ ë¬¸ì œ ìˆ˜
            difficulty_dist = difficulty['ë‚œì´ë„_êµ¬ê°„'].value_counts()
            # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬
            difficulty_dist = difficulty_dist.reindex(difficulty_order, fill_value=0)
            
            fig = px.bar(
                x=difficulty_dist.index,
                y=difficulty_dist.values,
                title=t['problem_count'] + (' by ' + t['difficulty_range'] if lang == 'en' else ' (' + t['difficulty_range'] + 'ë³„)'),
                labels={'x': t['difficulty_range'], 'y': t['problem_count']},
                text=difficulty_dist.values,
                color=difficulty_dist.values,
                color_continuous_scale='RdYlGn_r'
            )
            fig.update_traces(
                texttemplate='%{text}',
                textposition='outside',
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_layout(
                height=400,
                showlegend=False
            )
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
        
        # í†µê³„ ìš”ì•½
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric(
                t['correct_rate'] if lang == 'ko' else 'Average Correct Rate',
                f"{difficulty['difficulty_score'].mean():.1f}%"
            )
        with col2:
            st.metric(
                'ì¤‘ì•™ê°’' if lang == 'ko' else 'Median',
                f"{difficulty['difficulty_score'].median():.1f}%"
            )
        with col3:
            very_hard_label = difficulty_order[0]
            very_hard = len(difficulty[difficulty['ë‚œì´ë„_êµ¬ê°„'] == very_hard_label])
            st.metric(
                t['very_hard'] + (' ë¬¸ì œ' if lang == 'ko' else ' Problems'),
                f"{very_hard}" + (t['problems'] if lang == 'ko' else '')
            )
        with col4:
            very_easy_label = difficulty_order[-1]
            very_easy = len(difficulty[difficulty['ë‚œì´ë„_êµ¬ê°„'] == very_easy_label])
            st.metric(
                t['very_easy'] + (' ë¬¸ì œ' if lang == 'ko' else ' Problems'),
                f"{very_easy}" + (t['problems'] if lang == 'ko' else '')
            )
        
        st.markdown("---")
        
        # 2. ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥
        st.subheader("ğŸ¯ " + ('ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥' if lang == 'ko' else 'Model Performance by Difficulty Level'))
        
        # ëª¨ë¸ë³„ ë‚œì´ë„ êµ¬ê°„ë³„ ì •ë‹µë¥ 
        model_difficulty = analysis_df.groupby(['ëª¨ë¸', 'ë‚œì´ë„_êµ¬ê°„']).agg({
            'ì •ë‹µì—¬ë¶€': ['mean', 'count']
        }).reset_index()
        
        # ì»¬ëŸ¼ëª… ì–¸ì–´ë³„ ì„¤ì •
        if lang == 'ko':
            model_difficulty.columns = ['ëª¨ë¸', 'ë‚œì´ë„_êµ¬ê°„', 'ì •ë‹µë¥ ', 'ë¬¸ì œìˆ˜']
        else:
            model_difficulty.columns = ['Model', 'Difficulty', 'Correct Rate', 'Problem Count']
        
        # ì •ë‹µë¥  ì»¬ëŸ¼ëª… (ì–¸ì–´ë³„)
        acc_col = 'ì •ë‹µë¥ ' if lang == 'ko' else 'Correct Rate'
        model_col = 'ëª¨ë¸' if lang == 'ko' else 'Model'
        diff_col = 'ë‚œì´ë„_êµ¬ê°„' if lang == 'ko' else 'Difficulty'
        
        model_difficulty[acc_col] = model_difficulty[acc_col] * 100
        
        # ë¼ì¸ ì°¨íŠ¸
        fig = px.line(
            model_difficulty,
            x=diff_col,
            y=acc_col,
            color=model_col,
            markers=True,
            title='ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ' if lang == 'ko' else 'Model Performance by Difficulty Level',
            labels={
                acc_col: t['accuracy'] + ' (%)',
                diff_col: t['difficulty_range'],
                model_col: t['model']
            },
            category_orders={diff_col: difficulty_order}
        )
        fig.update_traces(
            marker_size=10,
            marker_line_color='black',
            marker_line_width=2,
            line_width=3
        )
        fig.update_layout(height=500)
        fig.update_xaxes(tickangle=45)
        st.plotly_chart(fig, use_container_width=True)
        
        # íˆíŠ¸ë§µ
        pivot_difficulty = model_difficulty.pivot(
            index=model_col,
            columns=diff_col,
            values=acc_col
        )
        
        # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬
        pivot_difficulty = pivot_difficulty.reindex(columns=difficulty_order)
        
        fig = go.Figure(data=go.Heatmap(
            z=pivot_difficulty.values,
            x=pivot_difficulty.columns,
            y=pivot_difficulty.index,
            colorscale='RdYlGn',
            text=np.round(pivot_difficulty.values, 1),
            texttemplate='%{text:.1f}',
            textfont={"size": int(12 * chart_text_size)},
            colorbar=dict(title=t['accuracy'] + " (%)"),
            xgap=2,  # ì…€ ê²½ê³„ì„ 
            ygap=2
        ))
        fig.update_layout(
            height=400,
            title='ëª¨ë¸ Ã— ë‚œì´ë„ íˆíŠ¸ë§µ' if lang == 'ko' else 'Model Ã— Difficulty Heatmap',
            xaxis_title=t['difficulty_range'],
            yaxis_title=t['model']
        )
        fig.update_xaxes(tickangle=45)
        st.plotly_chart(fig, use_container_width=True)
        
        st.markdown("---")
        
        # 3. ê³¼ëª©ë³„ ë‚œì´ë„ ë¶„ì„
        if 'Subject' in analysis_df.columns:
            st.subheader("ğŸ“š " + ('ê³¼ëª©ë³„ ë‚œì´ë„ ë¶„ì„' if lang == 'ko' else 'Difficulty Analysis by Subject'))
            
            subject_difficulty = analysis_df.groupby('Subject').agg({
                'difficulty_score': 'mean',
                'Question': 'count'
            }).reset_index()
            
            # ì»¬ëŸ¼ëª… ì–¸ì–´ë³„ ì„¤ì •
            if lang == 'ko':
                subject_difficulty.columns = ['ê³¼ëª©', 'í‰ê· _ë‚œì´ë„', 'ë¬¸ì œìˆ˜']
                subj_col = 'ê³¼ëª©'
                avg_diff_col = 'í‰ê· _ë‚œì´ë„'
            else:
                subject_difficulty.columns = ['Subject', 'Avg Difficulty', 'Problem Count']
                subj_col = 'Subject'
                avg_diff_col = 'Avg Difficulty'
            
            subject_difficulty = subject_difficulty.sort_values(avg_diff_col)
            
            fig = px.bar(
                subject_difficulty,
                x=subj_col,
                y=avg_diff_col,
                title='ê³¼ëª©ë³„ í‰ê·  ë‚œì´ë„ (ì •ë‹µë¥ )' if lang == 'ko' else 'Average Difficulty by Subject (Correct Rate)',
                text=avg_diff_col,
                color=avg_diff_col,
                color_continuous_scale='RdYlGn',
                labels={subj_col: t['by_subject'].replace('ë³„', ''), avg_diff_col: t['avg_difficulty']}
            )
            fig.update_traces(
                texttemplate='%{text:.1f}%',
                textposition='outside',
                marker_line_color='black',
                marker_line_width=1.5
            )
            fig.update_xaxes(tickangle=45)
            fig.update_layout(
                height=500,
                showlegend=False
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # ê³¼ëª© Ã— ë‚œì´ë„ êµ¬ê°„ íˆíŠ¸ë§µ
            subject_diff_dist = analysis_df.groupby(['Subject', 'ë‚œì´ë„_êµ¬ê°„']).size().reset_index(name='ë¬¸ì œìˆ˜')
            pivot_subject_diff = subject_diff_dist.pivot(
                index='Subject',
                columns='ë‚œì´ë„_êµ¬ê°„',
                values='ë¬¸ì œìˆ˜'
            ).fillna(0)
            
            # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬
            pivot_subject_diff = pivot_subject_diff.reindex(columns=difficulty_order, fill_value=0)
            
            fig = go.Figure(data=go.Heatmap(
                z=pivot_subject_diff.values,
                x=pivot_subject_diff.columns,
                y=pivot_subject_diff.index,
                colorscale='Blues',
                text=pivot_subject_diff.values.astype(int),
                texttemplate='%{text}',
                textfont={"size": int(12 * chart_text_size)},
                colorbar=dict(title=t['problem_count']),
                xgap=2,  # ì…€ ê²½ê³„ì„ 
                ygap=2
            ))
            fig.update_layout(
                height=500,
                title='ê³¼ëª© Ã— ë‚œì´ë„ ë¶„í¬' if lang == 'ko' else 'Subject Ã— Difficulty Distribution',
                xaxis_title=t['difficulty_range'],
                yaxis_title=t['by_subject'].replace('ë³„', '')  # 'ê³¼ëª©' or 'Subject'
            )
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
        
        st.markdown("---")
        
        # 4. ì–´ë ¤ìš´ ë¬¸ì œ vs ì‰¬ìš´ ë¬¸ì œ ìƒì„¸ ë¶„ì„
        st.subheader("ğŸ” " + (
            "ì–´ë ¤ìš´ ë¬¸ì œ vs ì‰¬ìš´ ë¬¸ì œ ë¹„êµ" if lang == 'ko' else "Hard vs Easy Problems Comparison"
        ))
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### " + (
                "ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œ (ì •ë‹µë¥  < 20%)" if lang == 'ko' else "Very Hard Problems (Correct Rate < 20%)"
            ))
            very_hard_problems = difficulty[difficulty['difficulty_score'] < 20].sort_values('difficulty_score')
            
            if len(very_hard_problems) > 0:
                st.metric(
                    t['problem_count'],
                    f"{len(very_hard_problems)}" + (t['problems'] if lang == 'ko' else '')
                )
                st.metric(
                    'í‰ê·  ì •ë‹µë¥ ' if lang == 'ko' else 'Average Correct Rate',
                    f"{very_hard_problems['difficulty_score'].mean():.1f}%"
                )
                
                # ëª¨ë¸ë³„ ì„±ëŠ¥
                very_hard_questions = very_hard_problems['Question'].tolist()
                very_hard_model_perf = filtered_df[filtered_df['Question'].isin(very_hard_questions)].groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean() * 100
                
                st.markdown("**" + (
                    "ëª¨ë¸ë³„ ì„±ëŠ¥" if lang == 'ko' else "Performance by Model"
                ) + "**")
                for model, acc in very_hard_model_perf.sort_values(ascending=False).items():
                    st.write(f"- {model}: {acc:.1f}%")
            else:
                st.info(
                    "ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No very hard problems found."
                )
        
        with col2:
            st.markdown("#### " + (
                "ë§¤ìš° ì‰¬ìš´ ë¬¸ì œ (ì •ë‹µë¥  > 80%)" if lang == 'ko' else "Very Easy Problems (Correct Rate > 80%)"
            ))
            very_easy_problems = difficulty[difficulty['difficulty_score'] > 80].sort_values('difficulty_score', ascending=False)
            
            if len(very_easy_problems) > 0:
                st.metric(
                    t['problem_count'],
                    f"{len(very_easy_problems)}" + (t['problems'] if lang == 'ko' else '')
                )
                st.metric(
                    'í‰ê·  ì •ë‹µë¥ ' if lang == 'ko' else 'Average Correct Rate',
                    f"{very_easy_problems['difficulty_score'].mean():.1f}%"
                )
                
                # ëª¨ë¸ë³„ ì„±ëŠ¥
                very_easy_questions = very_easy_problems['Question'].tolist()
                very_easy_model_perf = filtered_df[filtered_df['Question'].isin(very_easy_questions)].groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean() * 100
                
                st.markdown("**" + (
                    "ëª¨ë¸ë³„ ì„±ëŠ¥" if lang == 'ko' else "Performance by Model"
                ) + "**")
                for model, acc in very_easy_model_perf.sort_values(ascending=False).items():
                    st.write(f"- {model}: {acc:.1f}%")
            else:
                st.info(
                    "ë§¤ìš° ì‰¬ìš´ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤." if lang == 'ko' else "No very easy problems found."
                )
        
        st.markdown("---")
        
        # 5. ë‚œì´ë„ êµ¬ê°„ë³„ ìƒì„¸ í…Œì´ë¸”
        st.subheader("ğŸ“‹ " + t['difficulty_stats_by_range'])
        
        detailed_difficulty = model_difficulty.pivot_table(
            index=model_col,
            columns=diff_col,
            values=acc_col,
            aggfunc='mean'
        ).round(2)
        
        # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬
        detailed_difficulty = detailed_difficulty.reindex(columns=difficulty_order)
        
        st.dataframe(
            detailed_difficulty.style.background_gradient(cmap='RdYlGn', axis=None),
            use_container_width=True
        )
    
    # íƒ­ 9: í† í° ë° ë¹„ìš© ë¶„ì„
    with tabs[8]:
        st.header(f"ğŸ’° {t['token_cost_analysis']}")
        
        # í† í° ê´€ë ¨ ì»¬ëŸ¼ í™•ì¸
        token_columns = {
            'input': ['ì…ë ¥í† í°', 'input_tokens', 'Input Tokens'],
            'output': ['ì¶œë ¥í† í°', 'output_tokens', 'Output Tokens'],
            'total': ['ì´í† í°', 'total_tokens', 'Total Tokens'],
            'cost': ['ë¹„ìš©ìˆ˜ì¤€', 'cost_level', 'Cost Level']
        }
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ ì°¾ê¸°
        available_cols = {}
        for key, possible_names in token_columns.items():
            for col_name in possible_names:
                if col_name in filtered_df.columns:
                    available_cols[key] = col_name
                    break
        
        if not available_cols:
            st.info("Token usage data not available in the dataset." if lang == 'en' else "í† í° ì‚¬ìš©ëŸ‰ ë°ì´í„°ê°€ ë°ì´í„°ì…‹ì— ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ë°ì´í„° ì¤€ë¹„
            token_df = filtered_df.copy()
            
            # NaN ì œê±°
            for key, col in available_cols.items():
                if col in token_df.columns:
                    token_df = token_df[token_df[col].notna()]
            
            if len(token_df) == 0:
                st.info("No valid token data available after filtering." if lang == 'en' else "í•„í„°ë§ í›„ ìœ íš¨í•œ í† í° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                # 1. í† í° í†µê³„ ìš”ì•½
                st.subheader(f"ğŸ“Š {t['token_stats']}")
                
                # ëª¨ë¸ë³„ í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
                agg_dict = {}
                if 'input' in available_cols:
                    agg_dict[available_cols['input']] = ['sum', 'mean']
                if 'output' in available_cols:
                    agg_dict[available_cols['output']] = ['sum', 'mean']
                if 'total' in available_cols:
                    agg_dict[available_cols['total']] = ['sum', 'mean']
                
                model_token_stats = token_df.groupby('ëª¨ë¸').agg(agg_dict).reset_index()
                
                # ì»¬ëŸ¼ëª… ì •ë¦¬
                new_cols = ['ëª¨ë¸']
                for col in model_token_stats.columns[1:]:
                    if col[0] == available_cols.get('input', ''):
                        if col[1] == 'sum':
                            new_cols.append('ì´_ì…ë ¥í† í°')
                        else:
                            new_cols.append('í‰ê· _ì…ë ¥í† í°')
                    elif col[0] == available_cols.get('output', ''):
                        if col[1] == 'sum':
                            new_cols.append('ì´_ì¶œë ¥í† í°')
                        else:
                            new_cols.append('í‰ê· _ì¶œë ¥í† í°')
                    elif col[0] == available_cols.get('total', ''):
                        if col[1] == 'sum':
                            new_cols.append('ì´_í† í°')
                        else:
                            new_cols.append('í‰ê· _í† í°')
                
                model_token_stats.columns = new_cols
                
                # ì •í™•ë„ ì¶”ê°€
                model_acc = token_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
                model_acc.columns = ['ëª¨ë¸', 'ì •í™•ë„']
                model_acc['ì •í™•ë„'] = model_acc['ì •í™•ë„'] * 100
                
                model_token_stats = model_token_stats.merge(model_acc, on='ëª¨ë¸')
                
                # ë¬¸ì œ ìˆ˜ ì¶”ê°€
                model_problem_count = token_df.groupby('ëª¨ë¸')['Question'].count().reset_index()
                model_problem_count.columns = ['ëª¨ë¸', 'ë¬¸ì œìˆ˜']
                model_token_stats = model_token_stats.merge(model_problem_count, on='ëª¨ë¸')
                
                # ë¹„ìš© ìˆ˜ì¤€ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                if 'cost' in available_cols:
                    cost_col = available_cols['cost']
                    # ê°€ì¥ ë¹ˆë²ˆí•œ ë¹„ìš© ìˆ˜ì¤€ ì°¾ê¸°
                    model_cost = token_df.groupby('ëª¨ë¸')[cost_col].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else 'unknown').reset_index()
                    model_cost.columns = ['ëª¨ë¸', 'ë¹„ìš©ìˆ˜ì¤€']
                    model_token_stats = model_token_stats.merge(model_cost, on='ëª¨ë¸')
                
                # í† í° íš¨ìœ¨ì„± ê³„ì‚° (ì •ë‹µë‹¹ í† í°)
                if 'ì´_í† í°' in model_token_stats.columns:
                    model_token_stats['ì •ë‹µë‹¹_í† í°'] = model_token_stats.apply(
                        lambda row: row['ì´_í† í°'] / (row['ë¬¸ì œìˆ˜'] * row['ì •í™•ë„'] / 100) if row['ì •í™•ë„'] > 0 else 0,
                        axis=1
                    )
                
                # ì£¼ìš” ë©”íŠ¸ë¦­ í‘œì‹œ
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    if 'ì´_í† í°' in model_token_stats.columns:
                        total_tokens = model_token_stats['ì´_í† í°'].sum()
                        st.metric(
                            t['total_tokens'],
                            f"{total_tokens:,.0f}"
                        )
                
                with col2:
                    if 'í‰ê· _í† í°' in model_token_stats.columns:
                        avg_tokens = model_token_stats['í‰ê· _í† í°'].mean()
                        st.metric(
                            t['avg_tokens_per_problem'],
                            f"{avg_tokens:,.0f}"
                        )
                
                with col3:
                    if 'ì´_ì…ë ¥í† í°' in model_token_stats.columns and 'ì´_ì¶œë ¥í† í°' in model_token_stats.columns:
                        total_input = model_token_stats['ì´_ì…ë ¥í† í°'].sum()
                        total_output = model_token_stats['ì´_ì¶œë ¥í† í°'].sum()
                        io_ratio = total_input / total_output if total_output > 0 else 0
                        st.metric(
                            t['io_ratio'],
                            f"{io_ratio:.2f}:1"
                        )
                
                with col4:
                    if 'ì •ë‹µë‹¹_í† í°' in model_token_stats.columns and len(model_token_stats[model_token_stats['ì •ë‹µë‹¹_í† í°'] > 0]) > 0:
                        # ê°€ì¥ íš¨ìœ¨ì ì¸ ëª¨ë¸ (ì •ë‹µë‹¹ í† í°ì´ ì ì€ ëª¨ë¸)
                        valid_stats = model_token_stats[model_token_stats['ì •ë‹µë‹¹_í† í°'] > 0]
                        most_efficient = valid_stats.loc[valid_stats['ì •ë‹µë‹¹_í† í°'].idxmin()]
                        st.metric(
                            t['most_efficient'],
                            most_efficient['ëª¨ë¸'],
                            f"{most_efficient['ì •ë‹µë‹¹_í† í°']:,.0f} " + t['tokens']
                        )
                
                # ìƒì„¸ í…Œì´ë¸”
                st.markdown("---")
                st.subheader("ğŸ“‹ " + ("ëª¨ë¸ë³„ í† í° ì‚¬ìš©ëŸ‰ ìƒì„¸" if lang == 'ko' else "Detailed Token Usage by Model"))
                
                # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
                display_cols = ['ëª¨ë¸']
                if 'ì´_ì…ë ¥í† í°' in model_token_stats.columns:
                    display_cols.append('ì´_ì…ë ¥í† í°')
                if 'ì´_ì¶œë ¥í† í°' in model_token_stats.columns:
                    display_cols.append('ì´_ì¶œë ¥í† í°')
                if 'ì´_í† í°' in model_token_stats.columns:
                    display_cols.append('ì´_í† í°')
                if 'í‰ê· _í† í°' in model_token_stats.columns:
                    display_cols.append('í‰ê· _í† í°')
                display_cols.extend(['ì •í™•ë„', 'ë¬¸ì œìˆ˜'])
                if 'ë¹„ìš©ìˆ˜ì¤€' in model_token_stats.columns:
                    display_cols.append('ë¹„ìš©ìˆ˜ì¤€')
                if 'ì •ë‹µë‹¹_í† í°' in model_token_stats.columns:
                    display_cols.append('ì •ë‹µë‹¹_í† í°')
                
                display_df = model_token_stats[display_cols].sort_values('ì´_í† í°' if 'ì´_í† í°' in display_cols else 'ëª¨ë¸', ascending=False)
                
                # í¬ë§·íŒ…
                format_dict = {
                    'ì´_ì…ë ¥í† í°': '{:,.0f}',
                    'ì´_ì¶œë ¥í† í°': '{:,.0f}',
                    'ì´_í† í°': '{:,.0f}',
                    'í‰ê· _í† í°': '{:,.0f}',
                    'ì •í™•ë„': '{:.2f}%',
                    'ì •ë‹µë‹¹_í† í°': '{:,.0f}'
                }
                
                st.dataframe(
                    display_df.style.format(format_dict).background_gradient(
                        subset=['ì •ë‹µë‹¹_í† í°'] if 'ì •ë‹µë‹¹_í† í°' in display_cols else [],
                        cmap='RdYlGn_r'
                    ),
                    use_container_width=True
                )
                
                st.markdown("---")
                
                # 2. ì‹œê°í™”
                st.subheader("ğŸ“Š " + ("í† í° ì‚¬ìš©ëŸ‰ ì‹œê°í™”" if lang == 'ko' else "Token Usage Visualization"))
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # ëª¨ë¸ë³„ ì´ í† í° ì‚¬ìš©ëŸ‰
                    if 'ì´_í† í°' in model_token_stats.columns:
                        fig = px.bar(
                            display_df,
                            x='ëª¨ë¸',
                            y='ì´_í† í°',
                            title=t['total_tokens'] + ' (' + ('ëª¨ë¸ë³„' if lang == 'ko' else 'by Model') + ')',
                            text='ì´_í† í°',
                            color='ì´_í† í°',
                            color_continuous_scale='Blues'
                        )
                        fig.update_traces(
                            texttemplate='%{text:,.0f}',
                            textposition='outside',
                            marker_line_color='black',
                            marker_line_width=1.5
                        )
                        fig.update_layout(
                            height=400,
                            showlegend=False,
                            yaxis_title=t['total_tokens'],
                            xaxis_title=t['model']
                        )
                        fig.update_xaxes(tickangle=45)
                        st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    # ì…ì¶œë ¥ í† í° ë¹„êµ
                    if 'ì´_ì…ë ¥í† í°' in model_token_stats.columns and 'ì´_ì¶œë ¥í† í°' in model_token_stats.columns:
                        fig = go.Figure()
                        fig.add_trace(go.Bar(
                            name=t['input_tokens'],
                            x=display_df['ëª¨ë¸'],
                            y=display_df['ì´_ì…ë ¥í† í°'],
                            marker_color='lightblue',
                            marker_line_color='black',
                            marker_line_width=1.5
                        ))
                        fig.add_trace(go.Bar(
                            name=t['output_tokens'],
                            x=display_df['ëª¨ë¸'],
                            y=display_df['ì´_ì¶œë ¥í† í°'],
                            marker_color='lightcoral',
                            marker_line_color='black',
                            marker_line_width=1.5
                        ))
                        
                        fig.update_layout(
                            barmode='stack',
                            title=f"{t['input_tokens']} vs {t['output_tokens']}",
                            height=400,
                            yaxis_title=t['tokens'],
                            xaxis_title=t['model']
                        )
                        fig.update_xaxes(tickangle=45)
                        st.plotly_chart(fig, use_container_width=True)
                
                st.markdown("---")
                
                # 3. í† í° íš¨ìœ¨ì„± ë¶„ì„
                if 'ì •ë‹µë‹¹_í† í°' in model_token_stats.columns:
                    st.subheader("ğŸ¯ " + (t['token_efficiency']))
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # ì •ë‹µë‹¹ í† í° ì‚¬ìš©ëŸ‰
                        fig = px.bar(
                            display_df.sort_values('ì •ë‹µë‹¹_í† í°'),
                            x='ëª¨ë¸',
                            y='ì •ë‹µë‹¹_í† í°',
                            title=t['token_per_correct'],
                            text='ì •ë‹µë‹¹_í† í°',
                            color='ì •ë‹µë‹¹_í† í°',
                            color_continuous_scale='RdYlGn_r'
                        )
                        fig.update_traces(
                            texttemplate='%{text:,.0f}',
                            textposition='outside',
                            marker_line_color='black',
                            marker_line_width=1.5
                        )
                        fig.update_layout(
                            height=400,
                            showlegend=False,
                            yaxis_title=t['tokens'] + ' / ' + t['correct'],
                            xaxis_title=t['model']
                        )
                        fig.update_xaxes(tickangle=45)
                        st.plotly_chart(fig, use_container_width=True)
                    
                    with col2:
                        # í† í° vs ì •í™•ë„ ì‚°ì ë„
                        if 'í‰ê· _í† í°' in model_token_stats.columns:
                            fig = px.scatter(
                                display_df,
                                x='í‰ê· _í† í°',
                                y='ì •í™•ë„',
                                size='ë¬¸ì œìˆ˜',
                                text='ëª¨ë¸',
                                title=t['token_efficiency'] + ' vs ' + t['accuracy'],
                                labels={
                                    'í‰ê· _í† í°': t['avg_tokens_per_problem'],
                                    'ì •í™•ë„': t['accuracy'] + ' (%)'
                                }
                            )
                            fig.update_traces(
                                textposition='top center',
                                marker=dict(
                                    line=dict(width=2, color='black'),
                                    opacity=0.7
                                )
                            )
                            fig.update_layout(height=400)
                            st.plotly_chart(fig, use_container_width=True)
                
                st.markdown("---")
                
                # 4. ë¹„ìš© ë¶„ì„ (ë¹„ìš© ìˆ˜ì¤€ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
                if 'cost' in available_cols:
                    st.subheader("ğŸ’µ " + t['cost_analysis'])
                    
                    cost_col = available_cols['cost']
                    
                    # ë¹„ìš© ìˆ˜ì¤€ì„ ì •ê·œí™” ë° ìˆœì„œ ì •ì˜
                    def normalize_cost_level(level):
                        if pd.isna(level):
                            return 'unknown'
                        level_str = str(level).lower().strip()
                        # ë¬´ë£Œ/ë¡œì»¬ ëª¨ë¸
                        if level_str in ['ë¬´ë£Œ', 'free', 'f', '0', 'local', 'localhost', 'ë¡œì»¬']:
                            return t['free']
                        # ë§¤ìš° ë‚®ìŒ
                        elif level_str in ['ë§¤ìš°ë‚®ìŒ', 'very low', 'very_low', 'vl', 'verylow']:
                            return t['very_low']
                        # ë‚®ìŒ
                        elif level_str in ['ë‚®ìŒ', 'low', 'l']:
                            return t['low']
                        # ì¤‘ê°„
                        elif level_str in ['ì¤‘ê°„', 'medium', 'mid', 'm']:
                            return t['medium_cost']
                        # ë†’ìŒ
                        elif level_str in ['ë†’ìŒ', 'high', 'h']:
                            return t['high']
                        return level
                    
                    # ë¹„ìš© ìˆœì„œ ì •ì˜ (ë¬´ë£Œ â†’ ë§¤ìš°ë‚®ìŒ â†’ ë‚®ìŒ â†’ ì¤‘ê°„ â†’ ë†’ìŒ)
                    cost_order = [t['free'], t['very_low'], t['low'], t['medium_cost'], t['high']]
                    
                    token_df['ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”'] = token_df[cost_col].apply(normalize_cost_level)
                    model_token_stats['ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”'] = model_token_stats['ë¹„ìš©ìˆ˜ì¤€'].apply(normalize_cost_level) if 'ë¹„ìš©ìˆ˜ì¤€' in model_token_stats.columns else t['medium_cost']
                    
                    # ğŸ†• ì‹¤ì œ ë¹„ìš© ê³„ì‚° ê¸°ëŠ¥ ì¶”ê°€
                    st.markdown("---")
                    st.subheader("ğŸ’° " + t['actual_cost'] + " " + ('ê³„ì‚°ê¸°' if lang == 'ko' else 'Calculator'))
                    
                    # ëª¨ë¸ë³„ API ê°€ê²© ì •ì˜ (2024-2025 ê¸°ì¤€, USD per 1M tokens)
                    MODEL_PRICING = {
                        # OpenAI
                        'GPT-4o': {'input': 2.50, 'output': 10.00},
                        'GPT-4o-Mini': {'input': 0.150, 'output': 0.600},
                        'GPT-4-Turbo': {'input': 10.00, 'output': 30.00},
                        'GPT-3.5-Turbo': {'input': 0.50, 'output': 1.50},
                        # Anthropic
                        'Claude-3.5-Sonnet': {'input': 3.00, 'output': 15.00},
                        'Claude-Sonnet-4': {'input': 3.00, 'output': 15.00},
                        'Claude-3.5-Haiku': {'input': 0.80, 'output': 4.00},
                        'Claude-3-Opus': {'input': 15.00, 'output': 75.00},
                        'Claude-3-Sonnet': {'input': 3.00, 'output': 15.00},
                        'Claude-3-Haiku': {'input': 0.25, 'output': 1.25},
                        # Google
                        'Gemini-1.5-Pro': {'input': 1.25, 'output': 5.00},
                        'Gemini-1.5-Flash': {'input': 0.075, 'output': 0.30},
                        # LG AI Research
                        'EXAONE-3.5': {'input': 0.00, 'output': 0.00},  # ë¡œì»¬/ë¬´ë£Œ
                    }
                    
                    # ê°€ê²© ì •ë³´ í‘œì‹œ
                    with st.expander("ğŸ“‹ " + ("ëª¨ë¸ë³„ API ê°€ê²© ì •ë³´ (2024-2025)" if lang == 'ko' else "API Pricing by Model (2024-2025)")):
                        pricing_data = []
                        for model, prices in MODEL_PRICING.items():
                            pricing_data.append({
                                'ëª¨ë¸' if lang == 'ko' else 'Model': model,
                                'ì…ë ¥ ($/1M)' if lang == 'ko' else 'Input ($/1M)': f"${prices['input']:.3f}",
                                'ì¶œë ¥ ($/1M)' if lang == 'ko' else 'Output ($/1M)': f"${prices['output']:.3f}"
                            })
                        st.dataframe(pd.DataFrame(pricing_data), use_container_width=True)
                        st.caption("ğŸ’¡ " + ("ê°€ê²©ì€ ë³€ë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœì‹  ê°€ê²©ì€ ê° ì œê³µì—…ì²´ ì›¹ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”." if lang == 'ko' else "Prices may vary. Check provider websites for latest pricing."))
                    
                    # ì‹¤ì œ ë¹„ìš© ê³„ì‚°
                    if 'ì´_ì…ë ¥í† í°' in model_token_stats.columns and 'ì´_ì¶œë ¥í† í°' in model_token_stats.columns:
                        st.markdown("---")
                        
                        cost_calculations = []
                        for _, row in model_token_stats.iterrows():
                            model = row['ëª¨ë¸']
                            input_tokens = row['ì´_ì…ë ¥í† í°']
                            output_tokens = row['ì´_ì¶œë ¥í† í°']
                            
                            # ëª¨ë¸ëª… ë§¤ì¹­ (ë¶€ë¶„ ë§¤ì¹­)
                            matched_pricing = None
                            for price_model, pricing in MODEL_PRICING.items():
                                if price_model.replace('-', '').replace('.', '').lower() in model.replace('-', '').replace('.', '').lower():
                                    matched_pricing = pricing
                                    break
                            
                            if matched_pricing:
                                # ë¹„ìš© ê³„ì‚° (USD)
                                input_cost = (input_tokens / 1_000_000) * matched_pricing['input']
                                output_cost = (output_tokens / 1_000_000) * matched_pricing['output']
                                total_cost = input_cost + output_cost
                                
                                # ë¬¸ì œë‹¹ ë¹„ìš©
                                cost_per_problem = total_cost / row['ë¬¸ì œìˆ˜'] if row['ë¬¸ì œìˆ˜'] > 0 else 0
                                
                                # ì •ë‹µë‹¹ ë¹„ìš© (íš¨ìœ¨ì„± ì§€í‘œ)
                                correct_answers = row['ë¬¸ì œìˆ˜'] * row['ì •í™•ë„'] / 100
                                cost_per_correct = total_cost / correct_answers if correct_answers > 0 else 0
                                
                                cost_calculations.append({
                                    'ëª¨ë¸' if lang == 'ko' else 'Model': model,
                                    'ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)': total_cost,
                                    'ë¬¸ì œë‹¹ ($)' if lang == 'ko' else 'Per Problem ($)': cost_per_problem,
                                    'ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)': cost_per_correct,
                                    'ì •í™•ë„ (%)' if lang == 'ko' else 'Accuracy (%)': row['ì •í™•ë„'],
                                    'ì…ë ¥ë¹„ìš© ($)' if lang == 'ko' else 'Input Cost ($)': input_cost,
                                    'ì¶œë ¥ë¹„ìš© ($)' if lang == 'ko' else 'Output Cost ($)': output_cost
                                })
                        
                        if cost_calculations:
                            cost_df = pd.DataFrame(cost_calculations)
                            
                            # ë¹„ìš© íš¨ìœ¨ì„±ìœ¼ë¡œ ì •ë ¬ (ì •ë‹µë‹¹ ë¹„ìš© ê¸°ì¤€)
                            cost_df = cost_df.sort_values('ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)')
                            
                            st.subheader("ğŸ’µ " + t['actual_cost'] + " " + ('ë¶„ì„' if lang == 'ko' else 'Analysis'))
                            
                            # ì£¼ìš” ë©”íŠ¸ë¦­
                            col1, col2, col3, col4 = st.columns(4)
                            
                            with col1:
                                total_cost_all = cost_df['ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)'].sum()
                                st.metric(
                                    t['total_estimated_cost'],
                                    f"${total_cost_all:.4f}"
                                )
                            
                            with col2:
                                avg_cost_per_problem = cost_df['ë¬¸ì œë‹¹ ($)' if lang == 'ko' else 'Per Problem ($)'].mean()
                                st.metric(
                                    t['cost_per_problem'],
                                    f"${avg_cost_per_problem:.6f}"
                                )
                            
                            with col3:
                                # ê°€ì¥ ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸
                                most_efficient = cost_df.iloc[0]
                                st.metric(
                                    'ìµœê³  íš¨ìœ¨' if lang == 'ko' else 'Most Efficient',
                                    most_efficient['ëª¨ë¸' if lang == 'ko' else 'Model'],
                                    f"${most_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)']:.6f}"
                                )
                            
                            with col4:
                                # ê°€ì¥ ë¹„ìš© ë¹„íš¨ìœ¨ì ì¸ ëª¨ë¸
                                least_efficient = cost_df.iloc[-1]
                                st.metric(
                                    'ìµœì € íš¨ìœ¨' if lang == 'ko' else 'Least Efficient',
                                    least_efficient['ëª¨ë¸' if lang == 'ko' else 'Model'],
                                    f"${least_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)']:.6f}"
                                )
                            
                            # ìƒì„¸ í…Œì´ë¸”
                            st.markdown("---")
                            st.dataframe(
                                cost_df.style.format({
                                    'ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)': '${:.6f}',
                                    'ë¬¸ì œë‹¹ ($)' if lang == 'ko' else 'Per Problem ($)': '${:.8f}',
                                    'ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)': '${:.8f}',
                                    'ì •í™•ë„ (%)' if lang == 'ko' else 'Accuracy (%)': '{:.2f}%',
                                    'ì…ë ¥ë¹„ìš© ($)' if lang == 'ko' else 'Input Cost ($)': '${:.6f}',
                                    'ì¶œë ¥ë¹„ìš© ($)' if lang == 'ko' else 'Output Cost ($)': '${:.6f}'
                                }).background_gradient(
                                    subset=['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)'],
                                    cmap='RdYlGn_r'
                                ),
                                use_container_width=True
                            )
                            
                            st.markdown("---")
                            
                            # ë¹„ìš© ì‹œê°í™”
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                # ì´ ë¹„ìš© ë¹„êµ
                                fig = px.bar(
                                    cost_df,
                                    x='ëª¨ë¸' if lang == 'ko' else 'Model',
                                    y='ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)',
                                    title=t['total_estimated_cost'],
                                    text='ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)',
                                    color='ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)',
                                    color_continuous_scale='Reds'
                                )
                                fig.update_traces(
                                    texttemplate='$%{text:.6f}',
                                    textposition='outside',
                                    marker_line_color='black',
                                    marker_line_width=1.5
                                )
                                fig.update_layout(
                                    height=400,
                                    showlegend=False,
                                    yaxis_title=t['cost'] + ' (USD)',
                                    xaxis_title=t['model']
                                )
                                fig.update_xaxes(tickangle=45)
                                st.plotly_chart(fig, use_container_width=True)
                            
                            with col2:
                                # ì •ë‹µë‹¹ ë¹„ìš© (íš¨ìœ¨ì„±)
                                fig = px.bar(
                                    cost_df.sort_values('ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)'),
                                    x='ëª¨ë¸' if lang == 'ko' else 'Model',
                                    y='ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)',
                                    title=t['cost_efficiency'] + ' (' + ('ì •ë‹µë‹¹ ë¹„ìš©' if lang == 'ko' else 'Cost per Correct') + ')',
                                    text='ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)',
                                    color='ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)',
                                    color_continuous_scale='RdYlGn_r'
                                )
                                fig.update_traces(
                                    texttemplate='$%{text:.8f}',
                                    textposition='outside',
                                    marker_line_color='black',
                                    marker_line_width=1.5
                                )
                                fig.update_layout(
                                    height=400,
                                    showlegend=False,
                                    yaxis_title=t['cost'] + ' (USD)',
                                    xaxis_title=t['model']
                                )
                                fig.update_xaxes(tickangle=45)
                                st.plotly_chart(fig, use_container_width=True)
                            
                            st.markdown("---")
                            
                            # ë¹„ìš© vs ì •í™•ë„ ì‚°ì ë„
                            fig = px.scatter(
                                cost_df,
                                x='ì´ë¹„ìš© ($)' if lang == 'ko' else 'Total Cost ($)',
                                y='ì •í™•ë„ (%)' if lang == 'ko' else 'Accuracy (%)',
                                text='ëª¨ë¸' if lang == 'ko' else 'Model',
                                title=t['cost'] + ' vs ' + t['accuracy'],
                                color='ì •í™•ë„ (%)' if lang == 'ko' else 'Accuracy (%)',
                                color_continuous_scale='RdYlGn',
                                size='ë¬¸ì œë‹¹ ($)' if lang == 'ko' else 'Per Problem ($)'
                            )
                            fig.update_traces(
                                textposition='top center',
                                marker=dict(
                                    line=dict(width=2, color='black'),
                                    opacity=0.7
                                )
                            )
                            fig.update_layout(
                                height=500,
                                yaxis=dict(range=[0, 100])
                            )
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # ì¸ì‚¬ì´íŠ¸
                            st.success(f"""
                            ğŸ’¡ **{t['cost_efficiency']} {'ì¸ì‚¬ì´íŠ¸' if lang == 'ko' else 'Insights'}**:
                            - **{'ìµœê³  íš¨ìœ¨' if lang == 'ko' else 'Most Efficient'}**: {most_efficient['ëª¨ë¸' if lang == 'ko' else 'Model']} (${most_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)']:.8f} / {'ì •ë‹µ' if lang == 'ko' else 'correct'})
                            - **{'ìµœì € íš¨ìœ¨' if lang == 'ko' else 'Least Efficient'}**: {least_efficient['ëª¨ë¸' if lang == 'ko' else 'Model']} (${least_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)']:.8f} / {'ì •ë‹µ' if lang == 'ko' else 'correct'})
                            - **{'íš¨ìœ¨ ì°¨ì´' if lang == 'ko' else 'Efficiency Gap'}**: {(least_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)'] / most_efficient['ì •ë‹µë‹¹ ($)' if lang == 'ko' else 'Per Correct ($)']):.1f}x
                            """)
                        else:
                            st.info("ğŸ’¡ " + ("í˜„ì¬ ë°ì´í„°ì˜ ëª¨ë¸ë“¤ì— ëŒ€í•œ ê°€ê²© ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ëª…ì„ í™•ì¸í•˜ê±°ë‚˜ ê°€ê²© ì •ë³´ë¥¼ ì¶”ê°€í•˜ì„¸ìš”." if lang == 'ko' else "No pricing information available for current models. Please check model names or add pricing info."))
                    
                    st.markdown("---")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # ë¹„ìš© ìˆ˜ì¤€ë³„ ëª¨ë¸ ë¶„í¬
                        cost_dist = token_df.groupby('ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”')['ëª¨ë¸'].nunique().reset_index()
                        cost_dist.columns = ['ë¹„ìš©ìˆ˜ì¤€', 'ëª¨ë¸ìˆ˜']
                        
                        fig = px.pie(
                            cost_dist,
                            values='ëª¨ë¸ìˆ˜',
                            names='ë¹„ìš©ìˆ˜ì¤€',
                            title=t['cost_level'] + ' ' + ('ë¶„í¬' if lang == 'ko' else 'Distribution'),
                            hole=0.3,
                            category_orders={'ë¹„ìš©ìˆ˜ì¤€': cost_order}
                        )
                        fig.update_traces(
                            textposition='inside',
                            textinfo='percent+label',
                            marker=dict(line=dict(color='black', width=2))
                        )
                        fig.update_layout(height=400)
                        st.plotly_chart(fig, use_container_width=True)
                    
                    with col2:
                        # ë¹„ìš© ìˆ˜ì¤€ë³„ í‰ê·  ì •í™•ë„
                        cost_acc = token_df.groupby('ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
                        cost_acc.columns = ['ë¹„ìš©ìˆ˜ì¤€', 'ì •í™•ë„']
                        cost_acc['ì •í™•ë„'] = cost_acc['ì •í™•ë„'] * 100
                        
                        fig = px.bar(
                            cost_acc,
                            x='ë¹„ìš©ìˆ˜ì¤€',
                            y='ì •í™•ë„',
                            title=t['cost_level'] + ' vs ' + t['accuracy'],
                            text='ì •í™•ë„',
                            color='ì •í™•ë„',
                            color_continuous_scale='RdYlGn',
                            category_orders={'ë¹„ìš©ìˆ˜ì¤€': cost_order}
                        )
                        fig.update_traces(
                            texttemplate='%{text:.1f}%',
                            textposition='outside',
                            marker_line_color='black',
                            marker_line_width=1.5
                        )
                        fig.update_layout(
                            height=400,
                            showlegend=False,
                            yaxis_title=t['accuracy'] + ' (%)',
                            yaxis=dict(range=[0, 100]),
                            xaxis=dict(
                                categoryorder='array',
                                categoryarray=cost_order
                            )
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    
                    st.markdown("---")
                    
                    # ë¹„ìš© íš¨ìœ¨ì„± ë§¤íŠ¸ë¦­ìŠ¤
                    st.subheader("ğŸ“Š " + t['cost_efficiency'] + (' ë§¤íŠ¸ë¦­ìŠ¤' if lang == 'ko' else ' Matrix'))
                    
                    # ë¹„ìš© ìˆ˜ì¤€ê³¼ ì •í™•ë„ë¡œ ëª¨ë¸ ë¶„ë¥˜
                    if 'ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”' in model_token_stats.columns:
                        # ë°ì´í„° ì¤€ë¹„ (Categorical ë³€í™˜ ì œê±°)
                        plot_data = model_token_stats.copy()
                        
                        fig = px.scatter(
                            plot_data,
                            x='ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”',
                            y='ì •í™•ë„',
                            size='ì´_í† í°' if 'ì´_í† í°' in plot_data.columns else 'ë¬¸ì œìˆ˜',
                            text='ëª¨ë¸',
                            title=t['cost_level'] + ' vs ' + t['accuracy'],
                            color='ì •í™•ë„',
                            color_continuous_scale='RdYlGn',
                            category_orders={'ë¹„ìš©ìˆ˜ì¤€_ì •ê·œí™”': cost_order}
                        )
                        fig.update_traces(
                            textposition='top center',
                            marker=dict(
                                line=dict(width=2, color='black'),
                                opacity=0.7
                            )
                        )
                        fig.update_layout(
                            height=500,
                            yaxis=dict(range=[0, 100]),
                            xaxis=dict(
                                title=t['cost_level'],
                                categoryorder='array',
                                categoryarray=cost_order
                            ),
                            yaxis_title=t['accuracy'] + ' (%)'
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # ì¸ì‚¬ì´íŠ¸
                        st.info(f"""
                        ğŸ’¡ **{t['cost_efficiency']} {'ì¸ì‚¬ì´íŠ¸' if lang == 'ko' else 'Insights'}**:
                        - **{'ê³ íš¨ìœ¨ ì˜ì—­' if lang == 'ko' else 'High Efficiency Zone'}** ({'ë‚®ì€ ë¹„ìš© + ë†’ì€ ì •í™•ë„' if lang == 'ko' else 'Low cost + High accuracy'}): {'ì¢Œì¸¡ ìƒë‹¨' if lang == 'ko' else 'Top left'}
                        - **{'ê³ ë¹„ìš© ì˜ì—­' if lang == 'ko' else 'High Cost Zone'}** ({'ë†’ì€ ë¹„ìš©' if lang == 'ko' else 'High cost'}): {'ìš°ì¸¡' if lang == 'ko' else 'Right side'}
                        - {'ëª¨ë¸ ì„ íƒ ì‹œ ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ì„ ê³ ë ¤í•˜ì„¸ìš”' if lang == 'ko' else 'Consider cost-performance ratio when selecting models'}
                        """)
                
                st.markdown("---")
                
                # 5. í…ŒìŠ¤íŠ¸ë³„ í† í° ë¶„ì„ (í…ŒìŠ¤íŠ¸ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°)
                if 'í…ŒìŠ¤íŠ¸ëª…' in token_df.columns and token_df['í…ŒìŠ¤íŠ¸ëª…'].nunique() > 1:
                    st.subheader("ğŸ“š " + ("í…ŒìŠ¤íŠ¸ë³„ í† í° ì‚¬ìš©ëŸ‰" if lang == 'ko' else "Token Usage by Test"))
                    
                    token_col = available_cols.get('total', available_cols.get('input', list(available_cols.values())[0]))
                    test_token = token_df.groupby(['ëª¨ë¸', 'í…ŒìŠ¤íŠ¸ëª…'])[token_col].sum().reset_index()
                    test_token.columns = ['ëª¨ë¸', 'í…ŒìŠ¤íŠ¸ëª…', 'ì´í† í°']
                    
                    fig = px.bar(
                        test_token,
                        x='í…ŒìŠ¤íŠ¸ëª…',
                        y='ì´í† í°',
                        color='ëª¨ë¸',
                        barmode='group',
                        title='í…ŒìŠ¤íŠ¸ë³„ ëª¨ë¸ í† í° ì‚¬ìš©ëŸ‰' if lang == 'ko' else 'Token Usage by Test and Model',
                        labels={'ì´í† í°': t['total_tokens']}
                    )
                    fig.update_layout(
                        height=400,
                        xaxis_title=t['testname'],
                        yaxis_title=t['total_tokens']
                    )
                    fig.update_xaxes(tickangle=45)
                    st.plotly_chart(fig, use_container_width=True)
                
                st.markdown("---")
                
                # 6. ë¬¸ì œ ìœ í˜•ë³„ í† í° ë¶„ì„ (ì´ë¯¸ì§€ ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°)
                if 'image' in token_df.columns:
                    st.subheader("ğŸ–¼ï¸ " + ("ë¬¸ì œ ìœ í˜•ë³„ í† í° ì‚¬ìš©ëŸ‰" if lang == 'ko' else "Token Usage by Problem Type"))
                    
                    # ì´ë¯¸ì§€ ë¬¸ì œ ì—¬ë¶€ êµ¬ë¶„
                    token_df['ë¬¸ì œìœ í˜•'] = token_df['image'].apply(
                        lambda x: t['text_only'] if str(x).lower() == 'text_only' or str(x) == 'X' else t['image_problem']
                    )
                    
                    token_col = available_cols.get('total', available_cols.get('input', list(available_cols.values())[0]))
                    problem_type_token = token_df.groupby(['ëª¨ë¸', 'ë¬¸ì œìœ í˜•']).agg({
                        token_col: 'mean',
                        'ì •ë‹µì—¬ë¶€': 'mean'
                    }).reset_index()
                    problem_type_token.columns = ['ëª¨ë¸', 'ë¬¸ì œìœ í˜•', 'í‰ê· í† í°', 'ì •í™•ë„']
                    problem_type_token['ì •í™•ë„'] = problem_type_token['ì •í™•ë„'] * 100
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # ë¬¸ì œ ìœ í˜•ë³„ í‰ê·  í† í°
                        fig = px.bar(
                            problem_type_token,
                            x='ëª¨ë¸',
                            y='í‰ê· í† í°',
                            color='ë¬¸ì œìœ í˜•',
                            barmode='group',
                            title=t['avg_tokens_per_problem'] + ' (' + t['problem_type'] + 'ë³„)',
                            labels={'í‰ê· í† í°': t['avg_tokens_per_problem']}
                        )
                        fig.update_layout(
                            height=400,
                            xaxis_title=t['model']
                        )
                        fig.update_xaxes(tickangle=45)
                        st.plotly_chart(fig, use_container_width=True)
                    
                    with col2:
                        # ë¬¸ì œ ìœ í˜•ë³„ ì •í™•ë„ ë¹„êµ
                        fig = px.bar(
                            problem_type_token,
                            x='ëª¨ë¸',
                            y='ì •í™•ë„',
                            color='ë¬¸ì œìœ í˜•',
                            barmode='group',
                            title=t['accuracy'] + ' (' + t['problem_type'] + 'ë³„)',
                            labels={'ì •í™•ë„': t['accuracy'] + ' (%)'}
                        )
                        fig.update_layout(
                            height=400,
                            xaxis_title=t['model'],
                            yaxis=dict(range=[0, 100])
                        )
                        fig.update_xaxes(tickangle=45)
                        st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 10: í…ŒìŠ¤íŠ¸ì…‹ í†µê³„
    with tabs[9]:
        st.header(f"ğŸ“‹ {t['testset_stats']}")
        
        if selected_tests:
            # ì„ íƒëœ í…ŒìŠ¤íŠ¸ë“¤ì˜ í†µê³„ í‘œì‹œ
            for test_name in selected_tests:
                stats = get_testset_statistics(testsets, test_name, lang)
                if stats:
                    st.subheader(f"ğŸ“– {test_name}")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric(t['total_problems'], stats['total_problems'])
                    
                    with col2:
                        if 'law_problems' in stats:
                            st.metric(t['law_problems'], stats['law_problems'])
                    
                    with col3:
                        if 'non_law_problems' in stats:
                            st.metric(t['non_law_problems'], stats['non_law_problems'])
                    
                    # ê³¼ëª©ë³„, ì—°ë„ë³„, ì„¸ì…˜ë³„ í†µê³„
                    if 'by_subject' in stats or 'by_year' in stats or 'by_session' in stats:
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            if 'by_subject' in stats:
                                st.markdown(f"**{t['by_subject']}**")
                                subject_df = pd.DataFrame(list(stats['by_subject'].items()), 
                                                         columns=['Subject', 'Count'])
                                fig = px.bar(subject_df, x='Subject', y='Count', 
                                           title=t['subject_distribution'])
                                fig.update_xaxes(tickangle=45)
                                st.plotly_chart(fig, use_container_width=True)
                        
                        with col2:
                            if 'by_year' in stats:
                                st.markdown(f"**{t['by_year']}**")
                                year_df = pd.DataFrame(list(stats['by_year'].items()), 
                                                      columns=['Year', 'Count'])
                                fig = px.bar(year_df, x='Year', y='Count', 
                                           title=t['year_distribution'])
                                st.plotly_chart(fig, use_container_width=True)
                        
                        with col3:
                            if 'by_session' in stats:
                                st.markdown(f"**{t['by_session']}**")
                                session_df = pd.DataFrame(list(stats['by_session'].items()), 
                                                         columns=['Session', 'Count'])
                                fig = px.bar(session_df, x='Session', y='Count', 
                                           title=t['session_distribution'])
                                st.plotly_chart(fig, use_container_width=True)
                    
                    st.markdown("---")
        else:
            st.info("í…ŒìŠ¤íŠ¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    
    # ì‚¬ì´ë“œë°” í•˜ë‹¨ ì •ë³´
    st.sidebar.markdown("---")
    st.sidebar.markdown(f"### ğŸ“Œ {t['help']}")
    st.sidebar.markdown(f"""
    **{t['new_features']}:**
    - âœ¨ **{t['token_cost_analysis']}**: í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„
    - âœ¨ **{t['session']} {t['filters']}**: {t['session_filter']}
    - âœ¨ **{t['incorrect_analysis']}**: {t['incorrect_pattern']}
    - âœ¨ **{t['difficulty_analysis']}**: {t['difficulty_comparison']}
    - âœ¨ **{t['problem_type']} {t['filters']}**: {t['problem_type_filter']}
    
    **{t['existing_features']}:**
    - {t['basic_filters']}
    - {t['law_analysis_desc']}
    - {t['detail_analysis']}
    """)
    
    st.sidebar.info(f"ğŸ“Š {t['current_data']}: {len(filtered_df):,}{t['problems']}")

if __name__ == "__main__":
    main()