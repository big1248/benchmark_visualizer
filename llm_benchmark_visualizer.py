import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import os
import glob
from pathlib import Path
import numpy as np
from scipy import stats

import streamlit as st
import requests
import zipfile
import os

@st.cache_data(ttl=86400)
def download_data_from_github():
    """GitHub Releasesì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
    
    data_dir = Path('./data')
    
    # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
    if data_dir.exists() and len(list(data_dir.glob('*.csv'))) > 0:
        return
    
    try:
        # ê¸°ì¡´ data í´ë” ì‚­ì œ
        if data_dir.exists():
            import shutil
            shutil.rmtree(data_dir)
        
        repo = "kjs9964/benchmark_visualizer"
        tag = "v2.2.0"
        url = f"https://github.com/{repo}/releases/download/{tag}/data.zip"
        
        # ë‹¤ìš´ë¡œë“œ
        st.info("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
        response = requests.get(url, timeout=60)
        response.raise_for_status()
        
        # ì €ì¥ ë° ì••ì¶• í•´ì œ
        with open('data.zip', 'wb') as f:
            f.write(response.content)
        
        with zipfile.ZipFile('data.zip', 'r') as zip_ref:
            zip_ref.extractall('.')
        
        os.remove('data.zip')
        
        # ê²€ì¦
        if not data_dir.exists():
            raise Exception("data í´ë”ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        csv_count = len(list(data_dir.glob('*.csv')))
        if csv_count == 0:
            raise Exception("CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        st.success(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({csv_count}ê°œ íŒŒì¼)")
        
    except Exception as e:
        st.error(f"âŒ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        st.error("GitHub Release í™•ì¸: https://github.com/kjs9964/benchmark_visualizer/releases")
        st.stop()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="LLM ë²¤ì¹˜ë§ˆí¬ ì‹œê°í™”",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë‹¤êµ­ì–´ ì§€ì› ì„¤ì •
LANGUAGES = {
    'ko': {
        'title': 'LLM ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì‹œê°í™” ë„êµ¬',
        'data_dir': 'ë°ì´í„° ë””ë ‰í† ë¦¬',
        'filters': 'í•„í„° ì˜µì…˜',
        'testname': 'í…ŒìŠ¤íŠ¸ëª…',
        'all': 'ì „ì²´',
        'model': 'ëª¨ë¸',
        'detail_type': 'ìƒì„¸ë„',
        'prompting': 'í”„ë¡¬í”„íŒ… ë°©ì‹',
        'session': 'ì„¸ì…˜',
        'problem_type': 'ë¬¸ì œ ìœ í˜•',
        'image_problem': 'ì´ë¯¸ì§€ ë¬¸ì œ',
        'text_only': 'í…ìŠ¤íŠ¸ë§Œ',
        'year': 'ì—°ë„',
        'law_type': 'ë²•ë ¹ êµ¬ë¶„',
        'law': 'ë²•ë ¹',
        'non_law': 'ë¹„ë²•ë ¹',
        'overview': 'ì „ì²´ ìš”ì•½',
        'model_comparison': 'ëª¨ë¸ë³„ ë¹„êµ',
        'law_analysis': 'ë²•ë ¹/ë¹„ë²•ë ¹ ë¶„ì„',
        'subject_analysis': 'ê³¼ëª©ë³„ ë¶„ì„',
        'year_analysis': 'ì—°ë„ë³„ ë¶„ì„',
        'incorrect_analysis': 'ì˜¤ë‹µ ë¶„ì„',
        'difficulty_analysis': 'ë‚œì´ë„ ë¶„ì„',
        'testset_stats': 'í…ŒìŠ¤íŠ¸ì…‹ í†µê³„',
        'total_problems': 'ì´ ë¬¸ì œ ìˆ˜',
        'accuracy': 'ì •í™•ë„',
        'correct': 'ì •ë‹µ',
        'wrong': 'ì˜¤ë‹µ',
        'law_problems': 'ë²•ë ¹ ë¬¸ì œ',
        'non_law_problems': 'ë¹„ë²•ë ¹ ë¬¸ì œ',
        'correct_rate': 'ì •ë‹µë¥ ',
        'wrong_rate': 'ì˜¤ë‹µë¥ ',
        'performance_by_model': 'ëª¨ë¸ë³„ ì„±ëŠ¥ ì§€í‘œ',
        'comparison_chart': 'ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸',
        'overall_comparison': 'ì „ì²´ í…ŒìŠ¤íŠ¸ ë¹„êµ',
        'heatmap': 'ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ì…‹ ì •ë‹µë„ íˆíŠ¸ë§µ',
        'law_ratio': 'ë²•ë ¹/ë¹„ë²•ë ¹ ì „ì²´ í†µê³„',
        'model_law_performance': 'ëª¨ë¸ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì„±ëŠ¥ ë¹„êµ',
        'law_distribution': 'ëª¨ë¸ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë„',
        'subject_performance': 'ê³¼ëª©ë³„ ì„±ëŠ¥',
        'year_performance': 'ì—°ë„ë³„ ì„±ëŠ¥',
        'top_incorrect': 'ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ Top 20',
        'all_models_incorrect': 'ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ',
        'most_models_incorrect': 'ëŒ€ë¶€ë¶„ ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ (â‰¥20%)',
        'test_info': 'í…ŒìŠ¤íŠ¸',
        'problem_id': 'ë¬¸ì œë²ˆí˜¸',
        'incorrect_count': 'ì˜¤ë‹µ ëª¨ë¸ìˆ˜',
        'correct_count': 'ì •ë‹µ ëª¨ë¸ìˆ˜',
        'total_models': 'ì´ ëª¨ë¸ìˆ˜',
        'attempted_models': 'ì‹œë„í•œ ëª¨ë¸',
        'question': 'ë¬¸ì œ',
        'difficulty_score': 'ë‚œì´ë„ ì ìˆ˜',
        'by_session': 'ì„¸ì…˜ë³„',
        'by_subject': 'ê³¼ëª©ë³„',
        'by_year': 'ì—°ë„ë³„',
        'problem_count': 'ë¬¸ì œ ìˆ˜',
        'session_distribution': 'ì„¸ì…˜ë³„ ë¬¸ì œ ë¶„í¬',
        'subject_distribution': 'ê³¼ëª©ë³„ ë¬¸ì œ ë¶„í¬',
        'year_distribution': 'ì—°ë„ë³„ ë¬¸ì œ ë¶„í¬',
        'law_distribution_stat': 'ë²•ë ¹/ë¹„ë²•ë ¹ ë¬¸ì œ ë¶„í¬',
        'basic_stats': 'ê¸°ë³¸ í†µê³„',
        'help': 'ë„ì›€ë§',
        'new_features': 'ìƒˆë¡œìš´ ê¸°ëŠ¥',
        'existing_features': 'ê¸°ì¡´ ê¸°ëŠ¥',
        'current_data': 'í˜„ì¬ í‘œì‹œ ì¤‘ì¸ ë°ì´í„°',
        'problems': 'ê°œ ë¬¸ì œ',
        'session_filter': 'íŠ¹ì • ì„¸ì…˜ì˜ ê²°ê³¼ë§Œ ë¶„ì„',
        'incorrect_pattern': 'ì–´ë ¤ìš´ ë¬¸ì œì™€ ì˜¤ë‹µ íŒ¨í„´ ë¶„ì„',
        'difficulty_comparison': 'ë¬¸ì œ ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ',
        'problem_type_filter': 'ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¬¸ì œ êµ¬ë¶„',
        'basic_filters': 'í…ŒìŠ¤íŠ¸ëª…, ëª¨ë¸, ìƒì„¸ë„, í”„ë¡¬í”„íŒ… ë°©ì‹ìœ¼ë¡œ í•„í„°ë§',
        'law_analysis_desc': 'ë²•ë ¹/ë¹„ë²•ë ¹ êµ¬ë¶„ ë¶„ì„',
        'detail_analysis': 'ê³¼ëª©ë³„, ì—°ë„ë³„ ìƒì„¸ ë¶„ì„',
        'font_size': 'í°íŠ¸ í¬ê¸°',
    },
    'en': {
        'title': 'LLM Benchmark Results Visualization Tool',
        'data_dir': 'Data Directory',
        'filters': 'Filter Options',
        'testname': 'Test Name',
        'all': 'All',
        'model': 'Model',
        'detail_type': 'Detail Type',
        'prompting': 'Prompting Method',
        'session': 'Session',
        'problem_type': 'Problem Type',
        'image_problem': 'Image Problem',
        'text_only': 'Text Only',
        'year': 'Year',
        'law_type': 'Law Type',
        'law': 'Law',
        'non_law': 'Non-Law',
        'overview': 'Overview',
        'model_comparison': 'Model Comparison',
        'law_analysis': 'Law/Non-Law Analysis',
        'subject_analysis': 'Subject Analysis',
        'year_analysis': 'Year Analysis',
        'incorrect_analysis': 'Incorrect Answer Analysis',
        'difficulty_analysis': 'Difficulty Analysis',
        'testset_stats': 'Test Set Statistics',
        'total_problems': 'Total Problems',
        'accuracy': 'Accuracy',
        'correct': 'Correct',
        'wrong': 'Wrong',
        'law_problems': 'Law Problems',
        'non_law_problems': 'Non-Law Problems',
        'correct_rate': 'Correct Rate',
        'wrong_rate': 'Wrong Rate',
        'performance_by_model': 'Performance Metrics by Model',
        'comparison_chart': 'Model Performance Comparison Chart',
        'overall_comparison': 'Overall Test Comparison',
        'heatmap': 'Model Ã— Test Set Accuracy Heatmap',
        'law_ratio': 'Law/Non-Law Overall Statistics',
        'model_law_performance': 'Model Law/Non-Law Performance Comparison',
        'law_distribution': 'Law/Non-Law Accuracy by Model',
        'subject_performance': 'Performance by Subject',
        'year_performance': 'Performance by Year',
        'top_incorrect': 'Top 20 Problems with Highest Incorrect Rate',
        'all_models_incorrect': 'Problems All Models Got Wrong',
        'most_models_incorrect': 'Problems Most Models Got Wrong (â‰¥20%)',
        'test_info': 'Test',
        'problem_id': 'Problem ID',
        'incorrect_count': 'Incorrect Models',
        'correct_count': 'Correct Models',
        'total_models': 'Total Models',
        'attempted_models': 'Attempted Models',
        'question': 'Question',
        'difficulty_score': 'Difficulty Score',
        'by_session': 'By Session',
        'by_subject': 'By Subject',
        'by_year': 'By Year',
        'problem_count': 'Problem Count',
        'session_distribution': 'Problem Distribution by Session',
        'subject_distribution': 'Problem Distribution by Subject',
        'year_distribution': 'Problem Distribution by Year',
        'law_distribution_stat': 'Law/Non-Law Problem Distribution',
        'basic_stats': 'Basic Statistics',
        'help': 'Help',
        'new_features': 'New Features',
        'existing_features': 'Existing Features',
        'current_data': 'Currently Displayed Data',
        'problems': ' problems',
        'session_filter': 'Analyze specific session results only',
        'incorrect_pattern': 'Analyze difficult problems and incorrect patterns',
        'difficulty_comparison': 'Compare model performance by problem difficulty',
        'problem_type_filter': 'Distinguish image/text problems',
        'basic_filters': 'Filter by test name, model, detail type, prompting method',
        'law_analysis_desc': 'Analyze law/non-law distinction',
        'detail_analysis': 'Detailed analysis by subject and year',
        'font_size': 'Font Size',
    }
}

# ì»¤ìŠ¤í…€ CSS - í°íŠ¸ í¬ê¸° ë° ë ˆì´ì•„ì›ƒ ì¡°ì •
def apply_custom_css(font_size_multiplier=1.0):
    base_font = int(16 * font_size_multiplier)
    metric_value = int(32 * font_size_multiplier)
    metric_label = int(18 * font_size_multiplier)
    h1_size = f"{3 * font_size_multiplier}rem"
    h2_size = f"{2.2 * font_size_multiplier}rem"
    h3_size = f"{1.8 * font_size_multiplier}rem"
    
    st.markdown(f"""
    <style>
        /* ì „ì²´ í°íŠ¸ í¬ê¸° ì¦ê°€ */
        html, body, [class*="css"] {{
            font-size: {base_font}px;
        }}
        
        /* ë©”íŠ¸ë¦­ ì¹´ë“œ í°íŠ¸ í¬ê¸° */
        [data-testid="stMetricValue"] {{
            font-size: {metric_value}px !important;
        }}
        
        [data-testid="stMetricLabel"] {{
            font-size: {metric_label}px !important;
        }}
        
        /* í—¤ë” í°íŠ¸ í¬ê¸° */
        h1 {{
            font-size: {h1_size} !important;
            font-weight: 700 !important;
        }}
        
        h2 {{
            font-size: {h2_size} !important;
            font-weight: 600 !important;
            margin-top: 1.5rem !important;
        }}
        
        h3 {{
            font-size: {h3_size} !important;
            font-weight: 600 !important;
        }}
        
        /* í…Œì´ë¸” í°íŠ¸ í¬ê¸° */
        .dataframe {{
            font-size: {int(15 * font_size_multiplier)}px !important;
        }}
        
        /* ì‚¬ì´ë“œë°” í°íŠ¸ í¬ê¸° */
        .css-1d391kg, [data-testid="stSidebar"] {{
            font-size: {int(15 * font_size_multiplier)}px !important;
        }}
        
        /* íƒ­ í°íŠ¸ í¬ê¸° */
        .stTabs [data-baseweb="tab-list"] button {{
            font-size: {int(18 * font_size_multiplier)}px !important;
            padding: 12px 20px !important;
        }}
        
        /* ë²„íŠ¼ í°íŠ¸ í¬ê¸° */
        .stButton>button {{
            font-size: {base_font}px !important;
            padding: 0.5rem 1rem !important;
        }}
        
        /* ì…€ë ‰íŠ¸ë°•ìŠ¤ í°íŠ¸ í¬ê¸° */
        .stSelectbox label, .stMultiSelect label {{
            font-size: {base_font}px !important;
            font-weight: 600 !important;
        }}
        
        /* ì°¨íŠ¸ ì—¬ë°± ì¡°ì • */
        .js-plotly-plot {{
            margin: 1rem 0 !important;
        }}
        
        /* ì»¨í…Œì´ë„ˆ íŒ¨ë”© */
        .block-container {{
            padding-top: 2rem !important;
            padding-bottom: 2rem !important;
        }}
    </style>
    """, unsafe_allow_html=True)

# ì•ˆì „í•œ ì •ë ¬ í•¨ìˆ˜ (íƒ€ì… í˜¼í•© ëŒ€ì‘)
def safe_sort(values):
    """ë¬¸ìì—´ê³¼ ìˆ«ìê°€ ì„ì—¬ìˆì–´ë„ ì•ˆì „í•˜ê²Œ ì •ë ¬"""
    try:
        # íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì •ë ¬: ìˆ«ì ë¨¼ì €, ê·¸ ë‹¤ìŒ ë¬¸ìì—´
        return sorted(values, key=lambda x: (isinstance(x, str), x))
    except:
        # ì‹¤íŒ¨í•˜ë©´ ëª¨ë‘ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬
        return sorted(values, key=str)

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
@st.cache_data
def load_data(data_dir):
    """ëª¨ë“  CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  í†µí•©"""
    
    # testset íŒŒì¼ë“¤ ë¡œë“œ
    testset_files = glob.glob(os.path.join(data_dir, "testset_*.csv"))
    testsets = {}
    for file in testset_files:
        test_name = os.path.basename(file).replace("testset_", "").replace(".csv", "")
        try:
            df = pd.read_csv(file, encoding='utf-8')
            testsets[test_name] = df
        except:
            try:
                df = pd.read_csv(file, encoding='cp949')
                testsets[test_name] = df
            except:
                continue
    
    # ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ
    result_files = glob.glob(os.path.join(data_dir, "*_detailed_*.csv")) + \
                   glob.glob(os.path.join(data_dir, "*_summary_*.csv"))
    
    results = []
    for file in result_files:
        filename = os.path.basename(file)
        
        try:
            # íŒŒì¼ëª… íŒŒì‹± ê°œì„ 
            model = None
            detail_type = None
            prompt_type = None
            test_name = None
            
            # Claude ëª¨ë¸ íŒŒì‹±
            if "Claude" in filename:
                if "Claude-3-5-Sonnet" in filename or "Claude-3.5-Sonnet" in filename:
                    model = "Claude-3.5-Sonnet"
                elif "Claude-3-5-Haiku" in filename or "Claude-3.5-Haiku" in filename:
                    model = "Claude-3.5-Haiku"
                elif "Claude-Sonnet-4" in filename:
                    model = "Claude-Sonnet-4"
                
                if "detailed" in filename:
                    detail_type = "detailed"
                elif "summary" in filename:
                    detail_type = "summary"
            
            # GPT ëª¨ë¸ íŒŒì‹±
            elif "GPT" in filename:
                if "GPT-4o-Mini" in filename:
                    model = "GPT-4o-Mini"
                elif "GPT-4o" in filename:
                    model = "GPT-4o"
                
                if "detailed" in filename:
                    detail_type = "detailed"
                elif "summary" in filename:
                    detail_type = "summary"
            
            # ëª¨ë¸ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê±´ë„ˆë›°ê¸°
            if model is None or detail_type is None:
                continue
            
            # í”„ë¡¬í”„íŒ… ë°©ì‹ ì°¾ê¸° - ê°œì„ ëœ íŒŒì‹±
            if "noprompting" in filename or "no-prompting" in filename or "no_prompting" in filename:
                prompt_type = "no-prompting"
            elif "few-shot" in filename or "few_shot" in filename or "fewshot" in filename:
                prompt_type = "few-shot"
            elif "cot" in filename.lower() or "chain-of-thought" in filename:
                prompt_type = "cot"
            else:
                prompt_type = "unknown"
            
            # í…ŒìŠ¤íŠ¸ëª… ì°¾ê¸°
            test_names = ["ì‚°ì—…ì•ˆì „ê¸°ì‚¬", "ë°©ì¬ê¸°ì‚¬", "ê±´ì„¤ì•ˆì „ê¸°ì‚¬", "ë°©ì¬ì•ˆì „ì§"]
            for tn in test_names:
                if tn in filename:
                    test_name = tn
                    break
            
            if test_name is None:
                continue
            
            # CSV íŒŒì¼ ì½ê¸°
            try:
                df = pd.read_csv(file, encoding='utf-8')
            except:
                try:
                    df = pd.read_csv(file, encoding='cp949')
                except:
                    continue
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            df['ëª¨ë¸'] = model
            df['ìƒì„¸ë„'] = detail_type
            df['í”„ë¡¬í”„íŒ…'] = prompt_type
            df['í…ŒìŠ¤íŠ¸ëª…'] = test_name
            
            results.append(df)
            
        except Exception as e:
            st.sidebar.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {os.path.basename(file)}")
            continue
    
    if results:
        results_df = pd.concat(results, ignore_index=True)
    else:
        results_df = pd.DataFrame()
    
    return testsets, results_df

def safe_convert_to_int(value):
    """ì•ˆì „í•˜ê²Œ ê°’ì„ ì •ìˆ˜ë¡œ ë³€í™˜ - ì‰¼í‘œ êµ¬ë¶„ì ì²˜ë¦¬ ê°œì„ """
    try:
        # Noneì´ë‚˜ NaN ì²˜ë¦¬
        if pd.isna(value):
            return None
            
        # ë¬¸ìì—´ì¸ ê²½ìš° ì‰¼í‘œ ì œê±° (ì²œ ë‹¨ìœ„ êµ¬ë¶„ì)
        if isinstance(value, str):
            # ì‰¼í‘œëŠ” ì²œ ë‹¨ìœ„ êµ¬ë¶„ìì´ë¯€ë¡œ ê·¸ëƒ¥ ì œê±°
            value = value.replace(',', '')
        
        # floatë¡œ ë³€í™˜ í›„ intë¡œ ë³€í™˜
        return int(float(value))
    except (ValueError, TypeError):
        return None

def get_available_sessions(df, test_names):
    """íŠ¹ì • í…ŒìŠ¤íŠ¸ë“¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ì…˜ ëª©ë¡ ë°˜í™˜ (ë¬¸ìì—´ê³¼ ìˆ«ì ëª¨ë‘ ì§€ì›)"""
    if df is None or len(df) == 0:
        return []
    
    # ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì„ íƒ ì‹œ í•„í„°ë§
    if test_names:
        test_df = df[df['í…ŒìŠ¤íŠ¸ëª…'].isin(test_names)] if 'í…ŒìŠ¤íŠ¸ëª…' in df.columns else df
    else:
        test_df = df
    
    if 'Session' in test_df.columns:
        sessions_raw = test_df['Session'].dropna().unique().tolist()
        sessions_clean = []
        
        for s in sessions_raw:
            # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ ë¨¼ì € ì‹œë„
            s_int = safe_convert_to_int(s)
            if s_int is not None:
                # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•˜ë©´ ì •ìˆ˜ë¡œ ì €ì¥
                if s_int not in sessions_clean:
                    sessions_clean.append(s_int)
            else:
                # ìˆ«ìë¡œ ë³€í™˜ ë¶ˆê°€ëŠ¥í•˜ë©´ ë¬¸ìì—´ë¡œ ì €ì¥
                if isinstance(s, str):
                    s_clean = s.strip()
                    if s_clean and s_clean not in sessions_clean:
                        sessions_clean.append(s_clean)
        
        # ì •ë ¬: ìˆ«ì ë¨¼ì €, ê·¸ ë‹¤ìŒ ë¬¸ìì—´
        return sorted(sessions_clean, key=lambda x: (isinstance(x, str), x))
    return []

def create_problem_identifier(row, lang='ko'):
    """ë¬¸ì œ ì‹ë³„ì ìƒì„± (í…ŒìŠ¤íŠ¸ëª…/ì—°ë„/ì„¸ì…˜/ê³¼ëª©/ë¬¸ì œë²ˆí˜¸)"""
    parts = []
    
    if 'Test Name' in row and pd.notna(row['Test Name']):
        parts.append(str(row['Test Name']))
    elif 'í…ŒìŠ¤íŠ¸ëª…' in row and pd.notna(row['í…ŒìŠ¤íŠ¸ëª…']):
        parts.append(str(row['í…ŒìŠ¤íŠ¸ëª…']))
    
    if 'Year' in row and pd.notna(row['Year']):
        year_int = safe_convert_to_int(row['Year'])
        if year_int:
            parts.append(str(year_int))
    
    if 'Session' in row and pd.notna(row['Session']):
        session_int = safe_convert_to_int(row['Session'])
        if session_int:
            parts.append(f"S{session_int}")
    
    if 'Subject' in row and pd.notna(row['Subject']):
        parts.append(str(row['Subject']))
    
    if 'Number' in row and pd.notna(row['Number']):
        number_int = safe_convert_to_int(row['Number'])
        if number_int:
            parts.append(f"Q{number_int}")
    
    return " / ".join(parts) if parts else "Unknown"

def get_testset_statistics(testsets, test_name, lang='ko'):
    """í…ŒìŠ¤íŠ¸ì…‹ì˜ ê¸°ì´ˆ í†µê³„ ë°˜í™˜"""
    t = LANGUAGES[lang]
    
    if test_name not in testsets:
        return None
    
    df = testsets[test_name]
    stats = {}
    
    # ì´ ë¬¸ì œ ìˆ˜
    stats['total_problems'] = len(df)
    
    # ë²•ë ¹/ë¹„ë²•ë ¹ ë¬¸ì œ ìˆ˜
    if 'law' in df.columns:
        stats['law_problems'] = len(df[df['law'] == 'O'])
        stats['non_law_problems'] = len(df[df['law'] != 'O'])
    
    # ê³¼ëª©ë³„ ë¬¸ì œ ìˆ˜
    if 'Subject' in df.columns:
        stats['by_subject'] = df['Subject'].value_counts().to_dict()
    
    # ì—°ë„ë³„ ë¬¸ì œ ìˆ˜
    if 'Year' in df.columns:
        stats['by_year'] = df['Year'].value_counts().sort_index().to_dict()
    
    # ì„¸ì…˜ë³„ ë¬¸ì œ ìˆ˜
    if 'Session' in df.columns:
        stats['by_session'] = df['Session'].value_counts().sort_index().to_dict()
    
    return stats

# ë©”ì¸ ì‹¤í–‰
def main():
    # ğŸ”¥ GitHubì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)
    download_data_from_github()
    
    # ì–¸ì–´ ì„ íƒ (ì‚¬ì´ë“œë°” ìƒë‹¨ì— ë°°ì¹˜)
    st.sidebar.selectbox(
        "Language / ì–¸ì–´",
        options=['ko', 'en'],
        format_func=lambda x: "í•œêµ­ì–´" if x == 'ko' else "English",
        key='language'
    )
    
    lang = st.session_state.language
    t = LANGUAGES[lang]
    
    # í°íŠ¸ í¬ê¸° ì¡°ì •
    st.sidebar.markdown("---")
    font_size = st.sidebar.slider(
        t['font_size'],
        min_value=0.8,
        max_value=1.5,
        value=1.0,
        step=0.1
    )
    apply_custom_css(font_size)
    
    # ì œëª©
    st.title(f"ğŸ¯ {t['title']}")
    st.markdown("---")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ëŠ” í•­ìƒ ./data (GitHubì—ì„œ ë‹¤ìš´ë¡œë“œí•œ í´ë”)
    data_dir = "./data"
    
    if not os.path.exists(data_dir):
        st.error(f"Directory not found: {data_dir}")
        return
    
    # ë°ì´í„° ë¡œë“œ
    testsets, results_df = load_data(data_dir)
    
    if results_df.empty:
        st.warning("No data files found in the specified directory.")
        return
    
    # ì •ë‹µì—¬ë¶€ ì»¬ëŸ¼ ìƒì„±
    if 'Answer' in results_df.columns and 'ì˜ˆì¸¡ë‹µ' in results_df.columns:
        results_df['ì •ë‹µì—¬ë¶€'] = results_df.apply(
            lambda row: row['Answer'] == row['ì˜ˆì¸¡ë‹µ'] if pd.notna(row['Answer']) and pd.notna(row['ì˜ˆì¸¡ë‹µ']) else False,
            axis=1
        )
    
    # ì‚¬ì´ë“œë°” í•„í„°
    st.sidebar.markdown("---")
    st.sidebar.markdown(f"## {t['filters']}")
    
    # í…ŒìŠ¤íŠ¸ëª… í•„í„° (multiselectë¡œ ë³€ê²½)
    test_names = sorted(results_df['í…ŒìŠ¤íŠ¸ëª…'].unique().tolist())
    selected_tests = st.sidebar.multiselect(
        t['testname'],
        options=test_names,
        default=test_names,
        help="ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    
    # í…ŒìŠ¤íŠ¸ ì„ íƒì— ë”°ë¥¸ ë°ì´í„° í•„í„°ë§
    if selected_tests:
        filtered_df = results_df[results_df['í…ŒìŠ¤íŠ¸ëª…'].isin(selected_tests)].copy()
    else:
        filtered_df = results_df.copy()
    
    # ëª¨ë¸ í•„í„°
    models = sorted(filtered_df['ëª¨ë¸'].unique().tolist())
    selected_models = st.sidebar.multiselect(
        t['model'],
        options=models,
        default=models
    )
    
    if selected_models:
        filtered_df = filtered_df[filtered_df['ëª¨ë¸'].isin(selected_models)]
    
    # ìƒì„¸ë„ í•„í„° (multiselectë¡œ ë³€ê²½)
    details = sorted(filtered_df['ìƒì„¸ë„'].unique().tolist())
    selected_details = st.sidebar.multiselect(
        t['detail_type'],
        options=details,
        default=details,
        help="ì—¬ëŸ¬ ìƒì„¸ë„ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    
    if selected_details:
        filtered_df = filtered_df[filtered_df['ìƒì„¸ë„'].isin(selected_details)]
    
    # í”„ë¡¬í”„íŒ… ë°©ì‹ í•„í„° (multiselectë¡œ ë³€ê²½)
    prompts = sorted(filtered_df['í”„ë¡¬í”„íŒ…'].unique().tolist())
    selected_prompts = st.sidebar.multiselect(
        t['prompting'],
        options=prompts,
        default=prompts,
        help="ì—¬ëŸ¬ í”„ë¡¬í”„íŒ… ë°©ì‹ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    
    if selected_prompts:
        filtered_df = filtered_df[filtered_df['í”„ë¡¬í”„íŒ…'].isin(selected_prompts)]
    
    # ì„¸ì…˜ í•„í„° (ì›ë³¸ ë°ì´í„°ì—ì„œ ì¶”ì¶œ, multiselectë¡œ ë³€ê²½)
    if selected_tests:
        # ì„ íƒëœ í…ŒìŠ¤íŠ¸ë“¤ì˜ ì›ë³¸ ë°ì´í„°ì—ì„œ ì„¸ì…˜ ì¶”ì¶œ
        available_sessions = get_available_sessions(results_df, selected_tests)
        if available_sessions:
            selected_sessions = st.sidebar.multiselect(
                t['session'],
                options=available_sessions,
                default=available_sessions,
                help="ì—¬ëŸ¬ ì„¸ì…˜ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
            )
            
            if selected_sessions:
                # ì„ íƒëœ ì„¸ì…˜ê³¼ ë§¤ì¹­ (ë¬¸ìì—´ê³¼ ìˆ«ì ëª¨ë‘ ì§€ì›)
                def match_session(x):
                    if pd.isna(x):
                        return False
                    
                    # xë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ ì‹œë„
                    x_int = safe_convert_to_int(x)
                    
                    # ì„ íƒëœ ì„¸ì…˜ì— ì •ìˆ˜ë¡œ ë³€í™˜ëœ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                    if x_int is not None and x_int in selected_sessions:
                        return True
                    
                    # ë¬¸ìì—´ë¡œ ì§ì ‘ ë¹„êµ
                    if isinstance(x, str):
                        x_clean = x.strip()
                        return x_clean in selected_sessions
                    
                    return False
                
                filtered_df = filtered_df[filtered_df['Session'].apply(match_session)]
    
    # ë¬¸ì œ ìœ í˜• í•„í„°
    if 'image' in filtered_df.columns:
        problem_types = [t['all'], t['image_problem'], t['text_only']]
        selected_problem_type = st.sidebar.selectbox(
            t['problem_type'],
            options=problem_types
        )
        
        if selected_problem_type == t['image_problem']:
            filtered_df = filtered_df[filtered_df['image'] != 'text_only']
        elif selected_problem_type == t['text_only']:
            filtered_df = filtered_df[filtered_df['image'] == 'text_only']
    
    # ì—°ë„ í•„í„° (ì›ë³¸ ë°ì´í„°ì—ì„œ ì¶”ì¶œí•˜ì—¬ ëª¨ë“  ì—°ë„ í‘œì‹œ)
    if 'Year' in results_df.columns:
        # ì„ íƒëœ í…ŒìŠ¤íŠ¸ë“¤ì˜ ì—°ë„ë§Œ í‘œì‹œ
        if selected_tests:
            year_source_df = results_df[results_df['í…ŒìŠ¤íŠ¸ëª…'].isin(selected_tests)]
        else:
            year_source_df = results_df
        
        # ì—°ë„ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
        years_raw = year_source_df['Year'].dropna().unique().tolist()
        years_int = []
        for y in years_raw:
            y_int = safe_convert_to_int(y)
            if y_int and y_int not in years_int:
                years_int.append(y_int)
        years = safe_sort(years_int)
        
        if years:
            selected_years = st.sidebar.multiselect(
                t['year'],
                options=years,
                default=years
            )
            
            if selected_years:
                # ì„ íƒëœ ì—°ë„ì™€ ë§¤ì¹­ë˜ëŠ” ì›ë³¸ ë°ì´í„° í•„í„°ë§
                filtered_df = filtered_df[filtered_df['Year'].apply(
                    lambda x: safe_convert_to_int(x) in selected_years if pd.notna(x) else False
                )]
    
    # ë²•ë ¹ êµ¬ë¶„ í•„í„°
    if 'law' in filtered_df.columns:
        law_options = [t['all'], t['law'], t['non_law']]
        selected_law = st.sidebar.selectbox(
            t['law_type'],
            options=law_options
        )
        
        if selected_law == t['law']:
            filtered_df = filtered_df[filtered_df['law'] == 'O']
        elif selected_law == t['non_law']:
            filtered_df = filtered_df[filtered_df['law'] != 'O']
    
    # í•„í„°ë§ëœ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
    if filtered_df.empty:
        st.warning("No data matches the selected filters.")
        return
    
    # íƒ­ ìƒì„±
    tabs = st.tabs([
        f"ğŸ“Š {t['overview']}",
        f"ğŸ” {t['model_comparison']}",
        f"âš–ï¸ {t['law_analysis']}",
        f"ğŸ“š {t['subject_analysis']}",
        f"ğŸ“… {t['year_analysis']}",
        f"âŒ {t['incorrect_analysis']}",
        f"ğŸ“ˆ {t['difficulty_analysis']}",
        f"ğŸ“‹ {t['testset_stats']}"
    ])
    
    # íƒ­ 1: ì „ì²´ ìš”ì•½
    with tabs[0]:
        st.header(f"ğŸ“Š {t['overview']}")
        
        # í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ë¬¸ì œ ìˆ˜ ê³„ì‚°
        total_problems = 0
        if selected_tests:
            for test_name in selected_tests:
                if test_name in testsets:
                    total_problems += len(testsets[test_name])
        
        # ê³ ìœ  ë¬¸ì œ ìˆ˜ëŠ” filtered_dfì—ì„œ ì¤‘ë³µ ì œê±° (ë°±ì—…ìš©)
        unique_questions = filtered_df['Question'].nunique()
        num_models = filtered_df['ëª¨ë¸'].nunique()
        
        # í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë³¸ ì •ë³´
        st.subheader("ğŸ“‹ í…ŒìŠ¤íŠ¸ì…‹ ì •ë³´")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # í…ŒìŠ¤íŠ¸ì…‹ íŒŒì¼ì˜ ì‹¤ì œ ë¬¸ì œ ìˆ˜ ì‚¬ìš©
            display_problems = total_problems if total_problems > 0 else unique_questions
            st.metric("ì´ ë¬¸ì œ ìˆ˜", f"{display_problems:,}")
        with col2:
            st.metric("í‰ê°€ ëª¨ë¸ ìˆ˜", f"{num_models}")
        with col3:
            # ìˆ˜ì •: ì´ í‰ê°€ íšŸìˆ˜ = ì´ ë¬¸ì œ ìˆ˜ Ã— ëª¨ë¸ ìˆ˜
            actual_eval_count = display_problems * num_models
            st.metric("ì´ í‰ê°€ íšŸìˆ˜", f"{actual_eval_count:,}")
        
        st.markdown("---")
        
        # ëª¨ë¸ í‰ê·  ì„±ëŠ¥
        st.subheader("ğŸ¯ ëª¨ë¸ í‰ê·  ì„±ëŠ¥")
        col1, col2, col3, col4 = st.columns(4)
        
        # ëª¨ë¸ë³„ ì •í™•ë„ ê³„ì‚° í›„ í‰ê· 
        model_accuracies = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean()
        avg_accuracy = model_accuracies.mean() * 100
        
        # í‰ê·  ì •ë‹µ/ì˜¤ë‹µ ìˆ˜ (ëª¨ë¸ë‹¹)
        avg_problems_per_model = display_problems  # ëª¨ë¸ë‹¹ í‰ê°€í•œ ë¬¸ì œ ìˆ˜ (í…ŒìŠ¤íŠ¸ì…‹ ê¸°ì¤€)
        avg_correct = (avg_problems_per_model * avg_accuracy / 100) if avg_problems_per_model > 0 else 0
        avg_wrong = avg_problems_per_model - avg_correct
        
        with col1:
            st.metric("í‰ê·  ì •í™•ë„", f"{avg_accuracy:.2f}%")
        with col2:
            st.metric("ëª¨ë¸ë‹¹ í‰ê·  ë¬¸ì œ ìˆ˜", f"{avg_problems_per_model:.0f}")
        with col3:
            st.metric("í‰ê·  ì •ë‹µ ìˆ˜", f"{avg_correct:.0f}")
        with col4:
            st.metric("í‰ê·  ì˜¤ë‹µ ìˆ˜", f"{avg_wrong:.0f}")
        
        # ë²•ë ¹/ë¹„ë²•ë ¹ í†µê³„
        if 'law' in filtered_df.columns:
            st.markdown("---")
            st.subheader("âš–ï¸ ë²•ë ¹/ë¹„ë²•ë ¹ ë¶„ì„")
            
            # í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë°˜ìœ¼ë¡œ ë²•ë ¹/ë¹„ë²•ë ¹ ë¬¸ì œ ìˆ˜ ê³„ì‚°
            law_count_testset = 0
            non_law_count_testset = 0
            
            if selected_tests:
                for test_name in selected_tests:
                    if test_name in testsets and 'law' in testsets[test_name].columns:
                        test_df = testsets[test_name]
                        law_count_testset += len(test_df[test_df['law'] == 'O'])
                        non_law_count_testset += len(test_df[test_df['law'] != 'O'])
            
            # ë°±ì—…: filtered_dfì—ì„œ ê³„ì‚° (í…ŒìŠ¤íŠ¸ì…‹ì´ ì—†ëŠ” ê²½ìš°)
            unique_problems = filtered_df[['Question', 'law']].drop_duplicates()
            law_count_backup = len(unique_problems[unique_problems['law'] == 'O'])
            non_law_count_backup = len(unique_problems[unique_problems['law'] != 'O'])
            
            # í…ŒìŠ¤íŠ¸ì…‹ ê°’ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë°±ì—… ì‚¬ìš©
            law_count = law_count_testset if law_count_testset > 0 else law_count_backup
            non_law_count = non_law_count_testset if non_law_count_testset > 0 else non_law_count_backup
            
            # ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  (ëª¨ë“  ëª¨ë¸ í‰ê· )
            law_df = filtered_df[filtered_df['law'] == 'O']
            non_law_df = filtered_df[filtered_df['law'] != 'O']
            
            law_accuracy = (law_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if len(law_df) > 0 else 0
            non_law_accuracy = (non_law_df['ì •ë‹µì—¬ë¶€'].mean() * 100) if len(non_law_df) > 0 else 0
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric(t['law_problems'], f"{law_count:,}")
            with col2:
                st.metric(f"{t['law']} {t['correct_rate']}", f"{law_accuracy:.2f}%")
            with col3:
                st.metric(t['non_law_problems'], f"{non_law_count:,}")
            with col4:
                st.metric(f"{t['non_law']} {t['correct_rate']}", f"{non_law_accuracy:.2f}%")
        
        # ì‹œê°í™” ì°¨íŠ¸ ì¶”ê°€
        st.markdown("---")
        st.subheader("ğŸ“Š ì£¼ìš” ì§€í‘œ ì‹œê°í™”")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ëª¨ë¸ë³„ í‰ê·  ì •í™•ë„ ë°” ì°¨íŠ¸
            model_acc_df = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
            model_acc_df.columns = ['ëª¨ë¸', 'ì •í™•ë„']
            model_acc_df['ì •í™•ë„'] = model_acc_df['ì •í™•ë„'] * 100
            model_acc_df = model_acc_df.sort_values('ì •í™•ë„', ascending=False)
            
            fig = px.bar(
                model_acc_df,
                x='ëª¨ë¸',
                y='ì •í™•ë„',
                title='ëª¨ë¸ë³„ í‰ê·  ì •í™•ë„',
                text='ì •í™•ë„',
                color='ì •í™•ë„',
                color_continuous_scale='RdYlGn'
            )
            fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
            fig.update_layout(
                height=400,
                showlegend=False,
                yaxis_title='ì •í™•ë„ (%)',
                xaxis_title='ëª¨ë¸',
                yaxis=dict(range=[0, 100])
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  ë¹„êµ ì°¨íŠ¸
            if 'law' in filtered_df.columns:
                law_comparison = pd.DataFrame({
                    'êµ¬ë¶„': ['ë²•ë ¹', 'ë¹„ë²•ë ¹'],
                    'ì •ë‹µë¥ ': [law_accuracy, non_law_accuracy],
                    'ë¬¸ì œìˆ˜': [law_count, non_law_count]
                })
                
                fig = go.Figure()
                fig.add_trace(go.Bar(
                    name='ì •ë‹µë¥ ',
                    x=law_comparison['êµ¬ë¶„'],
                    y=law_comparison['ì •ë‹µë¥ '],
                    text=law_comparison['ì •ë‹µë¥ '].round(1),
                    texttemplate='%{text}%',
                    textposition='outside',
                    marker_color=['#FF6B6B', '#4ECDC4'],
                    yaxis='y'
                ))
                
                fig.add_trace(go.Scatter(
                    name='ë¬¸ì œ ìˆ˜',
                    x=law_comparison['êµ¬ë¶„'],
                    y=law_comparison['ë¬¸ì œìˆ˜'],
                    text=law_comparison['ë¬¸ì œìˆ˜'],
                    texttemplate='%{text}ê°œ',
                    textposition='top center',
                    mode='lines+markers+text',
                    marker=dict(size=10, color='orange'),
                    line=dict(width=2, color='orange'),
                    yaxis='y2'
                ))
                
                fig.update_layout(
                    title='ë²•ë ¹/ë¹„ë²•ë ¹ ì •ë‹µë¥  ë° ë¬¸ì œ ìˆ˜ ë¹„êµ',
                    height=400,
                    yaxis=dict(
                        title='ì •ë‹µë¥  (%)',
                        range=[0, 100]
                    ),
                    yaxis2=dict(
                        title='ë¬¸ì œ ìˆ˜',
                        overlaying='y',
                        side='right',
                        range=[0, max(law_count, non_law_count) * 1.2]
                    ),
                    hovermode='x unified',
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    )
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                # ë²•ë ¹ ì •ë³´ê°€ ì—†ì„ ë•Œ - ëª¨ë¸ë³„ ì •ë‹µ/ì˜¤ë‹µ ìˆ˜ ì°¨íŠ¸
                model_correct_wrong = filtered_df.groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].agg(['sum', 'count']).reset_index()
                model_correct_wrong.columns = ['ëª¨ë¸', 'ì •ë‹µ', 'ì´ë¬¸ì œ']
                model_correct_wrong['ì˜¤ë‹µ'] = model_correct_wrong['ì´ë¬¸ì œ'] - model_correct_wrong['ì •ë‹µ']
                
                fig = go.Figure()
                fig.add_trace(go.Bar(
                    name='ì •ë‹µ',
                    x=model_correct_wrong['ëª¨ë¸'],
                    y=model_correct_wrong['ì •ë‹µ'],
                    marker_color='lightgreen'
                ))
                fig.add_trace(go.Bar(
                    name='ì˜¤ë‹µ',
                    x=model_correct_wrong['ëª¨ë¸'],
                    y=model_correct_wrong['ì˜¤ë‹µ'],
                    marker_color='lightcoral'
                ))
                
                fig.update_layout(
                    barmode='stack',
                    title='ëª¨ë¸ë³„ ì •ë‹µ/ì˜¤ë‹µ ìˆ˜',
                    height=400,
                    yaxis_title='ë¬¸ì œ ìˆ˜',
                    xaxis_title='ëª¨ë¸'
                )
                st.plotly_chart(fig, use_container_width=True)
        
        # í…ŒìŠ¤íŠ¸ì…‹ë³„ ë¶„í¬ (ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ê°€ ìˆì„ ê²½ìš°)
        if 'í…ŒìŠ¤íŠ¸ëª…' in filtered_df.columns and filtered_df['í…ŒìŠ¤íŠ¸ëª…'].nunique() > 1:
            st.markdown("---")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # í…ŒìŠ¤íŠ¸ì…‹ë³„ ë¬¸ì œ ìˆ˜
                test_problem_count = filtered_df.groupby('í…ŒìŠ¤íŠ¸ëª…')['Question'].nunique().reset_index()
                test_problem_count.columns = ['í…ŒìŠ¤íŠ¸ëª…', 'ë¬¸ì œìˆ˜']
                test_problem_count = test_problem_count.sort_values('ë¬¸ì œìˆ˜', ascending=False)
                
                fig = px.bar(
                    test_problem_count,
                    x='í…ŒìŠ¤íŠ¸ëª…',
                    y='ë¬¸ì œìˆ˜',
                    title='í…ŒìŠ¤íŠ¸ì…‹ë³„ ë¬¸ì œ ìˆ˜',
                    text='ë¬¸ì œìˆ˜',
                    color='ë¬¸ì œìˆ˜',
                    color_continuous_scale='Blues'
                )
                fig.update_traces(textposition='outside')
                fig.update_layout(
                    height=400,
                    showlegend=False,
                    yaxis_title='ë¬¸ì œ ìˆ˜',
                    xaxis_title='í…ŒìŠ¤íŠ¸ëª…'
                )
                fig.update_xaxes(tickangle=45)
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # í…ŒìŠ¤íŠ¸ì…‹ë³„ í‰ê·  ì •í™•ë„
                test_accuracy = filtered_df.groupby('í…ŒìŠ¤íŠ¸ëª…')['ì •ë‹µì—¬ë¶€'].mean().reset_index()
                test_accuracy.columns = ['í…ŒìŠ¤íŠ¸ëª…', 'ì •í™•ë„']
                test_accuracy['ì •í™•ë„'] = test_accuracy['ì •í™•ë„'] * 100
                test_accuracy = test_accuracy.sort_values('ì •í™•ë„', ascending=False)
                
                fig = px.bar(
                    test_accuracy,
                    x='í…ŒìŠ¤íŠ¸ëª…',
                    y='ì •í™•ë„',
                    title='í…ŒìŠ¤íŠ¸ì…‹ë³„ í‰ê·  ì •í™•ë„',
                    text='ì •í™•ë„',
                    color='ì •í™•ë„',
                    color_continuous_scale='RdYlGn'
                )
                fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
                fig.update_layout(
                    height=400,
                    showlegend=False,
                    yaxis_title='ì •í™•ë„ (%)',
                    xaxis_title='í…ŒìŠ¤íŠ¸ëª…',
                    yaxis=dict(range=[0, 100])
                )
                fig.update_xaxes(tickangle=45)
                st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 2: ëª¨ë¸ë³„ ë¹„êµ
    with tabs[1]:
        st.header(f"ğŸ” {t['model_comparison']}")
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ê³„ì‚°
        model_stats = filtered_df.groupby('ëª¨ë¸').agg({
            'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
        }).reset_index()
        model_stats.columns = ['ëª¨ë¸', 'ì •ë‹µ', 'ì´ë¬¸ì œ', 'ì •í™•ë„']
        model_stats['ì •í™•ë„'] = model_stats['ì •í™•ë„'] * 100
        model_stats['ì˜¤ë‹µ'] = model_stats['ì´ë¬¸ì œ'] - model_stats['ì •ë‹µ']
        model_stats = model_stats.sort_values('ì •í™•ë„', ascending=False)
        
        # ì„±ëŠ¥ ì§€í‘œ í…Œì´ë¸”
        st.subheader(t['performance_by_model'])
        
        # í…Œì´ë¸” ì»¬ëŸ¼ëª… ë³€ê²½
        display_stats = model_stats.copy()
        if lang == 'en':
            display_stats.columns = ['Model', 'Correct', 'Total', 'Accuracy', 'Wrong']
        
        st.dataframe(
            display_stats.style.format({
                'ì •í™•ë„' if lang == 'ko' else 'Accuracy': '{:.2f}%'
            }).background_gradient(subset=['ì •í™•ë„' if lang == 'ko' else 'Accuracy'], cmap='RdYlGn'),
            use_container_width=True
        )
        
        # ë¹„êµ ì°¨íŠ¸
        st.markdown("---")
        st.subheader(t['comparison_chart'])
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ì •í™•ë„ ë°” ì°¨íŠ¸
            fig = px.bar(
                model_stats,
                x='ëª¨ë¸',
                y='ì •í™•ë„',
                title=t['overall_comparison'],
                text='ì •í™•ë„',
                color='ì •í™•ë„',
                color_continuous_scale='RdYlGn'
            )
            fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
            fig.update_layout(
                height=400,
                showlegend=False,
                yaxis_title=t['accuracy'] + ' (%)',
                xaxis_title=t['model']
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # ì •ë‹µ/ì˜¤ë‹µ ìŠ¤íƒ ë°” ì°¨íŠ¸
            fig = go.Figure()
            fig.add_trace(go.Bar(
                name=t['correct'],
                x=model_stats['ëª¨ë¸'],
                y=model_stats['ì •ë‹µ'],
                marker_color='lightgreen'
            ))
            fig.add_trace(go.Bar(
                name=t['wrong'],
                x=model_stats['ëª¨ë¸'],
                y=model_stats['ì˜¤ë‹µ'],
                marker_color='lightcoral'
            ))
            
            fig.update_layout(
                barmode='stack',
                title=f"{t['correct']}/{t['wrong']} {t['comparison_chart']}",
                height=400,
                yaxis_title=t['problem_count'],
                xaxis_title=t['model']
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # íˆíŠ¸ë§µ
        if 'í…ŒìŠ¤íŠ¸ëª…' in filtered_df.columns:
            st.markdown("---")
            st.subheader(t['heatmap'])
            
            # ëª¨ë¸ë³„, í…ŒìŠ¤íŠ¸ë³„ ì •í™•ë„ ê³„ì‚°
            heatmap_data = filtered_df.groupby(['ëª¨ë¸', 'í…ŒìŠ¤íŠ¸ëª…'])['ì •ë‹µì—¬ë¶€'].mean() * 100
            heatmap_pivot = heatmap_data.unstack(fill_value=0)
            
            # íˆíŠ¸ë§µ ìƒì„± (ìˆ«ì í‘œì‹œ ì¶”ê°€)
            fig = px.imshow(
                heatmap_pivot,
                labels=dict(x=t['testname'], y=t['model'], color=t['accuracy'] + " (%)"),
                x=heatmap_pivot.columns,
                y=heatmap_pivot.index,
                color_continuous_scale='RdYlGn',
                aspect="auto",
                text_auto='.1f'  # ìˆ«ì í‘œì‹œ ì¶”ê°€
            )
            
            fig.update_layout(height=400)
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 3: ë²•ë ¹/ë¹„ë²•ë ¹ ë¶„ì„
    with tabs[2]:
        if 'law' not in filtered_df.columns:
            st.info("Law classification data not available.")
        else:
            st.header(f"âš–ï¸ {t['law_analysis']}")
            
            # ì „ì²´ ë²•ë ¹/ë¹„ë²•ë ¹ ë¹„ìœ¨
            st.subheader(t['law_ratio'])
            
            # ì¤‘ë³µ ì œê±°í•œ ë¬¸ì œë¡œ ê³„ì‚°
            unique_problems = filtered_df.drop_duplicates(subset=['Question', 'law'])
            law_count = len(unique_problems[unique_problems['law'] == 'O'])
            non_law_count = len(unique_problems[unique_problems['law'] != 'O'])
            total_unique = law_count + non_law_count
            
            col1, col2 = st.columns(2)
            
            with col1:
                # íŒŒì´ ì°¨íŠ¸
                fig = go.Figure(data=[go.Pie(
                    labels=[t['law'], t['non_law']],
                    values=[law_count, non_law_count],
                    hole=0.3
                )])
                fig.update_layout(
                    title=t['law_distribution_stat'],
                    height=400
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # ìˆ˜ì¹˜ í‘œì‹œ
                st.metric(t['law_problems'], f"{law_count} ({law_count/total_unique*100:.1f}%)")
                st.metric(t['non_law_problems'], f"{non_law_count} ({non_law_count/total_unique*100:.1f}%)")
            
            # ëª¨ë¸ë³„ ë²•ë ¹/ë¹„ë²•ë ¹ ì„±ëŠ¥
            st.markdown("---")
            st.subheader(t['model_law_performance'])
            
            law_performance = []
            for model in filtered_df['ëª¨ë¸'].unique():
                model_df = filtered_df[filtered_df['ëª¨ë¸'] == model]
                
                law_model = model_df[model_df['law'] == 'O']
                non_law_model = model_df[model_df['law'] != 'O']
                
                law_acc = (law_model['ì •ë‹µì—¬ë¶€'].sum() / len(law_model) * 100) if len(law_model) > 0 else 0
                non_law_acc = (non_law_model['ì •ë‹µì—¬ë¶€'].sum() / len(non_law_model) * 100) if len(non_law_model) > 0 else 0
                
                law_performance.append({
                    'ëª¨ë¸': model,
                    'ë²•ë ¹': law_acc,
                    'ë¹„ë²•ë ¹': non_law_acc
                })
            
            law_perf_df = pd.DataFrame(law_performance)
            
            # ê·¸ë£¹ ë°” ì°¨íŠ¸
            fig = go.Figure()
            fig.add_trace(go.Bar(
                name=t['law'],
                x=law_perf_df['ëª¨ë¸'],
                y=law_perf_df['ë²•ë ¹'],
                marker_color='skyblue'
            ))
            fig.add_trace(go.Bar(
                name=t['non_law'],
                x=law_perf_df['ëª¨ë¸'],
                y=law_perf_df['ë¹„ë²•ë ¹'],
                marker_color='lightcoral'
            ))
            
            fig.update_layout(
                barmode='group',
                title=t['law_distribution'],
                height=500,
                yaxis_title=t['accuracy'] + ' (%)',
                xaxis_title=t['model']
            )
            st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 4: ê³¼ëª©ë³„ ë¶„ì„
    with tabs[3]:
        if 'Subject' not in filtered_df.columns:
            st.info("Subject data not available.")
        else:
            st.header(f"ğŸ“š {t['subject_analysis']}")
            
            # ê³¼ëª©ë³„ ì„±ëŠ¥
            subject_stats = filtered_df.groupby('Subject').agg({
                'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
            }).reset_index()
            subject_stats.columns = ['ê³¼ëª©', 'ì •ë‹µ', 'ì´ë¬¸ì œ', 'ì •í™•ë„']
            subject_stats['ì •í™•ë„'] = subject_stats['ì •í™•ë„'] * 100
            subject_stats = subject_stats.sort_values('ì •í™•ë„', ascending=False)
            
            col1, col2 = st.columns([1, 2])
            
            with col1:
                # í…Œì´ë¸”
                st.dataframe(
                    subject_stats.style.format({'ì •í™•ë„': '{:.2f}%'})
                    .background_gradient(subset=['ì •í™•ë„'], cmap='RdYlGn'),
                    use_container_width=True
                )
            
            with col2:
                # ë°” ì°¨íŠ¸
                fig = px.bar(
                    subject_stats,
                    x='ê³¼ëª©',
                    y='ì •í™•ë„',
                    title=t['subject_performance'],
                    text='ì •í™•ë„',
                    color='ì •í™•ë„',
                    color_continuous_scale='RdYlGn'
                )
                fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
                fig.update_layout(
                    height=400,
                    showlegend=False,
                    yaxis_title=t['accuracy'] + ' (%)'
                )
                fig.update_xaxes(tickangle=45)
                st.plotly_chart(fig, use_container_width=True)
            
            # ëª¨ë¸ë³„ ê³¼ëª© ì„±ëŠ¥ íˆíŠ¸ë§µ
            st.markdown("---")
            subject_model = filtered_df.groupby(['ëª¨ë¸', 'Subject'])['ì •ë‹µì—¬ë¶€'].mean() * 100
            subject_model_pivot = subject_model.unstack(fill_value=0)
            
            fig = px.imshow(
                subject_model_pivot,
                labels=dict(x=t['by_subject'], y=t['model'], color=t['accuracy'] + " (%)"),
                x=subject_model_pivot.columns,
                y=subject_model_pivot.index,
                color_continuous_scale='RdYlGn',
                aspect="auto",
                text_auto='.1f'  # ìˆ«ì í‘œì‹œ ì¶”ê°€
            )
            fig.update_layout(height=400)
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 5: ì—°ë„ë³„ ë¶„ì„
    with tabs[4]:
        if 'Year' not in filtered_df.columns:
            st.info("Year data not available.")
        else:
            st.header(f"ğŸ“… {t['year_analysis']}")
            
            # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
            with st.expander("ğŸ” ë””ë²„ê¹… ì •ë³´ (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)"):
                st.write("**í•„í„°ë§ ì „ ì›ë³¸ ë°ì´í„°:**")
                st.write(f"- ì „ì²´ ë°ì´í„° í–‰ ìˆ˜: {len(results_df):,}")
                st.write(f"- ì›ë³¸ Year ê³ ìœ ê°’: {sorted([str(y) for y in results_df['Year'].dropna().unique().tolist()])}")
                
                st.write("**í•„í„°ë§ í›„ ë°ì´í„°:**")
                st.write(f"- í•„í„°ë§ëœ ë°ì´í„° í–‰ ìˆ˜: {len(filtered_df):,}")
                st.write(f"- í•„í„°ë§ëœ Year ê³ ìœ ê°’: {sorted([str(y) for y in filtered_df['Year'].dropna().unique().tolist()])}")
                
                st.write("**í˜„ì¬ í•„í„° ì„¤ì •:**")
                st.write(f"- ì„ íƒëœ í…ŒìŠ¤íŠ¸: {selected_tests}")
                st.write(f"- ì„ íƒëœ ëª¨ë¸: {selected_models}")
                st.write(f"- ì„ íƒëœ ì—°ë„: {selected_years if 'selected_years' in locals() else 'ì „ì²´'}")
            
            # Yearë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
            filtered_df['Year_Int'] = filtered_df['Year'].apply(safe_convert_to_int)
            year_df = filtered_df[filtered_df['Year_Int'].notna()].copy()
            
            if not year_df.empty:
                # ì—°ë„ë³„ ì„±ëŠ¥
                year_stats = year_df.groupby('Year_Int').agg({
                    'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
                }).reset_index()
                year_stats.columns = ['ì—°ë„', 'ì •ë‹µ', 'ì´ë¬¸ì œ', 'ì •í™•ë„']
                year_stats['ì •í™•ë„'] = year_stats['ì •í™•ë„'] * 100
                year_stats = year_stats.sort_values('ì—°ë„')
                
                # ì—°ë„ë¥¼ ì •ìˆ˜ë¡œ í‘œì‹œ
                year_stats['ì—°ë„'] = year_stats['ì—°ë„'].astype(int)
                
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    # í…Œì´ë¸” (ì†Œìˆ˜ì  ì—†ì´ í‘œì‹œ)
                    st.dataframe(
                        year_stats.style.format({
                            'ì—°ë„': '{:.0f}',
                            'ì •ë‹µ': '{:.0f}',
                            'ì´ë¬¸ì œ': '{:.0f}',
                            'ì •í™•ë„': '{:.2f}%'
                        })
                        .background_gradient(subset=['ì •í™•ë„'], cmap='RdYlGn'),
                        use_container_width=True
                    )
                
                with col2:
                    # ë¼ì¸ ì°¨íŠ¸
                    fig = px.line(
                        year_stats,
                        x='ì—°ë„',
                        y='ì •í™•ë„',
                        title=t['year_performance'],
                        markers=True,
                        text='ì •í™•ë„'
                    )
                    fig.update_traces(texttemplate='%{text:.1f}%', textposition='top center')
                    fig.update_layout(
                        height=400,
                        yaxis_title=t['accuracy'] + ' (%)',
                        xaxis_title=t['year']
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # ì—°ë„ë³„ ë¬¸ì œ ìˆ˜ ì°¨íŠ¸ ì¶”ê°€
                st.markdown("---")
                st.subheader("ğŸ“Š ì—°ë„ë³„ ë¬¸ì œ ìˆ˜ ë¶„í¬")
                
                # í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ ì‹¤ì œ ë¬¸ì œ ìˆ˜ ê³„ì‚° (ì¤‘ë³µ ì œê±°)
                if selected_tests:
                    year_problem_count = []
                    for test_name in selected_tests:
                        if test_name in testsets and 'Year' in testsets[test_name].columns:
                            test_year_counts = testsets[test_name].groupby('Year').size()
                            for year, count in test_year_counts.items():
                                year_int = safe_convert_to_int(year)
                                if year_int:
                                    year_problem_count.append({'ì—°ë„': year_int, 'ë¬¸ì œìˆ˜': count})
                    
                    if year_problem_count:
                        year_problem_df = pd.DataFrame(year_problem_count)
                        year_problem_df = year_problem_df.groupby('ì—°ë„')['ë¬¸ì œìˆ˜'].sum().reset_index()
                        year_problem_df = year_problem_df.sort_values('ì—°ë„')
                    else:
                        # ë°±ì—…: filtered_dfì—ì„œ ê³ ìœ  ë¬¸ì œ ìˆ˜ ê³„ì‚°
                        year_problem_df = year_df.groupby('Year_Int')['Question'].nunique().reset_index()
                        year_problem_df.columns = ['ì—°ë„', 'ë¬¸ì œìˆ˜']
                        year_problem_df['ì—°ë„'] = year_problem_df['ì—°ë„'].astype(int)
                        year_problem_df = year_problem_df.sort_values('ì—°ë„')
                else:
                    # í…ŒìŠ¤íŠ¸ ì„ íƒ ì•ˆ ë¨: filtered_dfì—ì„œ ê³„ì‚°
                    year_problem_df = year_df.groupby('Year_Int')['Question'].nunique().reset_index()
                    year_problem_df.columns = ['ì—°ë„', 'ë¬¸ì œìˆ˜']
                    year_problem_df['ì—°ë„'] = year_problem_df['ì—°ë„'].astype(int)
                    year_problem_df = year_problem_df.sort_values('ì—°ë„')
                
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    # ì—°ë„ë³„ ë¬¸ì œ ìˆ˜ í…Œì´ë¸”
                    st.dataframe(
                        year_problem_df.style.format({
                            'ì—°ë„': '{:.0f}',
                            'ë¬¸ì œìˆ˜': '{:.0f}'
                        })
                        .background_gradient(subset=['ë¬¸ì œìˆ˜'], cmap='Blues'),
                        use_container_width=True
                    )
                    
                    # ì´ ë¬¸ì œ ìˆ˜ í‘œì‹œ
                    st.metric("ì´ ë¬¸ì œ ìˆ˜", f"{year_problem_df['ë¬¸ì œìˆ˜'].sum():,.0f}ê°œ")
                
                with col2:
                    # ë°” ì°¨íŠ¸
                    fig = px.bar(
                        year_problem_df,
                        x='ì—°ë„',
                        y='ë¬¸ì œìˆ˜',
                        title='ì—°ë„ë³„ ë¬¸ì œ ìˆ˜',
                        text='ë¬¸ì œìˆ˜',
                        color='ë¬¸ì œìˆ˜',
                        color_continuous_scale='Blues'
                    )
                    fig.update_traces(texttemplate='%{text}', textposition='outside')
                    fig.update_layout(
                        height=400,
                        showlegend=False,
                        yaxis_title='ë¬¸ì œ ìˆ˜',
                        xaxis_title='ì—°ë„',
                        xaxis=dict(tickmode='linear')
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # ëª¨ë¸ë³„ ì—°ë„ ì„±ëŠ¥ íˆíŠ¸ë§µ
                st.markdown("---")
                year_model = year_df.groupby(['ëª¨ë¸', 'Year_Int'])['ì •ë‹µì—¬ë¶€'].mean() * 100
                year_model_pivot = year_model.unstack(fill_value=0)
                
                # ì»¬ëŸ¼ëª…ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                year_model_pivot.columns = year_model_pivot.columns.astype(int)
                
                fig = px.imshow(
                    year_model_pivot,
                    labels=dict(x=t['year'], y=t['model'], color=t['accuracy'] + " (%)"),
                    x=year_model_pivot.columns,
                    y=year_model_pivot.index,
                    color_continuous_scale='RdYlGn',
                    aspect="auto",
                    text_auto='.1f'  # ìˆ«ì í‘œì‹œ ì¶”ê°€
                )
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("ì—°ë„ ì •ë³´ê°€ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # íƒ­ 6: ì˜¤ë‹µ ë¶„ì„
    with tabs[5]:
        st.header(f"âŒ {t['incorrect_analysis']}")
        
        # ë¬¸ì œë³„ ì˜¤ë‹µë¥  ê³„ì‚°
        problem_analysis = filtered_df.groupby('Question').agg({
            'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
        }).reset_index()
        problem_analysis.columns = ['Question', 'correct_count', 'total_count', 'correct_rate']
        problem_analysis['incorrect_rate'] = 1 - problem_analysis['correct_rate']
        problem_analysis['incorrect_count'] = problem_analysis['total_count'] - problem_analysis['correct_count']
        
        # ë¬¸ì œ ì‹ë³„ì ì¶”ê°€
        problem_ids = []
        for question in problem_analysis['Question']:
            matching_rows = filtered_df[filtered_df['Question'] == question]
            if len(matching_rows) > 0:
                problem_id = create_problem_identifier(matching_rows.iloc[0], lang)
                problem_ids.append(problem_id)
            else:
                problem_ids.append("Unknown")
        
        problem_analysis['problem_id'] = problem_ids
        
        # ì˜¤ë‹µë¥  ìˆœìœ¼ë¡œ ì •ë ¬ (ë™ì¼í•œ ì˜¤ë‹µë¥ ì´ë©´ ë¬¸ì œ IDë¡œ ì •ë ¬)
        problem_analysis = problem_analysis.sort_values(
            by=['incorrect_rate', 'problem_id'],
            ascending=[False, True]
        )
        
        # ì‹œë„í•œ ëª¨ë¸ ëª©ë¡ ì¶”ê°€
        attempted_models = []
        for question in problem_analysis['Question']:
            models = filtered_df[filtered_df['Question'] == question]['ëª¨ë¸'].unique().tolist()
            attempted_models.append(', '.join(sorted(models)))
        
        problem_analysis['attempted_models'] = attempted_models
        
        # ëª¨ë¸ë³„ ì •ì˜¤ë‹µ ì •ë³´ ì¶”ê°€
        correct_models_list = []
        incorrect_models_list = []
        
        for question in problem_analysis['Question']:
            q_df = filtered_df[filtered_df['Question'] == question]
            correct_models = q_df[q_df['ì •ë‹µì—¬ë¶€'] == True]['ëª¨ë¸'].unique().tolist()
            incorrect_models = q_df[q_df['ì •ë‹µì—¬ë¶€'] == False]['ëª¨ë¸'].unique().tolist()
            
            correct_models_list.append('âœ“ ' + ', '.join(sorted(correct_models)) if correct_models else '-')
            incorrect_models_list.append('âœ— ' + ', '.join(sorted(incorrect_models)) if incorrect_models else '-')
        
        problem_analysis['correct_models'] = correct_models_list
        problem_analysis['incorrect_models'] = incorrect_models_list
        
        # Top 20 ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ
        st.subheader(t['top_incorrect'])
        
        top_20 = problem_analysis.head(20)
        
        # ë””ìŠ¤í”Œë ˆì´ìš© ë°ì´í„°í”„ë ˆì„
        display_top_20 = pd.DataFrame({
            t['problem_id']: top_20['problem_id'],
            t['incorrect_count']: top_20['incorrect_count'].astype(int),
            t['correct_count']: top_20['correct_count'].astype(int),
            t['total_models']: top_20['total_count'].astype(int),
            t['wrong_rate']: (top_20['incorrect_rate'] * 100).round(2),
            'ì •ë‹µ ëª¨ë¸' if lang == 'ko' else 'Correct Models': top_20['correct_models'],
            'ì˜¤ë‹µ ëª¨ë¸' if lang == 'ko' else 'Incorrect Models': top_20['incorrect_models']
        })
        
        st.dataframe(
            display_top_20.style.background_gradient(
                subset=[t['wrong_rate']],
                cmap='Reds',
                vmin=0,
                vmax=100
            ),
            use_container_width=True,
            height=600
        )
        
        # ëª¨ë“  ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ
        st.markdown("---")
        st.subheader(t['all_models_incorrect'])
        
        all_wrong = problem_analysis[problem_analysis['correct_count'] == 0]
        
        if len(all_wrong) > 0:
            display_all_wrong = pd.DataFrame({
                t['problem_id']: all_wrong['problem_id'],
                t['incorrect_count']: all_wrong['incorrect_count'].astype(int),
                t['correct_count']: all_wrong['correct_count'].astype(int),
                t['total_models']: all_wrong['total_count'].astype(int),
                'ì˜¤ë‹µ ëª¨ë¸' if lang == 'ko' else 'Incorrect Models': all_wrong['incorrect_models']
            })
            
            st.dataframe(display_all_wrong, use_container_width=True)
            
            # ë¬¸ì œ ìƒì„¸ ë³´ê¸° ì˜µì…˜
            if st.checkbox('ë¬¸ì œ ë‚´ìš© ë³´ê¸°' if lang == 'ko' else 'Show Question Details'):
                for idx, row in all_wrong.iterrows():
                    with st.expander(f"{row['problem_id']}"):
                        q_detail = filtered_df[filtered_df['Question'] == row['Question']].iloc[0]
                        st.write(f"**{t['question']}:** {q_detail['Question']}")
                        if 'Subject' in q_detail and pd.notna(q_detail['Subject']):
                            st.write(f"**ê³¼ëª©/Subject:** {q_detail['Subject']}")
                        
                        # ì„ íƒì§€ í‘œì‹œ
                        if all(['Option 1' in q_detail, 'Option 2' in q_detail, 'Option 3' in q_detail, 'Option 4' in q_detail]):
                            st.write("**ì„ íƒì§€/Options:**")
                            for i in range(1, 5):
                                option_key = f'Option {i}'
                                if option_key in q_detail and pd.notna(q_detail[option_key]):
                                    st.write(f"  {i}. {q_detail[option_key]}")
                        
                        # ì •ë‹µ í‘œì‹œ
                        if 'Answer' in q_detail and pd.notna(q_detail['Answer']):
                            st.write(f"**ì •ë‹µ/Answer:** {q_detail['Answer']}")
                        
                        st.write(f"**ì˜¤ë‹µ ëª¨ë¸/Incorrect Models:** {row['incorrect_models']}")
        else:
            st.info("No problems that all models got wrong.")
        
        # ëŒ€ë¶€ë¶„ ëª¨ë¸ì´ í‹€ë¦° ë¬¸ì œ (20% ì´ìƒ)
        st.markdown("---")
        st.subheader(t['most_models_incorrect'])
        
        most_wrong = problem_analysis[problem_analysis['incorrect_rate'] >= 0.2]
        
        if len(most_wrong) > 0:
            display_most_wrong = pd.DataFrame({
                t['problem_id']: most_wrong['problem_id'],
                t['incorrect_count']: most_wrong['incorrect_count'].astype(int),
                t['correct_count']: most_wrong['correct_count'].astype(int),
                t['total_models']: most_wrong['total_count'].astype(int),
                t['wrong_rate']: (most_wrong['incorrect_rate'] * 100).round(2),
                'ì •ë‹µ ëª¨ë¸' if lang == 'ko' else 'Correct Models': most_wrong['correct_models'],
                'ì˜¤ë‹µ ëª¨ë¸' if lang == 'ko' else 'Incorrect Models': most_wrong['incorrect_models']
            })
            
            st.dataframe(
                display_most_wrong.style.background_gradient(
                    subset=[t['wrong_rate']],
                    cmap='Reds',
                    vmin=0,
                    vmax=100
                ),
                use_container_width=True
            )
            
            # ë¬¸ì œ ìƒì„¸ ë³´ê¸° ì˜µì…˜
            if st.checkbox('ë¬¸ì œ ë‚´ìš© ë³´ê¸° (ëŒ€ë¶€ë¶„ í‹€ë¦° ë¬¸ì œ)' if lang == 'ko' else 'Show Question Details (Most Incorrect)', key='most_wrong_details'):
                for idx, row in most_wrong.head(10).iterrows():  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                    with st.expander(f"{row['problem_id']} - ì˜¤ë‹µë¥  {row['incorrect_rate']*100:.1f}%"):
                        q_detail = filtered_df[filtered_df['Question'] == row['Question']].iloc[0]
                        st.write(f"**{t['question']}:** {q_detail['Question']}")
                        if 'Subject' in q_detail and pd.notna(q_detail['Subject']):
                            st.write(f"**ê³¼ëª©/Subject:** {q_detail['Subject']}")
                        
                        # ì„ íƒì§€ í‘œì‹œ
                        if all(['Option 1' in q_detail, 'Option 2' in q_detail, 'Option 3' in q_detail, 'Option 4' in q_detail]):
                            st.write("**ì„ íƒì§€/Options:**")
                            for i in range(1, 5):
                                option_key = f'Option {i}'
                                if option_key in q_detail and pd.notna(q_detail[option_key]):
                                    st.write(f"  {i}. {q_detail[option_key]}")
                        
                        # ì •ë‹µ í‘œì‹œ
                        if 'Answer' in q_detail and pd.notna(q_detail['Answer']):
                            st.write(f"**ì •ë‹µ/Answer:** {q_detail['Answer']}")
                        
                        st.write(f"**âœ“ ì •ë‹µ ëª¨ë¸/Correct Models:** {row['correct_models']}")
                        st.write(f"**âœ— ì˜¤ë‹µ ëª¨ë¸/Incorrect Models:** {row['incorrect_models']}")
        else:
            st.info("No problems that most models got wrong.")
        
        # Top 10 ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ ì°¨íŠ¸
        st.markdown("---")
        top_10_chart = top_20.head(10)
        
        fig = px.bar(
            top_10_chart,
            x='problem_id',
            y='incorrect_rate',
            title='ì˜¤ë‹µë¥  ë†’ì€ ë¬¸ì œ Top 10' if lang == 'ko' else 'Top 10 Problems by Incorrect Rate',
            text=[f"{x:.0%}" for x in top_10_chart['incorrect_rate']],
            color='incorrect_rate',
            color_continuous_scale='Reds',
            range_color=[0, 1]  # ì»¬ëŸ¬ë°” ë²”ìœ„ë¥¼ 0~1ë¡œ ê³ ì •
        )
        fig.update_traces(textposition='outside')
        fig.update_layout(
            height=500,
            showlegend=False,
            yaxis_title=t['wrong_rate'],
            xaxis_title=t['problem_id'],
            yaxis=dict(range=[0, 1])  # yì¶• ë²”ìœ„ë¥¼ 0~1ë¡œ ê³ ì •
        )
        fig.update_xaxes(tickangle=45)
        st.plotly_chart(fig, use_container_width=True)
    
    # íƒ­ 7: ë‚œì´ë„ ë¶„ì„
    with tabs[6]:
        st.header(f"ğŸ“ˆ {t['difficulty_analysis']}")
        
        # ë¬¸ì œë³„ ë‚œì´ë„ ê³„ì‚° (ì •ë‹µë¥  ê¸°ë°˜)
        difficulty = filtered_df.groupby('Question').agg({
            'ì •ë‹µì—¬ë¶€': ['sum', 'count', 'mean']
        }).reset_index()
        difficulty.columns = ['Question', 'correct_count', 'total_count', 'difficulty_score']
        difficulty['difficulty_score'] = difficulty['difficulty_score'] * 100
        
        # ë‚œì´ë„ êµ¬ê°„ ë¶„ë¥˜
        def classify_difficulty(score):
            if score < 20:
                return 'ë§¤ìš° ì–´ë ¤ì›€ (0-20%)'
            elif score < 40:
                return 'ì–´ë ¤ì›€ (20-40%)'
            elif score < 60:
                return 'ë³´í†µ (40-60%)'
            elif score < 80:
                return 'ì‰¬ì›€ (60-80%)'
            else:
                return 'ë§¤ìš° ì‰¬ì›€ (80-100%)'
        
        difficulty['ë‚œì´ë„_êµ¬ê°„'] = difficulty['difficulty_score'].apply(classify_difficulty)
        
        # ë‚œì´ë„ êµ¬ê°„ ìˆœì„œ ì •ì˜ (ì–´ë ¤ìš´ ê²ƒë¶€í„° ì‰¬ìš´ ê²ƒ ìˆœ)
        difficulty_order = [
            'ë§¤ìš° ì–´ë ¤ì›€ (0-20%)',
            'ì–´ë ¤ì›€ (20-40%)',
            'ë³´í†µ (40-60%)',
            'ì‰¬ì›€ (60-80%)',
            'ë§¤ìš° ì‰¬ì›€ (80-100%)'
        ]
        difficulty['ë‚œì´ë„_êµ¬ê°„'] = pd.Categorical(difficulty['ë‚œì´ë„_êµ¬ê°„'], categories=difficulty_order, ordered=True)
        
        # ì›ë³¸ ë°ì´í„°ì— ë‚œì´ë„ ì •ë³´ ë³‘í•©
        analysis_df = filtered_df.merge(difficulty[['Question', 'difficulty_score', 'ë‚œì´ë„_êµ¬ê°„']], on='Question')
        
        # analysis_dfì—ë„ ë™ì¼í•œ ìˆœì„œ ì ìš©
        analysis_df['ë‚œì´ë„_êµ¬ê°„'] = pd.Categorical(analysis_df['ë‚œì´ë„_êµ¬ê°„'], categories=difficulty_order, ordered=True)
        
        # 1. ë‚œì´ë„ ë¶„í¬
        st.subheader("ğŸ“ˆ ë¬¸ì œ ë‚œì´ë„ ë¶„í¬")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ë‚œì´ë„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
            fig = px.histogram(
                difficulty,
                x='difficulty_score',
                nbins=20,
                title=t['difficulty_score'] + ' Distribution',
                labels={'difficulty_score': t['difficulty_score'], 'count': t['problem_count']}
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # ë‚œì´ë„ êµ¬ê°„ë³„ ë¬¸ì œ ìˆ˜
            difficulty_dist = difficulty['ë‚œì´ë„_êµ¬ê°„'].value_counts()
            # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬
            difficulty_dist = difficulty_dist.reindex(difficulty_order, fill_value=0)
            
            fig = px.bar(
                x=difficulty_dist.index,
                y=difficulty_dist.values,
                title=t['problem_count'] + ' by Difficulty',
                labels={'x': 'Difficulty', 'y': t['problem_count']},
                color=difficulty_dist.values,
                color_continuous_scale='RdYlGn_r'
            )
            fig.update_layout(
                height=400,
                showlegend=False
            )
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
        
        # í†µê³„ ìš”ì•½
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("í‰ê·  ì •ë‹µë¥ ", f"{difficulty['difficulty_score'].mean():.1f}%")
        with col2:
            st.metric("ì¤‘ì•™ê°’", f"{difficulty['difficulty_score'].median():.1f}%")
        with col3:
            very_hard = len(difficulty[difficulty['ë‚œì´ë„_êµ¬ê°„'] == 'ë§¤ìš° ì–´ë ¤ì›€ (0-20%)'])
            st.metric("ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œ", f"{very_hard}ê°œ")
        with col4:
            very_easy = len(difficulty[difficulty['ë‚œì´ë„_êµ¬ê°„'] == 'ë§¤ìš° ì‰¬ì›€ (80-100%)'])
            st.metric("ë§¤ìš° ì‰¬ìš´ ë¬¸ì œ", f"{very_easy}ê°œ")
        
        st.markdown("---")
        
        # 2. ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥
        st.subheader("ğŸ¯ ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥")
        
        # ëª¨ë¸ë³„ ë‚œì´ë„ êµ¬ê°„ë³„ ì •ë‹µë¥ 
        model_difficulty = analysis_df.groupby(['ëª¨ë¸', 'ë‚œì´ë„_êµ¬ê°„']).agg({
            'ì •ë‹µì—¬ë¶€': ['mean', 'count']
        }).reset_index()
        model_difficulty.columns = ['ëª¨ë¸', 'ë‚œì´ë„_êµ¬ê°„', 'ì •ë‹µë¥ ', 'ë¬¸ì œìˆ˜']
        model_difficulty['ì •ë‹µë¥ '] = model_difficulty['ì •ë‹µë¥ '] * 100
        
        # ë¼ì¸ ì°¨íŠ¸
        fig = px.line(
            model_difficulty,
            x='ë‚œì´ë„_êµ¬ê°„',
            y='ì •ë‹µë¥ ',
            color='ëª¨ë¸',
            markers=True,
            title='ë‚œì´ë„ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ',
            labels={'ì •ë‹µë¥ ': t['accuracy'] + ' (%)', 'ë‚œì´ë„_êµ¬ê°„': 'Difficulty Level'},
            category_orders={'ë‚œì´ë„_êµ¬ê°„': difficulty_order}
        )
        fig.update_layout(height=500)
        fig.update_xaxes(tickangle=45)
        st.plotly_chart(fig, use_container_width=True)
        
        # íˆíŠ¸ë§µ
        pivot_difficulty = model_difficulty.pivot(
            index='ëª¨ë¸',
            columns='ë‚œì´ë„_êµ¬ê°„',
            values='ì •ë‹µë¥ '
        )
        
        # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬
        pivot_difficulty = pivot_difficulty.reindex(columns=difficulty_order)
        
        fig = px.imshow(
            pivot_difficulty,
            labels=dict(x="ë‚œì´ë„ êµ¬ê°„", y=t['model'], color=t['accuracy'] + " (%)"),
            x=pivot_difficulty.columns,
            y=pivot_difficulty.index,
            color_continuous_scale='RdYlGn',
            aspect="auto",
            title='ëª¨ë¸ Ã— ë‚œì´ë„ íˆíŠ¸ë§µ',
            text_auto='.1f'  # ìˆ«ì í‘œì‹œ
        )
        fig.update_layout(height=400)
        fig.update_xaxes(tickangle=45)
        st.plotly_chart(fig, use_container_width=True)
        
        st.markdown("---")
        
        # 3. ê³¼ëª©ë³„ ë‚œì´ë„ ë¶„ì„
        if 'Subject' in analysis_df.columns:
            st.subheader("ğŸ“š ê³¼ëª©ë³„ ë‚œì´ë„ ë¶„ì„")
            
            subject_difficulty = analysis_df.groupby('Subject').agg({
                'difficulty_score': 'mean',
                'Question': 'count'
            }).reset_index()
            subject_difficulty.columns = ['ê³¼ëª©', 'í‰ê· _ë‚œì´ë„', 'ë¬¸ì œìˆ˜']
            subject_difficulty = subject_difficulty.sort_values('í‰ê· _ë‚œì´ë„')
            
            fig = px.bar(
                subject_difficulty,
                x='ê³¼ëª©',
                y='í‰ê· _ë‚œì´ë„',
                title='ê³¼ëª©ë³„ í‰ê·  ë‚œì´ë„ (ì •ë‹µë¥ )',
                text='í‰ê· _ë‚œì´ë„',
                color='í‰ê· _ë‚œì´ë„',
                color_continuous_scale='RdYlGn'
            )
            fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
            fig.update_xaxes(tickangle=45)
            fig.update_layout(
                height=500,
                showlegend=False
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # ê³¼ëª© Ã— ë‚œì´ë„ êµ¬ê°„ íˆíŠ¸ë§µ
            subject_diff_dist = analysis_df.groupby(['Subject', 'ë‚œì´ë„_êµ¬ê°„']).size().reset_index(name='ë¬¸ì œìˆ˜')
            pivot_subject_diff = subject_diff_dist.pivot(
                index='Subject',
                columns='ë‚œì´ë„_êµ¬ê°„',
                values='ë¬¸ì œìˆ˜'
            ).fillna(0)
            
            # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬
            pivot_subject_diff = pivot_subject_diff.reindex(columns=difficulty_order, fill_value=0)
            
            fig = px.imshow(
                pivot_subject_diff,
                labels=dict(x="ë‚œì´ë„ êµ¬ê°„", y="ê³¼ëª©", color="ë¬¸ì œ ìˆ˜"),
                x=pivot_subject_diff.columns,
                y=pivot_subject_diff.index,
                color_continuous_scale='Blues',
                aspect="auto",
                title='ê³¼ëª© Ã— ë‚œì´ë„ ë¶„í¬',
                text_auto=True  # ìˆ«ì í‘œì‹œ
            )
            fig.update_layout(height=500)
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
        
        st.markdown("---")
        
        # 4. ì–´ë ¤ìš´ ë¬¸ì œ vs ì‰¬ìš´ ë¬¸ì œ ìƒì„¸ ë¶„ì„
        st.subheader("ğŸ” ì–´ë ¤ìš´ ë¬¸ì œ vs ì‰¬ìš´ ë¬¸ì œ ë¹„êµ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œ (ì •ë‹µë¥  < 20%)")
            very_hard_problems = difficulty[difficulty['difficulty_score'] < 20].sort_values('difficulty_score')
            
            if len(very_hard_problems) > 0:
                st.metric("ë¬¸ì œ ìˆ˜", f"{len(very_hard_problems)}ê°œ")
                st.metric("í‰ê·  ì •ë‹µë¥ ", f"{very_hard_problems['difficulty_score'].mean():.1f}%")
                
                # ëª¨ë¸ë³„ ì„±ëŠ¥
                very_hard_questions = very_hard_problems['Question'].tolist()
                very_hard_model_perf = filtered_df[filtered_df['Question'].isin(very_hard_questions)].groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean() * 100
                
                st.markdown("**ëª¨ë¸ë³„ ì„±ëŠ¥**")
                for model, acc in very_hard_model_perf.sort_values(ascending=False).items():
                    st.write(f"- {model}: {acc:.1f}%")
            else:
                st.info("ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with col2:
            st.markdown("#### ë§¤ìš° ì‰¬ìš´ ë¬¸ì œ (ì •ë‹µë¥  > 80%)")
            very_easy_problems = difficulty[difficulty['difficulty_score'] > 80].sort_values('difficulty_score', ascending=False)
            
            if len(very_easy_problems) > 0:
                st.metric("ë¬¸ì œ ìˆ˜", f"{len(very_easy_problems)}ê°œ")
                st.metric("í‰ê·  ì •ë‹µë¥ ", f"{very_easy_problems['difficulty_score'].mean():.1f}%")
                
                # ëª¨ë¸ë³„ ì„±ëŠ¥
                very_easy_questions = very_easy_problems['Question'].tolist()
                very_easy_model_perf = filtered_df[filtered_df['Question'].isin(very_easy_questions)].groupby('ëª¨ë¸')['ì •ë‹µì—¬ë¶€'].mean() * 100
                
                st.markdown("**ëª¨ë¸ë³„ ì„±ëŠ¥**")
                for model, acc in very_easy_model_perf.sort_values(ascending=False).items():
                    st.write(f"- {model}: {acc:.1f}%")
            else:
                st.info("ë§¤ìš° ì‰¬ìš´ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        st.markdown("---")
        
        # 5. ë‚œì´ë„ êµ¬ê°„ë³„ ìƒì„¸ í…Œì´ë¸”
        st.subheader("ğŸ“‹ ë‚œì´ë„ êµ¬ê°„ë³„ ìƒì„¸ í†µê³„")
        
        detailed_difficulty = model_difficulty.pivot_table(
            index='ëª¨ë¸',
            columns='ë‚œì´ë„_êµ¬ê°„',
            values='ì •ë‹µë¥ ',
            aggfunc='mean'
        ).round(2)
        
        # ë‚œì´ë„ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬
        detailed_difficulty = detailed_difficulty.reindex(columns=difficulty_order)
        
        st.dataframe(
            detailed_difficulty.style.background_gradient(cmap='RdYlGn', axis=None),
            use_container_width=True
        )
    
    # íƒ­ 8: í…ŒìŠ¤íŠ¸ì…‹ í†µê³„
    with tabs[7]:
        st.header(f"ğŸ“‹ {t['testset_stats']}")
        
        if selected_tests:
            # ì„ íƒëœ í…ŒìŠ¤íŠ¸ë“¤ì˜ í†µê³„ í‘œì‹œ
            for test_name in selected_tests:
                stats = get_testset_statistics(testsets, test_name, lang)
                if stats:
                    st.subheader(f"ğŸ“– {test_name}")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric(t['total_problems'], stats['total_problems'])
                    
                    with col2:
                        if 'law_problems' in stats:
                            st.metric(t['law_problems'], stats['law_problems'])
                    
                    with col3:
                        if 'non_law_problems' in stats:
                            st.metric(t['non_law_problems'], stats['non_law_problems'])
                    
                    # ê³¼ëª©ë³„, ì—°ë„ë³„, ì„¸ì…˜ë³„ í†µê³„
                    if 'by_subject' in stats or 'by_year' in stats or 'by_session' in stats:
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            if 'by_subject' in stats:
                                st.markdown(f"**{t['by_subject']}**")
                                subject_df = pd.DataFrame(list(stats['by_subject'].items()), 
                                                         columns=['Subject', 'Count'])
                                fig = px.bar(subject_df, x='Subject', y='Count', 
                                           title=t['subject_distribution'])
                                fig.update_xaxes(tickangle=45)
                                st.plotly_chart(fig, use_container_width=True)
                        
                        with col2:
                            if 'by_year' in stats:
                                st.markdown(f"**{t['by_year']}**")
                                year_df = pd.DataFrame(list(stats['by_year'].items()), 
                                                      columns=['Year', 'Count'])
                                fig = px.bar(year_df, x='Year', y='Count', 
                                           title=t['year_distribution'])
                                st.plotly_chart(fig, use_container_width=True)
                        
                        with col3:
                            if 'by_session' in stats:
                                st.markdown(f"**{t['by_session']}**")
                                session_df = pd.DataFrame(list(stats['by_session'].items()), 
                                                         columns=['Session', 'Count'])
                                fig = px.bar(session_df, x='Session', y='Count', 
                                           title=t['session_distribution'])
                                st.plotly_chart(fig, use_container_width=True)
                    
                    st.markdown("---")
        else:
            st.info("í…ŒìŠ¤íŠ¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    
    # ì‚¬ì´ë“œë°” í•˜ë‹¨ ì •ë³´
    st.sidebar.markdown("---")
    st.sidebar.markdown(f"### ğŸ“Œ {t['help']}")
    st.sidebar.markdown(f"""
    **{t['new_features']}:**
    - âœ¨ **{t['session']} {t['filters']}**: {t['session_filter']}
    - âœ¨ **{t['incorrect_analysis']}**: {t['incorrect_pattern']}
    - âœ¨ **{t['difficulty_analysis']}**: {t['difficulty_comparison']}
    - âœ¨ **{t['problem_type']} {t['filters']}**: {t['problem_type_filter']}
    
    **{t['existing_features']}:**
    - {t['basic_filters']}
    - {t['law_analysis_desc']}
    - {t['detail_analysis']}
    """)
    
    st.sidebar.info(f"ğŸ“Š {t['current_data']}: {len(filtered_df):,}{t['problems']}")

if __name__ == "__main__":
    main()